================================================================================
COMPREHENSIVE HYPERPARAMETER SEARCH LOG
Started: 2025-10-20 13:11:43
Device: cpu
Class Weights: tensor([1.0812, 0.9302])
================================================================================

[13:11:43] ================================================================================
[13:11:43] STARTING COMPREHENSIVE HYPERPARAMETER SEARCH
[13:11:43] ================================================================================
[13:11:43] Max trials: Infinite (Ctrl+C to stop)
[13:11:43] Device: cpu
[13:11:43] Results will be saved to: hyperparameter_results.json
[13:11:43] Log file: hyperparameter_search_log.txt
[13:11:43] ================================================================================

[13:11:43] ================================================================================
[13:11:43] TRIAL 1 - Starting
[13:11:43] ================================================================================
[13:11:43] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1.3e-06,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:11:43] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1.3e-06, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:11:43] ❌ Trial 1 failed with error: expected scalar type Long but found Float
[13:11:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:43] ✓ Results saved to hyperparameter_results.json
[13:11:43] ================================================================================
[13:11:43] TRIAL 2 - Starting
[13:11:43] ================================================================================
[13:11:43] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:11:43] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:11:43] ❌ Trial 2 failed with error: expected scalar type Long but found Float
[13:11:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:43] ✓ Results saved to hyperparameter_results.json
[13:11:43] ================================================================================
[13:11:43] TRIAL 3 - Starting
[13:11:43] ================================================================================
[13:11:43] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:11:43] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:11:43] ❌ Trial 3 failed with error: expected scalar type Long but found Float
[13:11:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:43] ✓ Results saved to hyperparameter_results.json
[13:11:43] ================================================================================
[13:11:43] TRIAL 4 - Starting
[13:11:43] ================================================================================
[13:11:43] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:11:43] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:11:43] ❌ Trial 4 failed with error: expected scalar type Long but found Float
[13:11:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:43] ✓ Results saved to hyperparameter_results.json
[13:11:43] ================================================================================
[13:11:43] TRIAL 5 - Starting
[13:11:43] ================================================================================
[13:11:43] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:11:43] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:11:43] ❌ Trial 5 failed with error: expected scalar type Long but found Float
[13:11:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:43] ✓ Results saved to hyperparameter_results.json
[13:11:43] 
================================================================================
[13:11:43] PROGRESS UPDATE - Completed 5 trials
[13:11:43] Best so far: 0.0000
[13:11:43] ================================================================================

[13:11:43] ================================================================================
[13:11:43] TRIAL 6 - Starting
[13:11:43] ================================================================================
[13:11:43] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:43] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:43] ❌ Trial 6 failed with error: expected scalar type Long but found Float
[13:11:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:43] ✓ Results saved to hyperparameter_results.json
[13:11:43] ================================================================================
[13:11:43] TRIAL 7 - Starting
[13:11:43] ================================================================================
[13:11:43] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:11:43] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:11:43] ❌ Trial 7 failed with error: expected scalar type Long but found Float
[13:11:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:43] ✓ Results saved to hyperparameter_results.json
[13:11:43] ================================================================================
[13:11:43] TRIAL 8 - Starting
[13:11:43] ================================================================================
[13:11:43] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:11:43] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:11:44] ❌ Trial 8 failed with error: expected scalar type Long but found Float
[13:11:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:44] ✓ Results saved to hyperparameter_results.json
[13:11:44] ================================================================================
[13:11:44] TRIAL 9 - Starting
[13:11:44] ================================================================================
[13:11:44] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:11:44] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:11:44] ❌ Trial 9 failed with error: expected scalar type Long but found Float
[13:11:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:44] ✓ Results saved to hyperparameter_results.json
[13:11:44] ================================================================================
[13:11:44] TRIAL 10 - Starting
[13:11:44] ================================================================================
[13:11:44] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:11:44] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:11:44] ❌ Trial 10 failed with error: expected scalar type Long but found Float
[13:11:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:44] ✓ Results saved to hyperparameter_results.json
[13:11:44] 
================================================================================
[13:11:44] PROGRESS UPDATE - Completed 10 trials
[13:11:44] Best so far: 0.0000
[13:11:44] ================================================================================

[13:11:44] ================================================================================
[13:11:44] TRIAL 11 - Starting
[13:11:44] ================================================================================
[13:11:44] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:44] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:44] ❌ Trial 11 failed with error: expected scalar type Long but found Float
[13:11:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:44] ✓ Results saved to hyperparameter_results.json
[13:11:44] ================================================================================
[13:11:44] TRIAL 12 - Starting
[13:11:44] ================================================================================
[13:11:44] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:44] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:44] ❌ Trial 12 failed with error: expected scalar type Long but found Float
[13:11:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:44] ✓ Results saved to hyperparameter_results.json
[13:11:44] ================================================================================
[13:11:44] TRIAL 13 - Starting
[13:11:44] ================================================================================
[13:11:44] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:11:44] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:11:44] ❌ Trial 13 failed with error: expected scalar type Long but found Float
[13:11:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:44] ✓ Results saved to hyperparameter_results.json
[13:11:44] ================================================================================
[13:11:44] TRIAL 14 - Starting
[13:11:44] ================================================================================
[13:11:44] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:44] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:44] ❌ Trial 14 failed with error: expected scalar type Long but found Float
[13:11:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:44] ✓ Results saved to hyperparameter_results.json
[13:11:44] ================================================================================
[13:11:44] TRIAL 15 - Starting
[13:11:44] ================================================================================
[13:11:44] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:11:44] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:11:44] ❌ Trial 15 failed with error: expected scalar type Long but found Float
[13:11:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:44] ✓ Results saved to hyperparameter_results.json
[13:11:44] 
================================================================================
[13:11:44] PROGRESS UPDATE - Completed 15 trials
[13:11:44] Best so far: 0.0000
[13:11:44] ================================================================================

[13:11:44] ================================================================================
[13:11:44] TRIAL 16 - Starting
[13:11:44] ================================================================================
[13:11:44] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:44] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:44] ❌ Trial 16 failed with error: expected scalar type Long but found Float
[13:11:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:44] ✓ Results saved to hyperparameter_results.json
[13:11:44] ================================================================================
[13:11:44] TRIAL 17 - Starting
[13:11:44] ================================================================================
[13:11:44] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:11:44] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:11:44] ❌ Trial 17 failed with error: expected scalar type Long but found Float
[13:11:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:44] ✓ Results saved to hyperparameter_results.json
[13:11:44] ================================================================================
[13:11:44] TRIAL 18 - Starting
[13:11:44] ================================================================================
[13:11:44] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:44] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:44] ❌ Trial 18 failed with error: expected scalar type Long but found Float
[13:11:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:44] ✓ Results saved to hyperparameter_results.json
[13:11:44] ================================================================================
[13:11:44] TRIAL 19 - Starting
[13:11:44] ================================================================================
[13:11:44] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:11:44] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:11:44] ❌ Trial 19 failed with error: expected scalar type Long but found Float
[13:11:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:44] ✓ Results saved to hyperparameter_results.json
[13:11:44] ================================================================================
[13:11:44] TRIAL 20 - Starting
[13:11:44] ================================================================================
[13:11:44] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:11:45] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:11:45] ❌ Trial 20 failed with error: expected scalar type Long but found Float
[13:11:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:45] ✓ Results saved to hyperparameter_results.json
[13:11:45] 
================================================================================
[13:11:45] PROGRESS UPDATE - Completed 20 trials
[13:11:45] Best so far: 0.0000
[13:11:45] ================================================================================

[13:11:45] ================================================================================
[13:11:45] TRIAL 21 - Starting
[13:11:45] ================================================================================
[13:11:45] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:45] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:45] ❌ Trial 21 failed with error: expected scalar type Long but found Float
[13:11:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:45] ✓ Results saved to hyperparameter_results.json
[13:11:45] ================================================================================
[13:11:45] TRIAL 22 - Starting
[13:11:45] ================================================================================
[13:11:45] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:11:45] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:11:45] ❌ Trial 22 failed with error: expected scalar type Long but found Float
[13:11:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:45] ✓ Results saved to hyperparameter_results.json
[13:11:45] ================================================================================
[13:11:45] TRIAL 23 - Starting
[13:11:45] ================================================================================
[13:11:45] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:45] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:45] ❌ Trial 23 failed with error: expected scalar type Long but found Float
[13:11:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:45] ✓ Results saved to hyperparameter_results.json
[13:11:45] ================================================================================
[13:11:45] TRIAL 24 - Starting
[13:11:45] ================================================================================
[13:11:45] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:11:45] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:11:45] ❌ Trial 24 failed with error: expected scalar type Long but found Float
[13:11:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:45] ✓ Results saved to hyperparameter_results.json
[13:11:45] ================================================================================
[13:11:45] TRIAL 25 - Starting
[13:11:45] ================================================================================
[13:11:45] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:45] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:45] ❌ Trial 25 failed with error: expected scalar type Long but found Float
[13:11:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:45] ✓ Results saved to hyperparameter_results.json
[13:11:45] 
================================================================================
[13:11:45] PROGRESS UPDATE - Completed 25 trials
[13:11:45] Best so far: 0.0000
[13:11:45] ================================================================================

[13:11:45] ================================================================================
[13:11:45] TRIAL 26 - Starting
[13:11:45] ================================================================================
[13:11:45] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:11:45] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:11:45] ❌ Trial 26 failed with error: expected scalar type Long but found Float
[13:11:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:45] ✓ Results saved to hyperparameter_results.json
[13:11:45] ================================================================================
[13:11:45] TRIAL 27 - Starting
[13:11:45] ================================================================================
[13:11:45] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:11:45] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:11:45] ❌ Trial 27 failed with error: expected scalar type Long but found Float
[13:11:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:45] ✓ Results saved to hyperparameter_results.json
[13:11:45] ================================================================================
[13:11:45] TRIAL 28 - Starting
[13:11:45] ================================================================================
[13:11:45] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:45] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:45] ❌ Trial 28 failed with error: expected scalar type Long but found Float
[13:11:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:45] ✓ Results saved to hyperparameter_results.json
[13:11:45] ================================================================================
[13:11:45] TRIAL 29 - Starting
[13:11:45] ================================================================================
[13:11:45] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:11:45] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:11:45] ❌ Trial 29 failed with error: expected scalar type Long but found Float
[13:11:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:45] ✓ Results saved to hyperparameter_results.json
[13:11:45] ================================================================================
[13:11:45] TRIAL 30 - Starting
[13:11:45] ================================================================================
[13:11:45] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:45] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:45] ❌ Trial 30 failed with error: expected scalar type Long but found Float
[13:11:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:46] ✓ Results saved to hyperparameter_results.json
[13:11:46] 
================================================================================
[13:11:46] PROGRESS UPDATE - Completed 30 trials
[13:11:46] Best so far: 0.0000
[13:11:46] ================================================================================

[13:11:46] ================================================================================
[13:11:46] TRIAL 31 - Starting
[13:11:46] ================================================================================
[13:11:46] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:11:46] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:11:46] ❌ Trial 31 failed with error: expected scalar type Long but found Float
[13:11:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:46] ✓ Results saved to hyperparameter_results.json
[13:11:46] ================================================================================
[13:11:46] TRIAL 32 - Starting
[13:11:46] ================================================================================
[13:11:46] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:46] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:46] ❌ Trial 32 failed with error: expected scalar type Long but found Float
[13:11:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:46] ✓ Results saved to hyperparameter_results.json
[13:11:46] ================================================================================
[13:11:46] TRIAL 33 - Starting
[13:11:46] ================================================================================
[13:11:46] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:46] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:46] ❌ Trial 33 failed with error: expected scalar type Long but found Float
[13:11:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:46] ✓ Results saved to hyperparameter_results.json
[13:11:46] ================================================================================
[13:11:46] TRIAL 34 - Starting
[13:11:46] ================================================================================
[13:11:46] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:11:46] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:11:46] ❌ Trial 34 failed with error: expected scalar type Long but found Float
[13:11:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:46] ✓ Results saved to hyperparameter_results.json
[13:11:46] ================================================================================
[13:11:46] TRIAL 35 - Starting
[13:11:46] ================================================================================
[13:11:46] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:11:46] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:11:46] ❌ Trial 35 failed with error: expected scalar type Long but found Float
[13:11:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:46] ✓ Results saved to hyperparameter_results.json
[13:11:46] 
================================================================================
[13:11:46] PROGRESS UPDATE - Completed 35 trials
[13:11:46] Best so far: 0.0000
[13:11:46] ================================================================================

[13:11:46] ================================================================================
[13:11:46] TRIAL 36 - Starting
[13:11:46] ================================================================================
[13:11:46] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:46] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:46] ❌ Trial 36 failed with error: expected scalar type Long but found Float
[13:11:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:46] ✓ Results saved to hyperparameter_results.json
[13:11:46] ================================================================================
[13:11:46] TRIAL 37 - Starting
[13:11:46] ================================================================================
[13:11:46] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:11:46] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:11:46] ❌ Trial 37 failed with error: expected scalar type Long but found Float
[13:11:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:46] ✓ Results saved to hyperparameter_results.json
[13:11:46] ================================================================================
[13:11:46] TRIAL 38 - Starting
[13:11:46] ================================================================================
[13:11:46] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:11:46] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:11:46] ❌ Trial 38 failed with error: expected scalar type Long but found Float
[13:11:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:46] ✓ Results saved to hyperparameter_results.json
[13:11:46] ================================================================================
[13:11:46] TRIAL 39 - Starting
[13:11:46] ================================================================================
[13:11:46] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:11:46] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:11:46] ❌ Trial 39 failed with error: expected scalar type Long but found Float
[13:11:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:46] ✓ Results saved to hyperparameter_results.json
[13:11:46] ================================================================================
[13:11:46] TRIAL 40 - Starting
[13:11:46] ================================================================================
[13:11:46] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:11:46] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:11:47] ❌ Trial 40 failed with error: expected scalar type Long but found Float
[13:11:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:47] ✓ Results saved to hyperparameter_results.json
[13:11:47] 
================================================================================
[13:11:47] PROGRESS UPDATE - Completed 40 trials
[13:11:47] Best so far: 0.0000
[13:11:47] ================================================================================

[13:11:47] ================================================================================
[13:11:47] TRIAL 41 - Starting
[13:11:47] ================================================================================
[13:11:47] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:11:47] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:11:47] ❌ Trial 41 failed with error: expected scalar type Long but found Float
[13:11:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:47] ✓ Results saved to hyperparameter_results.json
[13:11:47] ================================================================================
[13:11:47] TRIAL 42 - Starting
[13:11:47] ================================================================================
[13:11:47] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:11:47] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:11:47] ❌ Trial 42 failed with error: expected scalar type Long but found Float
[13:11:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:47] ✓ Results saved to hyperparameter_results.json
[13:11:47] ================================================================================
[13:11:47] TRIAL 43 - Starting
[13:11:47] ================================================================================
[13:11:47] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:11:47] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:11:47] ❌ Trial 43 failed with error: expected scalar type Long but found Float
[13:11:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:47] ✓ Results saved to hyperparameter_results.json
[13:11:47] ================================================================================
[13:11:47] TRIAL 44 - Starting
[13:11:47] ================================================================================
[13:11:47] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:47] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:47] ❌ Trial 44 failed with error: expected scalar type Long but found Float
[13:11:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:47] ✓ Results saved to hyperparameter_results.json
[13:11:47] ================================================================================
[13:11:47] TRIAL 45 - Starting
[13:11:47] ================================================================================
[13:11:47] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:11:47] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:11:47] ❌ Trial 45 failed with error: expected scalar type Long but found Float
[13:11:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:47] ✓ Results saved to hyperparameter_results.json
[13:11:47] 
================================================================================
[13:11:47] PROGRESS UPDATE - Completed 45 trials
[13:11:47] Best so far: 0.0000
[13:11:47] ================================================================================

[13:11:47] ================================================================================
[13:11:47] TRIAL 46 - Starting
[13:11:47] ================================================================================
[13:11:47] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:47] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:47] ❌ Trial 46 failed with error: expected scalar type Long but found Float
[13:11:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:47] ✓ Results saved to hyperparameter_results.json
[13:11:47] ================================================================================
[13:11:47] TRIAL 47 - Starting
[13:11:47] ================================================================================
[13:11:47] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:11:47] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:11:47] ❌ Trial 47 failed with error: expected scalar type Long but found Float
[13:11:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:47] ✓ Results saved to hyperparameter_results.json
[13:11:47] ================================================================================
[13:11:47] TRIAL 48 - Starting
[13:11:47] ================================================================================
[13:11:47] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:11:47] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:11:47] ❌ Trial 48 failed with error: expected scalar type Long but found Float
[13:11:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:47] ✓ Results saved to hyperparameter_results.json
[13:11:47] ================================================================================
[13:11:47] TRIAL 49 - Starting
[13:11:47] ================================================================================
[13:11:47] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:47] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:48] ❌ Trial 49 failed with error: expected scalar type Long but found Float
[13:11:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:48] ✓ Results saved to hyperparameter_results.json
[13:11:48] ================================================================================
[13:11:48] TRIAL 50 - Starting
[13:11:48] ================================================================================
[13:11:48] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:11:48] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:11:48] ❌ Trial 50 failed with error: expected scalar type Long but found Float
[13:11:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:48] ✓ Results saved to hyperparameter_results.json
[13:11:48] 
================================================================================
[13:11:48] PROGRESS UPDATE - Completed 50 trials
[13:11:48] Best so far: 0.0000
[13:11:48] ================================================================================

[13:11:48] ================================================================================
[13:11:48] TRIAL 51 - Starting
[13:11:48] ================================================================================
[13:11:48] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:48] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:48] ❌ Trial 51 failed with error: expected scalar type Long but found Float
[13:11:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:48] ✓ Results saved to hyperparameter_results.json
[13:11:48] ================================================================================
[13:11:48] TRIAL 52 - Starting
[13:11:48] ================================================================================
[13:11:48] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:11:48] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:11:48] ❌ Trial 52 failed with error: expected scalar type Long but found Float
[13:11:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:48] ✓ Results saved to hyperparameter_results.json
[13:11:48] ================================================================================
[13:11:48] TRIAL 53 - Starting
[13:11:48] ================================================================================
[13:11:48] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:11:48] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:11:48] ❌ Trial 53 failed with error: expected scalar type Long but found Float
[13:11:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:48] ✓ Results saved to hyperparameter_results.json
[13:11:48] ================================================================================
[13:11:48] TRIAL 54 - Starting
[13:11:48] ================================================================================
[13:11:48] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:48] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:48] ❌ Trial 54 failed with error: expected scalar type Long but found Float
[13:11:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:48] ✓ Results saved to hyperparameter_results.json
[13:11:48] ================================================================================
[13:11:48] TRIAL 55 - Starting
[13:11:48] ================================================================================
[13:11:48] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:11:48] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:11:48] ❌ Trial 55 failed with error: expected scalar type Long but found Float
[13:11:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:48] ✓ Results saved to hyperparameter_results.json
[13:11:48] 
================================================================================
[13:11:48] PROGRESS UPDATE - Completed 55 trials
[13:11:48] Best so far: 0.0000
[13:11:48] ================================================================================

[13:11:48] ================================================================================
[13:11:48] TRIAL 56 - Starting
[13:11:48] ================================================================================
[13:11:48] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:48] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:48] ❌ Trial 56 failed with error: expected scalar type Long but found Float
[13:11:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:48] ✓ Results saved to hyperparameter_results.json
[13:11:48] ================================================================================
[13:11:48] TRIAL 57 - Starting
[13:11:48] ================================================================================
[13:11:48] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:48] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:48] ❌ Trial 57 failed with error: expected scalar type Long but found Float
[13:11:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:48] ✓ Results saved to hyperparameter_results.json
[13:11:48] ================================================================================
[13:11:48] TRIAL 58 - Starting
[13:11:48] ================================================================================
[13:11:48] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:11:48] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:11:48] ❌ Trial 58 failed with error: expected scalar type Long but found Float
[13:11:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:48] ✓ Results saved to hyperparameter_results.json
[13:11:48] ================================================================================
[13:11:48] TRIAL 59 - Starting
[13:11:48] ================================================================================
[13:11:48] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:11:48] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:11:49] ❌ Trial 59 failed with error: expected scalar type Long but found Float
[13:11:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:49] ✓ Results saved to hyperparameter_results.json
[13:11:49] ================================================================================
[13:11:49] TRIAL 60 - Starting
[13:11:49] ================================================================================
[13:11:49] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:11:49] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:11:49] ❌ Trial 60 failed with error: expected scalar type Long but found Float
[13:11:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:49] ✓ Results saved to hyperparameter_results.json
[13:11:49] 
================================================================================
[13:11:49] PROGRESS UPDATE - Completed 60 trials
[13:11:49] Best so far: 0.0000
[13:11:49] ================================================================================

[13:11:49] ================================================================================
[13:11:49] TRIAL 61 - Starting
[13:11:49] ================================================================================
[13:11:49] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:49] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:49] ❌ Trial 61 failed with error: expected scalar type Long but found Float
[13:11:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:49] ✓ Results saved to hyperparameter_results.json
[13:11:49] ================================================================================
[13:11:49] TRIAL 62 - Starting
[13:11:49] ================================================================================
[13:11:49] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:11:49] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:11:49] ❌ Trial 62 failed with error: expected scalar type Long but found Float
[13:11:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:49] ✓ Results saved to hyperparameter_results.json
[13:11:49] ================================================================================
[13:11:49] TRIAL 63 - Starting
[13:11:49] ================================================================================
[13:11:49] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:11:49] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:11:49] ❌ Trial 63 failed with error: expected scalar type Long but found Float
[13:11:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:49] ✓ Results saved to hyperparameter_results.json
[13:11:49] ================================================================================
[13:11:49] TRIAL 64 - Starting
[13:11:49] ================================================================================
[13:11:49] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:11:49] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:11:49] ❌ Trial 64 failed with error: expected scalar type Long but found Float
[13:11:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:49] ✓ Results saved to hyperparameter_results.json
[13:11:49] ================================================================================
[13:11:49] TRIAL 65 - Starting
[13:11:49] ================================================================================
[13:11:49] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:11:49] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:11:49] ❌ Trial 65 failed with error: expected scalar type Long but found Float
[13:11:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:49] ✓ Results saved to hyperparameter_results.json
[13:11:49] 
================================================================================
[13:11:49] PROGRESS UPDATE - Completed 65 trials
[13:11:49] Best so far: 0.0000
[13:11:49] ================================================================================

[13:11:49] ================================================================================
[13:11:49] TRIAL 66 - Starting
[13:11:49] ================================================================================
[13:11:49] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:11:49] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:11:49] ❌ Trial 66 failed with error: expected scalar type Long but found Float
[13:11:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:49] ✓ Results saved to hyperparameter_results.json
[13:11:49] ================================================================================
[13:11:49] TRIAL 67 - Starting
[13:11:49] ================================================================================
[13:11:49] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:11:49] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:11:49] ❌ Trial 67 failed with error: expected scalar type Long but found Float
[13:11:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:49] ✓ Results saved to hyperparameter_results.json
[13:11:49] ================================================================================
[13:11:49] TRIAL 68 - Starting
[13:11:49] ================================================================================
[13:11:49] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:11:49] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:11:49] ❌ Trial 68 failed with error: expected scalar type Long but found Float
[13:11:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:49] ✓ Results saved to hyperparameter_results.json
[13:11:49] ================================================================================
[13:11:49] TRIAL 69 - Starting
[13:11:50] ================================================================================
[13:11:50] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:11:50] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:11:50] ❌ Trial 69 failed with error: expected scalar type Long but found Float
[13:11:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:50] ✓ Results saved to hyperparameter_results.json
[13:11:50] ================================================================================
[13:11:50] TRIAL 70 - Starting
[13:11:50] ================================================================================
[13:11:50] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:11:50] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:11:50] ❌ Trial 70 failed with error: expected scalar type Long but found Float
[13:11:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:50] ✓ Results saved to hyperparameter_results.json
[13:11:50] 
================================================================================
[13:11:50] PROGRESS UPDATE - Completed 70 trials
[13:11:50] Best so far: 0.0000
[13:11:50] ================================================================================

[13:11:50] ================================================================================
[13:11:50] TRIAL 71 - Starting
[13:11:50] ================================================================================
[13:11:50] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:11:50] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:11:50] ❌ Trial 71 failed with error: expected scalar type Long but found Float
[13:11:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:50] ✓ Results saved to hyperparameter_results.json
[13:11:50] ================================================================================
[13:11:50] TRIAL 72 - Starting
[13:11:50] ================================================================================
[13:11:50] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:11:50] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:11:50] ❌ Trial 72 failed with error: expected scalar type Long but found Float
[13:11:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:50] ✓ Results saved to hyperparameter_results.json
[13:11:50] ================================================================================
[13:11:50] TRIAL 73 - Starting
[13:11:50] ================================================================================
[13:11:50] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:11:50] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:11:50] ❌ Trial 73 failed with error: expected scalar type Long but found Float
[13:11:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:50] ✓ Results saved to hyperparameter_results.json
[13:11:50] ================================================================================
[13:11:50] TRIAL 74 - Starting
[13:11:50] ================================================================================
[13:11:50] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:50] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:50] ❌ Trial 74 failed with error: expected scalar type Long but found Float
[13:11:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:50] ✓ Results saved to hyperparameter_results.json
[13:11:50] ================================================================================
[13:11:50] TRIAL 75 - Starting
[13:11:50] ================================================================================
[13:11:50] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:11:50] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:11:50] ❌ Trial 75 failed with error: expected scalar type Long but found Float
[13:11:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:50] ✓ Results saved to hyperparameter_results.json
[13:11:50] 
================================================================================
[13:11:50] PROGRESS UPDATE - Completed 75 trials
[13:11:50] Best so far: 0.0000
[13:11:50] ================================================================================

[13:11:50] ================================================================================
[13:11:50] TRIAL 76 - Starting
[13:11:50] ================================================================================
[13:11:50] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:11:50] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:11:50] ❌ Trial 76 failed with error: expected scalar type Long but found Float
[13:11:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:50] ✓ Results saved to hyperparameter_results.json
[13:11:50] ================================================================================
[13:11:50] TRIAL 77 - Starting
[13:11:50] ================================================================================
[13:11:50] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:11:50] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:11:50] ❌ Trial 77 failed with error: expected scalar type Long but found Float
[13:11:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:50] ✓ Results saved to hyperparameter_results.json
[13:11:50] ================================================================================
[13:11:50] TRIAL 78 - Starting
[13:11:50] ================================================================================
[13:11:50] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:50] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:50] ❌ Trial 78 failed with error: expected scalar type Long but found Float
[13:11:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:50] ✓ Results saved to hyperparameter_results.json
[13:11:50] ================================================================================
[13:11:50] TRIAL 79 - Starting
[13:11:50] ================================================================================
[13:11:50] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:50] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:50] ❌ Trial 79 failed with error: expected scalar type Long but found Float
[13:11:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:50] ✓ Results saved to hyperparameter_results.json
[13:11:51] ================================================================================
[13:11:51] TRIAL 80 - Starting
[13:11:51] ================================================================================
[13:11:51] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:11:51] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:11:51] ❌ Trial 80 failed with error: expected scalar type Long but found Float
[13:11:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:51] ✓ Results saved to hyperparameter_results.json
[13:11:51] 
================================================================================
[13:11:51] PROGRESS UPDATE - Completed 80 trials
[13:11:51] Best so far: 0.0000
[13:11:51] ================================================================================

[13:11:51] ================================================================================
[13:11:51] TRIAL 81 - Starting
[13:11:51] ================================================================================
[13:11:51] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:11:51] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:11:51] ❌ Trial 81 failed with error: expected scalar type Long but found Float
[13:11:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:51] ✓ Results saved to hyperparameter_results.json
[13:11:51] ================================================================================
[13:11:51] TRIAL 82 - Starting
[13:11:51] ================================================================================
[13:11:51] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:11:51] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:11:51] ❌ Trial 82 failed with error: expected scalar type Long but found Float
[13:11:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:51] ✓ Results saved to hyperparameter_results.json
[13:11:51] ================================================================================
[13:11:51] TRIAL 83 - Starting
[13:11:51] ================================================================================
[13:11:51] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:11:51] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:11:51] ❌ Trial 83 failed with error: expected scalar type Long but found Float
[13:11:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:51] ✓ Results saved to hyperparameter_results.json
[13:11:51] ================================================================================
[13:11:51] TRIAL 84 - Starting
[13:11:51] ================================================================================
[13:11:51] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:51] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:51] ❌ Trial 84 failed with error: expected scalar type Long but found Float
[13:11:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:51] ✓ Results saved to hyperparameter_results.json
[13:11:51] ================================================================================
[13:11:51] TRIAL 85 - Starting
[13:11:51] ================================================================================
[13:11:51] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:11:51] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:11:51] ❌ Trial 85 failed with error: expected scalar type Long but found Float
[13:11:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:51] ✓ Results saved to hyperparameter_results.json
[13:11:51] 
================================================================================
[13:11:51] PROGRESS UPDATE - Completed 85 trials
[13:11:51] Best so far: 0.0000
[13:11:51] ================================================================================

[13:11:51] ================================================================================
[13:11:51] TRIAL 86 - Starting
[13:11:51] ================================================================================
[13:11:51] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:11:51] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:11:51] ❌ Trial 86 failed with error: expected scalar type Long but found Float
[13:11:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:51] ✓ Results saved to hyperparameter_results.json
[13:11:51] ================================================================================
[13:11:51] TRIAL 87 - Starting
[13:11:51] ================================================================================
[13:11:51] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:11:51] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:11:51] ❌ Trial 87 failed with error: expected scalar type Long but found Float
[13:11:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:51] ✓ Results saved to hyperparameter_results.json
[13:11:51] ================================================================================
[13:11:51] TRIAL 88 - Starting
[13:11:51] ================================================================================
[13:11:51] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:51] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:51] ❌ Trial 88 failed with error: expected scalar type Long but found Float
[13:11:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:51] ✓ Results saved to hyperparameter_results.json
[13:11:51] ================================================================================
[13:11:51] TRIAL 89 - Starting
[13:11:51] ================================================================================
[13:11:51] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:11:51] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:11:51] ❌ Trial 89 failed with error: expected scalar type Long but found Float
[13:11:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:51] ✓ Results saved to hyperparameter_results.json
[13:11:51] ================================================================================
[13:11:51] TRIAL 90 - Starting
[13:11:51] ================================================================================
[13:11:51] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:11:51] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:11:51] ❌ Trial 90 failed with error: expected scalar type Long but found Float
[13:11:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:52] ✓ Results saved to hyperparameter_results.json
[13:11:52] 
================================================================================
[13:11:52] PROGRESS UPDATE - Completed 90 trials
[13:11:52] Best so far: 0.0000
[13:11:52] ================================================================================

[13:11:52] ================================================================================
[13:11:52] TRIAL 91 - Starting
[13:11:52] ================================================================================
[13:11:52] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:11:52] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:11:52] ❌ Trial 91 failed with error: expected scalar type Long but found Float
[13:11:52] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:52] ✓ Results saved to hyperparameter_results.json
[13:11:52] ================================================================================
[13:11:52] TRIAL 92 - Starting
[13:11:52] ================================================================================
[13:11:52] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:52] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:52] ❌ Trial 92 failed with error: expected scalar type Long but found Float
[13:11:52] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:52] ✓ Results saved to hyperparameter_results.json
[13:11:52] ================================================================================
[13:11:52] TRIAL 93 - Starting
[13:11:52] ================================================================================
[13:11:52] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:52] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:52] ❌ Trial 93 failed with error: expected scalar type Long but found Float
[13:11:52] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:52] ✓ Results saved to hyperparameter_results.json
[13:11:52] ================================================================================
[13:11:52] TRIAL 94 - Starting
[13:11:52] ================================================================================
[13:11:52] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:11:52] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:11:52] ❌ Trial 94 failed with error: expected scalar type Long but found Float
[13:11:52] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:52] ✓ Results saved to hyperparameter_results.json
[13:11:52] ================================================================================
[13:11:52] TRIAL 95 - Starting
[13:11:52] ================================================================================
[13:11:52] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:11:52] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:11:52] ❌ Trial 95 failed with error: expected scalar type Long but found Float
[13:11:52] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:52] ✓ Results saved to hyperparameter_results.json
[13:11:52] 
================================================================================
[13:11:52] PROGRESS UPDATE - Completed 95 trials
[13:11:52] Best so far: 0.0000
[13:11:52] ================================================================================

[13:11:52] ================================================================================
[13:11:52] TRIAL 96 - Starting
[13:11:52] ================================================================================
[13:11:52] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:52] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:52] ❌ Trial 96 failed with error: expected scalar type Long but found Float
[13:11:52] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:52] ✓ Results saved to hyperparameter_results.json
[13:11:52] ================================================================================
[13:11:52] TRIAL 97 - Starting
[13:11:52] ================================================================================
[13:11:52] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:11:52] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:11:52] ❌ Trial 97 failed with error: expected scalar type Long but found Float
[13:11:52] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:52] ✓ Results saved to hyperparameter_results.json
[13:11:52] ================================================================================
[13:11:52] TRIAL 98 - Starting
[13:11:52] ================================================================================
[13:11:52] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:52] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:52] ❌ Trial 98 failed with error: expected scalar type Long but found Float
[13:11:52] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:52] ✓ Results saved to hyperparameter_results.json
[13:11:52] ================================================================================
[13:11:52] TRIAL 99 - Starting
[13:11:52] ================================================================================
[13:11:52] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:11:52] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:11:53] ❌ Trial 99 failed with error: expected scalar type Long but found Float
[13:11:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:53] ✓ Results saved to hyperparameter_results.json
[13:11:53] ================================================================================
[13:11:53] TRIAL 100 - Starting
[13:11:53] ================================================================================
[13:11:53] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:11:53] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:11:53] ❌ Trial 100 failed with error: expected scalar type Long but found Float
[13:11:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:53] ✓ Results saved to hyperparameter_results.json
[13:11:53] 
================================================================================
[13:11:53] PROGRESS UPDATE - Completed 100 trials
[13:11:53] Best so far: 0.0000
[13:11:53] ================================================================================

[13:11:53] ================================================================================
[13:11:53] TRIAL 101 - Starting
[13:11:53] ================================================================================
[13:11:53] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:11:53] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:11:53] ❌ Trial 101 failed with error: expected scalar type Long but found Float
[13:11:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:53] ✓ Results saved to hyperparameter_results.json
[13:11:53] ================================================================================
[13:11:53] TRIAL 102 - Starting
[13:11:53] ================================================================================
[13:11:53] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:53] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:53] ❌ Trial 102 failed with error: expected scalar type Long but found Float
[13:11:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:53] ✓ Results saved to hyperparameter_results.json
[13:11:53] ================================================================================
[13:11:53] TRIAL 103 - Starting
[13:11:53] ================================================================================
[13:11:53] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:11:53] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:11:53] ❌ Trial 103 failed with error: expected scalar type Long but found Float
[13:11:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:53] ✓ Results saved to hyperparameter_results.json
[13:11:53] ================================================================================
[13:11:53] TRIAL 104 - Starting
[13:11:53] ================================================================================
[13:11:53] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:53] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:53] ❌ Trial 104 failed with error: expected scalar type Long but found Float
[13:11:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:53] ✓ Results saved to hyperparameter_results.json
[13:11:53] ================================================================================
[13:11:53] TRIAL 105 - Starting
[13:11:53] ================================================================================
[13:11:53] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:53] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:53] ❌ Trial 105 failed with error: expected scalar type Long but found Float
[13:11:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:53] ✓ Results saved to hyperparameter_results.json
[13:11:53] 
================================================================================
[13:11:53] PROGRESS UPDATE - Completed 105 trials
[13:11:53] Best so far: 0.0000
[13:11:53] ================================================================================

[13:11:53] ================================================================================
[13:11:53] TRIAL 106 - Starting
[13:11:53] ================================================================================
[13:11:53] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:11:53] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:11:53] ❌ Trial 106 failed with error: expected scalar type Long but found Float
[13:11:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:53] ✓ Results saved to hyperparameter_results.json
[13:11:53] ================================================================================
[13:11:53] TRIAL 107 - Starting
[13:11:53] ================================================================================
[13:11:53] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:53] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:54] ❌ Trial 107 failed with error: expected scalar type Long but found Float
[13:11:54] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:54] ✓ Results saved to hyperparameter_results.json
[13:11:54] ================================================================================
[13:11:54] TRIAL 108 - Starting
[13:11:54] ================================================================================
[13:11:54] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:11:54] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:11:54] ❌ Trial 108 failed with error: expected scalar type Long but found Float
[13:11:54] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:54] ✓ Results saved to hyperparameter_results.json
[13:11:54] ================================================================================
[13:11:54] TRIAL 109 - Starting
[13:11:54] ================================================================================
[13:11:54] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:11:54] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:11:54] ❌ Trial 109 failed with error: expected scalar type Long but found Float
[13:11:54] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:54] ✓ Results saved to hyperparameter_results.json
[13:11:54] ================================================================================
[13:11:54] TRIAL 110 - Starting
[13:11:54] ================================================================================
[13:11:54] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:11:54] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:11:54] ❌ Trial 110 failed with error: expected scalar type Long but found Float
[13:11:54] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:54] ✓ Results saved to hyperparameter_results.json
[13:11:54] 
================================================================================
[13:11:54] PROGRESS UPDATE - Completed 110 trials
[13:11:54] Best so far: 0.0000
[13:11:54] ================================================================================

[13:11:54] ================================================================================
[13:11:54] TRIAL 111 - Starting
[13:11:54] ================================================================================
[13:11:54] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:11:54] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:11:54] ❌ Trial 111 failed with error: expected scalar type Long but found Float
[13:11:54] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:54] ✓ Results saved to hyperparameter_results.json
[13:11:54] ================================================================================
[13:11:54] TRIAL 112 - Starting
[13:11:54] ================================================================================
[13:11:54] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:11:54] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:11:54] ❌ Trial 112 failed with error: expected scalar type Long but found Float
[13:11:54] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:54] ✓ Results saved to hyperparameter_results.json
[13:11:54] ================================================================================
[13:11:54] TRIAL 113 - Starting
[13:11:54] ================================================================================
[13:11:54] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:11:54] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:11:54] ❌ Trial 113 failed with error: expected scalar type Long but found Float
[13:11:54] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:54] ✓ Results saved to hyperparameter_results.json
[13:11:54] ================================================================================
[13:11:54] TRIAL 114 - Starting
[13:11:54] ================================================================================
[13:11:54] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:11:54] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:11:54] ❌ Trial 114 failed with error: expected scalar type Long but found Float
[13:11:54] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:54] ✓ Results saved to hyperparameter_results.json
[13:11:54] ================================================================================
[13:11:54] TRIAL 115 - Starting
[13:11:54] ================================================================================
[13:11:54] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:11:54] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:11:54] ❌ Trial 115 failed with error: expected scalar type Long but found Float
[13:11:54] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:54] ✓ Results saved to hyperparameter_results.json
[13:11:54] 
================================================================================
[13:11:54] PROGRESS UPDATE - Completed 115 trials
[13:11:54] Best so far: 0.0000
[13:11:54] ================================================================================

[13:11:54] ================================================================================
[13:11:54] TRIAL 116 - Starting
[13:11:54] ================================================================================
[13:11:54] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:11:54] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:11:55] ❌ Trial 116 failed with error: expected scalar type Long but found Float
[13:11:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:55] ✓ Results saved to hyperparameter_results.json
[13:11:55] ================================================================================
[13:11:55] TRIAL 117 - Starting
[13:11:55] ================================================================================
[13:11:55] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:55] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:55] ❌ Trial 117 failed with error: expected scalar type Long but found Float
[13:11:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:55] ✓ Results saved to hyperparameter_results.json
[13:11:55] ================================================================================
[13:11:55] TRIAL 118 - Starting
[13:11:55] ================================================================================
[13:11:55] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:11:55] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:11:55] ❌ Trial 118 failed with error: expected scalar type Long but found Float
[13:11:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:55] ✓ Results saved to hyperparameter_results.json
[13:11:55] ================================================================================
[13:11:55] TRIAL 119 - Starting
[13:11:55] ================================================================================
[13:11:55] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:11:55] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:11:55] ❌ Trial 119 failed with error: expected scalar type Long but found Float
[13:11:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:55] ✓ Results saved to hyperparameter_results.json
[13:11:55] ================================================================================
[13:11:55] TRIAL 120 - Starting
[13:11:55] ================================================================================
[13:11:55] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:11:55] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:11:55] ❌ Trial 120 failed with error: expected scalar type Long but found Float
[13:11:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:55] ✓ Results saved to hyperparameter_results.json
[13:11:55] 
================================================================================
[13:11:55] PROGRESS UPDATE - Completed 120 trials
[13:11:55] Best so far: 0.0000
[13:11:55] ================================================================================

[13:11:55] ================================================================================
[13:11:55] TRIAL 121 - Starting
[13:11:55] ================================================================================
[13:11:55] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:55] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:55] ❌ Trial 121 failed with error: expected scalar type Long but found Float
[13:11:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:55] ✓ Results saved to hyperparameter_results.json
[13:11:55] ================================================================================
[13:11:55] TRIAL 122 - Starting
[13:11:55] ================================================================================
[13:11:55] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:11:55] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:11:55] ❌ Trial 122 failed with error: expected scalar type Long but found Float
[13:11:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:55] ✓ Results saved to hyperparameter_results.json
[13:11:55] ================================================================================
[13:11:55] TRIAL 123 - Starting
[13:11:55] ================================================================================
[13:11:55] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:11:55] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:11:55] ❌ Trial 123 failed with error: expected scalar type Long but found Float
[13:11:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:55] ✓ Results saved to hyperparameter_results.json
[13:11:55] ================================================================================
[13:11:55] TRIAL 124 - Starting
[13:11:55] ================================================================================
[13:11:55] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:11:55] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:11:55] ❌ Trial 124 failed with error: expected scalar type Long but found Float
[13:11:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:55] ✓ Results saved to hyperparameter_results.json
[13:11:55] ================================================================================
[13:11:55] TRIAL 125 - Starting
[13:11:55] ================================================================================
[13:11:55] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:55] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:55] ❌ Trial 125 failed with error: expected scalar type Long but found Float
[13:11:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:56] ✓ Results saved to hyperparameter_results.json
[13:11:56] 
================================================================================
[13:11:56] PROGRESS UPDATE - Completed 125 trials
[13:11:56] Best so far: 0.0000
[13:11:56] ================================================================================

[13:11:56] ================================================================================
[13:11:56] TRIAL 126 - Starting
[13:11:56] ================================================================================
[13:11:56] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:11:56] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:11:56] ❌ Trial 126 failed with error: expected scalar type Long but found Float
[13:11:56] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:56] ✓ Results saved to hyperparameter_results.json
[13:11:56] ================================================================================
[13:11:56] TRIAL 127 - Starting
[13:11:56] ================================================================================
[13:11:56] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:56] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:56] ❌ Trial 127 failed with error: expected scalar type Long but found Float
[13:11:56] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:56] ✓ Results saved to hyperparameter_results.json
[13:11:56] ================================================================================
[13:11:56] TRIAL 128 - Starting
[13:11:56] ================================================================================
[13:11:56] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:56] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:56] ❌ Trial 128 failed with error: expected scalar type Long but found Float
[13:11:56] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:56] ✓ Results saved to hyperparameter_results.json
[13:11:56] ================================================================================
[13:11:56] TRIAL 129 - Starting
[13:11:56] ================================================================================
[13:11:56] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:56] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:56] ❌ Trial 129 failed with error: expected scalar type Long but found Float
[13:11:56] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:56] ✓ Results saved to hyperparameter_results.json
[13:11:56] ================================================================================
[13:11:56] TRIAL 130 - Starting
[13:11:56] ================================================================================
[13:11:56] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:11:56] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:11:56] ❌ Trial 130 failed with error: expected scalar type Long but found Float
[13:11:56] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:56] ✓ Results saved to hyperparameter_results.json
[13:11:56] 
================================================================================
[13:11:56] PROGRESS UPDATE - Completed 130 trials
[13:11:56] Best so far: 0.0000
[13:11:56] ================================================================================

[13:11:56] ================================================================================
[13:11:56] TRIAL 131 - Starting
[13:11:56] ================================================================================
[13:11:56] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:56] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:56] ❌ Trial 131 failed with error: expected scalar type Long but found Float
[13:11:56] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:56] ✓ Results saved to hyperparameter_results.json
[13:11:56] ================================================================================
[13:11:56] TRIAL 132 - Starting
[13:11:56] ================================================================================
[13:11:56] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:11:56] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:11:56] ❌ Trial 132 failed with error: expected scalar type Long but found Float
[13:11:56] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:56] ✓ Results saved to hyperparameter_results.json
[13:11:56] ================================================================================
[13:11:56] TRIAL 133 - Starting
[13:11:56] ================================================================================
[13:11:56] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:11:56] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:11:56] ❌ Trial 133 failed with error: expected scalar type Long but found Float
[13:11:56] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:57] ✓ Results saved to hyperparameter_results.json
[13:11:57] ================================================================================
[13:11:57] TRIAL 134 - Starting
[13:11:57] ================================================================================
[13:11:57] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:11:57] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:11:57] ❌ Trial 134 failed with error: expected scalar type Long but found Float
[13:11:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:57] ✓ Results saved to hyperparameter_results.json
[13:11:57] ================================================================================
[13:11:57] TRIAL 135 - Starting
[13:11:57] ================================================================================
[13:11:57] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:57] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:57] ❌ Trial 135 failed with error: expected scalar type Long but found Float
[13:11:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:57] ✓ Results saved to hyperparameter_results.json
[13:11:57] 
================================================================================
[13:11:57] PROGRESS UPDATE - Completed 135 trials
[13:11:57] Best so far: 0.0000
[13:11:57] ================================================================================

[13:11:57] ================================================================================
[13:11:57] TRIAL 136 - Starting
[13:11:57] ================================================================================
[13:11:57] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:11:57] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:11:57] ❌ Trial 136 failed with error: expected scalar type Long but found Float
[13:11:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:57] ✓ Results saved to hyperparameter_results.json
[13:11:57] ================================================================================
[13:11:57] TRIAL 137 - Starting
[13:11:57] ================================================================================
[13:11:57] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:11:57] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:11:57] ❌ Trial 137 failed with error: expected scalar type Long but found Float
[13:11:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:57] ✓ Results saved to hyperparameter_results.json
[13:11:57] ================================================================================
[13:11:57] TRIAL 138 - Starting
[13:11:57] ================================================================================
[13:11:57] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:57] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:57] ❌ Trial 138 failed with error: expected scalar type Long but found Float
[13:11:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:57] ✓ Results saved to hyperparameter_results.json
[13:11:57] ================================================================================
[13:11:57] TRIAL 139 - Starting
[13:11:57] ================================================================================
[13:11:57] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:11:57] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:11:57] ❌ Trial 139 failed with error: expected scalar type Long but found Float
[13:11:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:57] ✓ Results saved to hyperparameter_results.json
[13:11:57] ================================================================================
[13:11:57] TRIAL 140 - Starting
[13:11:57] ================================================================================
[13:11:57] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:57] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:57] ❌ Trial 140 failed with error: expected scalar type Long but found Float
[13:11:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:57] ✓ Results saved to hyperparameter_results.json
[13:11:57] 
================================================================================
[13:11:57] PROGRESS UPDATE - Completed 140 trials
[13:11:57] Best so far: 0.0000
[13:11:57] ================================================================================

[13:11:57] ================================================================================
[13:11:57] TRIAL 141 - Starting
[13:11:57] ================================================================================
[13:11:57] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:57] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:57] ❌ Trial 141 failed with error: expected scalar type Long but found Float
[13:11:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:57] ✓ Results saved to hyperparameter_results.json
[13:11:57] ================================================================================
[13:11:57] TRIAL 142 - Starting
[13:11:57] ================================================================================
[13:11:57] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:11:57] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:11:57] ❌ Trial 142 failed with error: expected scalar type Long but found Float
[13:11:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:57] ✓ Results saved to hyperparameter_results.json
[13:11:57] ================================================================================
[13:11:57] TRIAL 143 - Starting
[13:11:57] ================================================================================
[13:11:57] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:11:57] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:11:57] ❌ Trial 143 failed with error: expected scalar type Long but found Float
[13:11:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:58] ✓ Results saved to hyperparameter_results.json
[13:11:58] ================================================================================
[13:11:58] TRIAL 144 - Starting
[13:11:58] ================================================================================
[13:11:58] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:11:58] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:11:58] ❌ Trial 144 failed with error: expected scalar type Long but found Float
[13:11:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:58] ✓ Results saved to hyperparameter_results.json
[13:11:58] ================================================================================
[13:11:58] TRIAL 145 - Starting
[13:11:58] ================================================================================
[13:11:58] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:11:58] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:11:58] ❌ Trial 145 failed with error: expected scalar type Long but found Float
[13:11:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:58] ✓ Results saved to hyperparameter_results.json
[13:11:58] 
================================================================================
[13:11:58] PROGRESS UPDATE - Completed 145 trials
[13:11:58] Best so far: 0.0000
[13:11:58] ================================================================================

[13:11:58] ================================================================================
[13:11:58] TRIAL 146 - Starting
[13:11:58] ================================================================================
[13:11:58] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:11:58] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:11:58] ❌ Trial 146 failed with error: expected scalar type Long but found Float
[13:11:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:58] ✓ Results saved to hyperparameter_results.json
[13:11:58] ================================================================================
[13:11:58] TRIAL 147 - Starting
[13:11:58] ================================================================================
[13:11:58] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:11:58] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:11:58] ❌ Trial 147 failed with error: expected scalar type Long but found Float
[13:11:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:58] ✓ Results saved to hyperparameter_results.json
[13:11:58] ================================================================================
[13:11:58] TRIAL 148 - Starting
[13:11:58] ================================================================================
[13:11:58] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:11:58] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:11:58] ❌ Trial 148 failed with error: expected scalar type Long but found Float
[13:11:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:58] ✓ Results saved to hyperparameter_results.json
[13:11:58] ================================================================================
[13:11:58] TRIAL 149 - Starting
[13:11:58] ================================================================================
[13:11:58] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:11:58] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:11:58] ❌ Trial 149 failed with error: expected scalar type Long but found Float
[13:11:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:58] ✓ Results saved to hyperparameter_results.json
[13:11:58] ================================================================================
[13:11:58] TRIAL 150 - Starting
[13:11:58] ================================================================================
[13:11:58] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:11:58] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:11:58] ❌ Trial 150 failed with error: expected scalar type Long but found Float
[13:11:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:58] ✓ Results saved to hyperparameter_results.json
[13:11:58] 
================================================================================
[13:11:58] PROGRESS UPDATE - Completed 150 trials
[13:11:58] Best so far: 0.0000
[13:11:58] ================================================================================

[13:11:58] ================================================================================
[13:11:58] TRIAL 151 - Starting
[13:11:58] ================================================================================
[13:11:58] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:11:58] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:11:58] ❌ Trial 151 failed with error: expected scalar type Long but found Float
[13:11:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:58] ✓ Results saved to hyperparameter_results.json
[13:11:58] ================================================================================
[13:11:58] TRIAL 152 - Starting
[13:11:58] ================================================================================
[13:11:58] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:11:58] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:11:59] ❌ Trial 152 failed with error: expected scalar type Long but found Float
[13:11:59] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:59] ✓ Results saved to hyperparameter_results.json
[13:11:59] ================================================================================
[13:11:59] TRIAL 153 - Starting
[13:11:59] ================================================================================
[13:11:59] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:11:59] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:11:59] ❌ Trial 153 failed with error: expected scalar type Long but found Float
[13:11:59] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:59] ✓ Results saved to hyperparameter_results.json
[13:11:59] ================================================================================
[13:11:59] TRIAL 154 - Starting
[13:11:59] ================================================================================
[13:11:59] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:59] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:11:59] ❌ Trial 154 failed with error: expected scalar type Long but found Float
[13:11:59] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:59] ✓ Results saved to hyperparameter_results.json
[13:11:59] ================================================================================
[13:11:59] TRIAL 155 - Starting
[13:11:59] ================================================================================
[13:11:59] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:11:59] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:11:59] ❌ Trial 155 failed with error: expected scalar type Long but found Float
[13:11:59] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:59] ✓ Results saved to hyperparameter_results.json
[13:11:59] 
================================================================================
[13:11:59] PROGRESS UPDATE - Completed 155 trials
[13:11:59] Best so far: 0.0000
[13:11:59] ================================================================================

[13:11:59] ================================================================================
[13:11:59] TRIAL 156 - Starting
[13:11:59] ================================================================================
[13:11:59] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:11:59] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:11:59] ❌ Trial 156 failed with error: expected scalar type Long but found Float
[13:11:59] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:59] ✓ Results saved to hyperparameter_results.json
[13:11:59] ================================================================================
[13:11:59] TRIAL 157 - Starting
[13:11:59] ================================================================================
[13:11:59] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:11:59] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:11:59] ❌ Trial 157 failed with error: expected scalar type Long but found Float
[13:11:59] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:59] ✓ Results saved to hyperparameter_results.json
[13:11:59] ================================================================================
[13:11:59] TRIAL 158 - Starting
[13:11:59] ================================================================================
[13:11:59] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:11:59] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:11:59] ❌ Trial 158 failed with error: expected scalar type Long but found Float
[13:11:59] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:59] ✓ Results saved to hyperparameter_results.json
[13:11:59] ================================================================================
[13:11:59] TRIAL 159 - Starting
[13:11:59] ================================================================================
[13:11:59] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:11:59] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:11:59] ❌ Trial 159 failed with error: expected scalar type Long but found Float
[13:11:59] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:11:59] ✓ Results saved to hyperparameter_results.json
[13:11:59] ================================================================================
[13:11:59] TRIAL 160 - Starting
[13:11:59] ================================================================================
[13:11:59] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:11:59] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:00] ❌ Trial 160 failed with error: expected scalar type Long but found Float
[13:12:00] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:00] ✓ Results saved to hyperparameter_results.json
[13:12:00] 
================================================================================
[13:12:00] PROGRESS UPDATE - Completed 160 trials
[13:12:00] Best so far: 0.0000
[13:12:00] ================================================================================

[13:12:00] ================================================================================
[13:12:00] TRIAL 161 - Starting
[13:12:00] ================================================================================
[13:12:00] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:00] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:00] ❌ Trial 161 failed with error: expected scalar type Long but found Float
[13:12:00] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:00] ✓ Results saved to hyperparameter_results.json
[13:12:00] ================================================================================
[13:12:00] TRIAL 162 - Starting
[13:12:00] ================================================================================
[13:12:00] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:00] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:00] ❌ Trial 162 failed with error: expected scalar type Long but found Float
[13:12:00] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:00] ✓ Results saved to hyperparameter_results.json
[13:12:00] ================================================================================
[13:12:00] TRIAL 163 - Starting
[13:12:00] ================================================================================
[13:12:00] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:00] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:00] ❌ Trial 163 failed with error: expected scalar type Long but found Float
[13:12:00] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:00] ✓ Results saved to hyperparameter_results.json
[13:12:00] ================================================================================
[13:12:00] TRIAL 164 - Starting
[13:12:00] ================================================================================
[13:12:00] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:00] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:00] ❌ Trial 164 failed with error: expected scalar type Long but found Float
[13:12:00] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:00] ✓ Results saved to hyperparameter_results.json
[13:12:00] ================================================================================
[13:12:00] TRIAL 165 - Starting
[13:12:00] ================================================================================
[13:12:00] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:00] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:00] ❌ Trial 165 failed with error: expected scalar type Long but found Float
[13:12:00] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:00] ✓ Results saved to hyperparameter_results.json
[13:12:00] 
================================================================================
[13:12:00] PROGRESS UPDATE - Completed 165 trials
[13:12:00] Best so far: 0.0000
[13:12:00] ================================================================================

[13:12:00] ================================================================================
[13:12:00] TRIAL 166 - Starting
[13:12:00] ================================================================================
[13:12:00] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:00] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:00] ❌ Trial 166 failed with error: expected scalar type Long but found Float
[13:12:00] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:00] ✓ Results saved to hyperparameter_results.json
[13:12:00] ================================================================================
[13:12:00] TRIAL 167 - Starting
[13:12:00] ================================================================================
[13:12:00] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:00] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:00] ❌ Trial 167 failed with error: expected scalar type Long but found Float
[13:12:00] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:00] ✓ Results saved to hyperparameter_results.json
[13:12:00] ================================================================================
[13:12:00] TRIAL 168 - Starting
[13:12:00] ================================================================================
[13:12:00] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:00] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:01] ❌ Trial 168 failed with error: expected scalar type Long but found Float
[13:12:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:01] ✓ Results saved to hyperparameter_results.json
[13:12:01] ================================================================================
[13:12:01] TRIAL 169 - Starting
[13:12:01] ================================================================================
[13:12:01] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:01] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:01] ❌ Trial 169 failed with error: expected scalar type Long but found Float
[13:12:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:01] ✓ Results saved to hyperparameter_results.json
[13:12:01] ================================================================================
[13:12:01] TRIAL 170 - Starting
[13:12:01] ================================================================================
[13:12:01] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:01] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:01] ❌ Trial 170 failed with error: expected scalar type Long but found Float
[13:12:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:01] ✓ Results saved to hyperparameter_results.json
[13:12:01] 
================================================================================
[13:12:01] PROGRESS UPDATE - Completed 170 trials
[13:12:01] Best so far: 0.0000
[13:12:01] ================================================================================

[13:12:01] ================================================================================
[13:12:01] TRIAL 171 - Starting
[13:12:01] ================================================================================
[13:12:01] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:01] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:01] ❌ Trial 171 failed with error: expected scalar type Long but found Float
[13:12:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:01] ✓ Results saved to hyperparameter_results.json
[13:12:01] ================================================================================
[13:12:01] TRIAL 172 - Starting
[13:12:01] ================================================================================
[13:12:01] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:01] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:01] ❌ Trial 172 failed with error: expected scalar type Long but found Float
[13:12:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:01] ✓ Results saved to hyperparameter_results.json
[13:12:01] ================================================================================
[13:12:01] TRIAL 173 - Starting
[13:12:01] ================================================================================
[13:12:01] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:01] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:01] ❌ Trial 173 failed with error: expected scalar type Long but found Float
[13:12:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:01] ✓ Results saved to hyperparameter_results.json
[13:12:01] ================================================================================
[13:12:01] TRIAL 174 - Starting
[13:12:01] ================================================================================
[13:12:01] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:01] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:01] ❌ Trial 174 failed with error: expected scalar type Long but found Float
[13:12:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:01] ✓ Results saved to hyperparameter_results.json
[13:12:01] ================================================================================
[13:12:01] TRIAL 175 - Starting
[13:12:01] ================================================================================
[13:12:01] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:01] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:01] ❌ Trial 175 failed with error: expected scalar type Long but found Float
[13:12:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:01] ✓ Results saved to hyperparameter_results.json
[13:12:01] 
================================================================================
[13:12:01] PROGRESS UPDATE - Completed 175 trials
[13:12:01] Best so far: 0.0000
[13:12:01] ================================================================================

[13:12:01] ================================================================================
[13:12:01] TRIAL 176 - Starting
[13:12:01] ================================================================================
[13:12:01] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:01] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:01] ❌ Trial 176 failed with error: expected scalar type Long but found Float
[13:12:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:02] ✓ Results saved to hyperparameter_results.json
[13:12:02] ================================================================================
[13:12:02] TRIAL 177 - Starting
[13:12:02] ================================================================================
[13:12:02] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:02] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:02] ❌ Trial 177 failed with error: expected scalar type Long but found Float
[13:12:02] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:02] ✓ Results saved to hyperparameter_results.json
[13:12:02] ================================================================================
[13:12:02] TRIAL 178 - Starting
[13:12:02] ================================================================================
[13:12:02] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:02] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:02] ❌ Trial 178 failed with error: expected scalar type Long but found Float
[13:12:02] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:02] ✓ Results saved to hyperparameter_results.json
[13:12:02] ================================================================================
[13:12:02] TRIAL 179 - Starting
[13:12:02] ================================================================================
[13:12:02] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:02] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:02] ❌ Trial 179 failed with error: expected scalar type Long but found Float
[13:12:02] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:02] ✓ Results saved to hyperparameter_results.json
[13:12:02] ================================================================================
[13:12:02] TRIAL 180 - Starting
[13:12:02] ================================================================================
[13:12:02] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:02] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:02] ❌ Trial 180 failed with error: expected scalar type Long but found Float
[13:12:02] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:02] ✓ Results saved to hyperparameter_results.json
[13:12:02] 
================================================================================
[13:12:02] PROGRESS UPDATE - Completed 180 trials
[13:12:02] Best so far: 0.0000
[13:12:02] ================================================================================

[13:12:02] ================================================================================
[13:12:02] TRIAL 181 - Starting
[13:12:02] ================================================================================
[13:12:02] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:02] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:02] ❌ Trial 181 failed with error: expected scalar type Long but found Float
[13:12:02] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:02] ✓ Results saved to hyperparameter_results.json
[13:12:02] ================================================================================
[13:12:02] TRIAL 182 - Starting
[13:12:02] ================================================================================
[13:12:02] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:02] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:02] ❌ Trial 182 failed with error: expected scalar type Long but found Float
[13:12:02] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:02] ✓ Results saved to hyperparameter_results.json
[13:12:02] ================================================================================
[13:12:02] TRIAL 183 - Starting
[13:12:02] ================================================================================
[13:12:02] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:02] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:02] ❌ Trial 183 failed with error: expected scalar type Long but found Float
[13:12:02] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:02] ✓ Results saved to hyperparameter_results.json
[13:12:02] ================================================================================
[13:12:02] TRIAL 184 - Starting
[13:12:02] ================================================================================
[13:12:02] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:02] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:02] ❌ Trial 184 failed with error: expected scalar type Long but found Float
[13:12:02] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:02] ✓ Results saved to hyperparameter_results.json
[13:12:02] ================================================================================
[13:12:02] TRIAL 185 - Starting
[13:12:02] ================================================================================
[13:12:02] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:02] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:02] ❌ Trial 185 failed with error: expected scalar type Long but found Float
[13:12:02] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:02] ✓ Results saved to hyperparameter_results.json
[13:12:02] 
================================================================================
[13:12:02] PROGRESS UPDATE - Completed 185 trials
[13:12:02] Best so far: 0.0000
[13:12:02] ================================================================================

[13:12:02] ================================================================================
[13:12:02] TRIAL 186 - Starting
[13:12:02] ================================================================================
[13:12:02] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:03] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:03] ❌ Trial 186 failed with error: expected scalar type Long but found Float
[13:12:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:03] ✓ Results saved to hyperparameter_results.json
[13:12:03] ================================================================================
[13:12:03] TRIAL 187 - Starting
[13:12:03] ================================================================================
[13:12:03] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:03] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:03] ❌ Trial 187 failed with error: expected scalar type Long but found Float
[13:12:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:03] ✓ Results saved to hyperparameter_results.json
[13:12:03] ================================================================================
[13:12:03] TRIAL 188 - Starting
[13:12:03] ================================================================================
[13:12:03] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:03] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:03] ❌ Trial 188 failed with error: expected scalar type Long but found Float
[13:12:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:03] ✓ Results saved to hyperparameter_results.json
[13:12:03] ================================================================================
[13:12:03] TRIAL 189 - Starting
[13:12:03] ================================================================================
[13:12:03] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:03] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:03] ❌ Trial 189 failed with error: expected scalar type Long but found Float
[13:12:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:03] ✓ Results saved to hyperparameter_results.json
[13:12:03] ================================================================================
[13:12:03] TRIAL 190 - Starting
[13:12:03] ================================================================================
[13:12:03] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:03] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:03] ❌ Trial 190 failed with error: expected scalar type Long but found Float
[13:12:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:03] ✓ Results saved to hyperparameter_results.json
[13:12:03] 
================================================================================
[13:12:03] PROGRESS UPDATE - Completed 190 trials
[13:12:03] Best so far: 0.0000
[13:12:03] ================================================================================

[13:12:03] ================================================================================
[13:12:03] TRIAL 191 - Starting
[13:12:03] ================================================================================
[13:12:03] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:03] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:03] ❌ Trial 191 failed with error: expected scalar type Long but found Float
[13:12:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:03] ✓ Results saved to hyperparameter_results.json
[13:12:03] ================================================================================
[13:12:03] TRIAL 192 - Starting
[13:12:03] ================================================================================
[13:12:03] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:03] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:03] ❌ Trial 192 failed with error: expected scalar type Long but found Float
[13:12:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:03] ✓ Results saved to hyperparameter_results.json
[13:12:03] ================================================================================
[13:12:03] TRIAL 193 - Starting
[13:12:03] ================================================================================
[13:12:03] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:03] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:03] ❌ Trial 193 failed with error: expected scalar type Long but found Float
[13:12:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:03] ✓ Results saved to hyperparameter_results.json
[13:12:03] ================================================================================
[13:12:03] TRIAL 194 - Starting
[13:12:03] ================================================================================
[13:12:03] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:03] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:03] ❌ Trial 194 failed with error: expected scalar type Long but found Float
[13:12:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:03] ✓ Results saved to hyperparameter_results.json
[13:12:03] ================================================================================
[13:12:03] TRIAL 195 - Starting
[13:12:03] ================================================================================
[13:12:03] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:03] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:04] ❌ Trial 195 failed with error: expected scalar type Long but found Float
[13:12:04] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:04] ✓ Results saved to hyperparameter_results.json
[13:12:04] 
================================================================================
[13:12:04] PROGRESS UPDATE - Completed 195 trials
[13:12:04] Best so far: 0.0000
[13:12:04] ================================================================================

[13:12:04] ================================================================================
[13:12:04] TRIAL 196 - Starting
[13:12:04] ================================================================================
[13:12:04] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:04] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:04] ❌ Trial 196 failed with error: expected scalar type Long but found Float
[13:12:04] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:04] ✓ Results saved to hyperparameter_results.json
[13:12:04] ================================================================================
[13:12:04] TRIAL 197 - Starting
[13:12:04] ================================================================================
[13:12:04] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:04] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:04] ❌ Trial 197 failed with error: expected scalar type Long but found Float
[13:12:04] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:04] ✓ Results saved to hyperparameter_results.json
[13:12:04] ================================================================================
[13:12:04] TRIAL 198 - Starting
[13:12:04] ================================================================================
[13:12:04] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:04] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:04] ❌ Trial 198 failed with error: expected scalar type Long but found Float
[13:12:04] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:04] ✓ Results saved to hyperparameter_results.json
[13:12:04] ================================================================================
[13:12:04] TRIAL 199 - Starting
[13:12:04] ================================================================================
[13:12:04] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:04] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:04] ❌ Trial 199 failed with error: expected scalar type Long but found Float
[13:12:04] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:04] ✓ Results saved to hyperparameter_results.json
[13:12:04] ================================================================================
[13:12:04] TRIAL 200 - Starting
[13:12:04] ================================================================================
[13:12:04] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:04] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:04] ❌ Trial 200 failed with error: expected scalar type Long but found Float
[13:12:04] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:04] ✓ Results saved to hyperparameter_results.json
[13:12:04] 
================================================================================
[13:12:04] PROGRESS UPDATE - Completed 200 trials
[13:12:04] Best so far: 0.0000
[13:12:04] ================================================================================

[13:12:04] ================================================================================
[13:12:04] TRIAL 201 - Starting
[13:12:04] ================================================================================
[13:12:04] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:04] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:04] ❌ Trial 201 failed with error: expected scalar type Long but found Float
[13:12:04] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:04] ✓ Results saved to hyperparameter_results.json
[13:12:04] ================================================================================
[13:12:04] TRIAL 202 - Starting
[13:12:04] ================================================================================
[13:12:04] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:04] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:04] ❌ Trial 202 failed with error: expected scalar type Long but found Float
[13:12:04] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:04] ✓ Results saved to hyperparameter_results.json
[13:12:04] ================================================================================
[13:12:04] TRIAL 203 - Starting
[13:12:04] ================================================================================
[13:12:04] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:04] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:05] ❌ Trial 203 failed with error: expected scalar type Long but found Float
[13:12:05] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:05] ✓ Results saved to hyperparameter_results.json
[13:12:05] ================================================================================
[13:12:05] TRIAL 204 - Starting
[13:12:05] ================================================================================
[13:12:05] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:05] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:05] ❌ Trial 204 failed with error: expected scalar type Long but found Float
[13:12:05] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:05] ✓ Results saved to hyperparameter_results.json
[13:12:05] ================================================================================
[13:12:05] TRIAL 205 - Starting
[13:12:05] ================================================================================
[13:12:05] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:05] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:05] ❌ Trial 205 failed with error: expected scalar type Long but found Float
[13:12:05] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:05] ✓ Results saved to hyperparameter_results.json
[13:12:05] 
================================================================================
[13:12:05] PROGRESS UPDATE - Completed 205 trials
[13:12:05] Best so far: 0.0000
[13:12:05] ================================================================================

[13:12:05] ================================================================================
[13:12:05] TRIAL 206 - Starting
[13:12:05] ================================================================================
[13:12:05] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:05] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:05] ❌ Trial 206 failed with error: expected scalar type Long but found Float
[13:12:05] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:05] ✓ Results saved to hyperparameter_results.json
[13:12:05] ================================================================================
[13:12:05] TRIAL 207 - Starting
[13:12:05] ================================================================================
[13:12:05] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:05] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:05] ❌ Trial 207 failed with error: expected scalar type Long but found Float
[13:12:05] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:05] ✓ Results saved to hyperparameter_results.json
[13:12:05] ================================================================================
[13:12:05] TRIAL 208 - Starting
[13:12:05] ================================================================================
[13:12:05] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:05] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:05] ❌ Trial 208 failed with error: expected scalar type Long but found Float
[13:12:05] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:05] ✓ Results saved to hyperparameter_results.json
[13:12:05] ================================================================================
[13:12:05] TRIAL 209 - Starting
[13:12:05] ================================================================================
[13:12:05] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:05] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:05] ❌ Trial 209 failed with error: expected scalar type Long but found Float
[13:12:05] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:05] ✓ Results saved to hyperparameter_results.json
[13:12:05] ================================================================================
[13:12:05] TRIAL 210 - Starting
[13:12:05] ================================================================================
[13:12:05] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:05] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:05] ❌ Trial 210 failed with error: expected scalar type Long but found Float
[13:12:05] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:05] ✓ Results saved to hyperparameter_results.json
[13:12:05] 
================================================================================
[13:12:05] PROGRESS UPDATE - Completed 210 trials
[13:12:05] Best so far: 0.0000
[13:12:05] ================================================================================

[13:12:05] ================================================================================
[13:12:05] TRIAL 211 - Starting
[13:12:05] ================================================================================
[13:12:05] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:05] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:05] ❌ Trial 211 failed with error: expected scalar type Long but found Float
[13:12:05] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:05] ✓ Results saved to hyperparameter_results.json
[13:12:05] ================================================================================
[13:12:05] TRIAL 212 - Starting
[13:12:05] ================================================================================
[13:12:05] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:05] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:06] ❌ Trial 212 failed with error: expected scalar type Long but found Float
[13:12:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:06] ✓ Results saved to hyperparameter_results.json
[13:12:06] ================================================================================
[13:12:06] TRIAL 213 - Starting
[13:12:06] ================================================================================
[13:12:06] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:06] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:06] ❌ Trial 213 failed with error: expected scalar type Long but found Float
[13:12:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:06] ✓ Results saved to hyperparameter_results.json
[13:12:06] ================================================================================
[13:12:06] TRIAL 214 - Starting
[13:12:06] ================================================================================
[13:12:06] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:06] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:06] ❌ Trial 214 failed with error: expected scalar type Long but found Float
[13:12:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:06] ✓ Results saved to hyperparameter_results.json
[13:12:06] ================================================================================
[13:12:06] TRIAL 215 - Starting
[13:12:06] ================================================================================
[13:12:06] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:06] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:06] ❌ Trial 215 failed with error: expected scalar type Long but found Float
[13:12:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:06] ✓ Results saved to hyperparameter_results.json
[13:12:06] 
================================================================================
[13:12:06] PROGRESS UPDATE - Completed 215 trials
[13:12:06] Best so far: 0.0000
[13:12:06] ================================================================================

[13:12:06] ================================================================================
[13:12:06] TRIAL 216 - Starting
[13:12:06] ================================================================================
[13:12:06] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:06] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:06] ❌ Trial 216 failed with error: expected scalar type Long but found Float
[13:12:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:06] ✓ Results saved to hyperparameter_results.json
[13:12:06] ================================================================================
[13:12:06] TRIAL 217 - Starting
[13:12:06] ================================================================================
[13:12:06] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:06] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:06] ❌ Trial 217 failed with error: expected scalar type Long but found Float
[13:12:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:06] ✓ Results saved to hyperparameter_results.json
[13:12:06] ================================================================================
[13:12:06] TRIAL 218 - Starting
[13:12:06] ================================================================================
[13:12:06] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:06] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:06] ❌ Trial 218 failed with error: expected scalar type Long but found Float
[13:12:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:06] ✓ Results saved to hyperparameter_results.json
[13:12:06] ================================================================================
[13:12:06] TRIAL 219 - Starting
[13:12:06] ================================================================================
[13:12:06] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:06] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:06] ❌ Trial 219 failed with error: expected scalar type Long but found Float
[13:12:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:06] ✓ Results saved to hyperparameter_results.json
[13:12:06] ================================================================================
[13:12:06] TRIAL 220 - Starting
[13:12:06] ================================================================================
[13:12:06] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:06] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:07] ❌ Trial 220 failed with error: expected scalar type Long but found Float
[13:12:07] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:07] ✓ Results saved to hyperparameter_results.json
[13:12:07] 
================================================================================
[13:12:07] PROGRESS UPDATE - Completed 220 trials
[13:12:07] Best so far: 0.0000
[13:12:07] ================================================================================

[13:12:07] ================================================================================
[13:12:07] TRIAL 221 - Starting
[13:12:07] ================================================================================
[13:12:07] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:07] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:07] ❌ Trial 221 failed with error: expected scalar type Long but found Float
[13:12:07] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:07] ✓ Results saved to hyperparameter_results.json
[13:12:07] ================================================================================
[13:12:07] TRIAL 222 - Starting
[13:12:07] ================================================================================
[13:12:07] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:07] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:07] ❌ Trial 222 failed with error: expected scalar type Long but found Float
[13:12:07] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:07] ✓ Results saved to hyperparameter_results.json
[13:12:07] ================================================================================
[13:12:07] TRIAL 223 - Starting
[13:12:07] ================================================================================
[13:12:07] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:07] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:07] ❌ Trial 223 failed with error: expected scalar type Long but found Float
[13:12:07] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:07] ✓ Results saved to hyperparameter_results.json
[13:12:07] ================================================================================
[13:12:07] TRIAL 224 - Starting
[13:12:07] ================================================================================
[13:12:07] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:07] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:07] ❌ Trial 224 failed with error: expected scalar type Long but found Float
[13:12:07] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:07] ✓ Results saved to hyperparameter_results.json
[13:12:07] ================================================================================
[13:12:07] TRIAL 225 - Starting
[13:12:07] ================================================================================
[13:12:07] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:07] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:07] ❌ Trial 225 failed with error: expected scalar type Long but found Float
[13:12:07] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:07] ✓ Results saved to hyperparameter_results.json
[13:12:07] 
================================================================================
[13:12:07] PROGRESS UPDATE - Completed 225 trials
[13:12:07] Best so far: 0.0000
[13:12:07] ================================================================================

[13:12:07] ================================================================================
[13:12:07] TRIAL 226 - Starting
[13:12:07] ================================================================================
[13:12:07] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:07] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:07] ❌ Trial 226 failed with error: expected scalar type Long but found Float
[13:12:07] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:07] ✓ Results saved to hyperparameter_results.json
[13:12:07] ================================================================================
[13:12:07] TRIAL 227 - Starting
[13:12:07] ================================================================================
[13:12:07] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:07] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:07] ❌ Trial 227 failed with error: expected scalar type Long but found Float
[13:12:07] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:07] ✓ Results saved to hyperparameter_results.json
[13:12:07] ================================================================================
[13:12:07] TRIAL 228 - Starting
[13:12:07] ================================================================================
[13:12:07] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:07] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:08] ❌ Trial 228 failed with error: expected scalar type Long but found Float
[13:12:08] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:08] ✓ Results saved to hyperparameter_results.json
[13:12:08] ================================================================================
[13:12:08] TRIAL 229 - Starting
[13:12:08] ================================================================================
[13:12:08] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:08] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:08] ❌ Trial 229 failed with error: expected scalar type Long but found Float
[13:12:08] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:08] ✓ Results saved to hyperparameter_results.json
[13:12:08] ================================================================================
[13:12:08] TRIAL 230 - Starting
[13:12:08] ================================================================================
[13:12:08] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:08] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:08] ❌ Trial 230 failed with error: expected scalar type Long but found Float
[13:12:08] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:08] ✓ Results saved to hyperparameter_results.json
[13:12:08] 
================================================================================
[13:12:08] PROGRESS UPDATE - Completed 230 trials
[13:12:08] Best so far: 0.0000
[13:12:08] ================================================================================

[13:12:08] ================================================================================
[13:12:08] TRIAL 231 - Starting
[13:12:08] ================================================================================
[13:12:08] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:08] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:08] ❌ Trial 231 failed with error: expected scalar type Long but found Float
[13:12:08] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:08] ✓ Results saved to hyperparameter_results.json
[13:12:08] ================================================================================
[13:12:08] TRIAL 232 - Starting
[13:12:08] ================================================================================
[13:12:08] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:08] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:08] ❌ Trial 232 failed with error: expected scalar type Long but found Float
[13:12:08] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:08] ✓ Results saved to hyperparameter_results.json
[13:12:08] ================================================================================
[13:12:08] TRIAL 233 - Starting
[13:12:08] ================================================================================
[13:12:08] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:08] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:08] ❌ Trial 233 failed with error: expected scalar type Long but found Float
[13:12:08] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:08] ✓ Results saved to hyperparameter_results.json
[13:12:08] ================================================================================
[13:12:08] TRIAL 234 - Starting
[13:12:08] ================================================================================
[13:12:08] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:08] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:08] ❌ Trial 234 failed with error: expected scalar type Long but found Float
[13:12:08] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:08] ✓ Results saved to hyperparameter_results.json
[13:12:08] ================================================================================
[13:12:08] TRIAL 235 - Starting
[13:12:08] ================================================================================
[13:12:08] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:08] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:08] ❌ Trial 235 failed with error: expected scalar type Long but found Float
[13:12:08] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:08] ✓ Results saved to hyperparameter_results.json
[13:12:08] 
================================================================================
[13:12:08] PROGRESS UPDATE - Completed 235 trials
[13:12:08] Best so far: 0.0000
[13:12:08] ================================================================================

[13:12:08] ================================================================================
[13:12:08] TRIAL 236 - Starting
[13:12:08] ================================================================================
[13:12:08] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:08] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:08] ❌ Trial 236 failed with error: expected scalar type Long but found Float
[13:12:08] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:09] ✓ Results saved to hyperparameter_results.json
[13:12:09] ================================================================================
[13:12:09] TRIAL 237 - Starting
[13:12:09] ================================================================================
[13:12:09] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:09] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:09] ❌ Trial 237 failed with error: expected scalar type Long but found Float
[13:12:09] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:09] ✓ Results saved to hyperparameter_results.json
[13:12:09] ================================================================================
[13:12:09] TRIAL 238 - Starting
[13:12:09] ================================================================================
[13:12:09] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:09] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:09] ❌ Trial 238 failed with error: expected scalar type Long but found Float
[13:12:09] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:09] ✓ Results saved to hyperparameter_results.json
[13:12:09] ================================================================================
[13:12:09] TRIAL 239 - Starting
[13:12:09] ================================================================================
[13:12:09] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:09] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:09] ❌ Trial 239 failed with error: expected scalar type Long but found Float
[13:12:09] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:09] ✓ Results saved to hyperparameter_results.json
[13:12:09] ================================================================================
[13:12:09] TRIAL 240 - Starting
[13:12:09] ================================================================================
[13:12:09] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:09] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:09] ❌ Trial 240 failed with error: expected scalar type Long but found Float
[13:12:09] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:09] ✓ Results saved to hyperparameter_results.json
[13:12:09] 
================================================================================
[13:12:09] PROGRESS UPDATE - Completed 240 trials
[13:12:09] Best so far: 0.0000
[13:12:09] ================================================================================

[13:12:09] ================================================================================
[13:12:09] TRIAL 241 - Starting
[13:12:09] ================================================================================
[13:12:09] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:09] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:09] ❌ Trial 241 failed with error: expected scalar type Long but found Float
[13:12:09] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:09] ✓ Results saved to hyperparameter_results.json
[13:12:09] ================================================================================
[13:12:09] TRIAL 242 - Starting
[13:12:09] ================================================================================
[13:12:09] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:09] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:09] ❌ Trial 242 failed with error: expected scalar type Long but found Float
[13:12:09] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:09] ✓ Results saved to hyperparameter_results.json
[13:12:09] ================================================================================
[13:12:09] TRIAL 243 - Starting
[13:12:09] ================================================================================
[13:12:09] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:09] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:09] ❌ Trial 243 failed with error: expected scalar type Long but found Float
[13:12:09] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:09] ✓ Results saved to hyperparameter_results.json
[13:12:09] ================================================================================
[13:12:09] TRIAL 244 - Starting
[13:12:09] ================================================================================
[13:12:09] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:09] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:09] ❌ Trial 244 failed with error: expected scalar type Long but found Float
[13:12:09] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:09] ✓ Results saved to hyperparameter_results.json
[13:12:09] ================================================================================
[13:12:09] TRIAL 245 - Starting
[13:12:09] ================================================================================
[13:12:09] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:09] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:09] ❌ Trial 245 failed with error: expected scalar type Long but found Float
[13:12:09] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:09] ✓ Results saved to hyperparameter_results.json
[13:12:09] 
================================================================================
[13:12:09] PROGRESS UPDATE - Completed 245 trials
[13:12:09] Best so far: 0.0000
[13:12:09] ================================================================================

[13:12:09] ================================================================================
[13:12:09] TRIAL 246 - Starting
[13:12:09] ================================================================================
[13:12:09] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:10] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:10] ❌ Trial 246 failed with error: expected scalar type Long but found Float
[13:12:10] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:10] ✓ Results saved to hyperparameter_results.json
[13:12:10] ================================================================================
[13:12:10] TRIAL 247 - Starting
[13:12:10] ================================================================================
[13:12:10] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:10] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:10] ❌ Trial 247 failed with error: expected scalar type Long but found Float
[13:12:10] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:10] ✓ Results saved to hyperparameter_results.json
[13:12:10] ================================================================================
[13:12:10] TRIAL 248 - Starting
[13:12:10] ================================================================================
[13:12:10] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:10] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:10] ❌ Trial 248 failed with error: expected scalar type Long but found Float
[13:12:10] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:10] ✓ Results saved to hyperparameter_results.json
[13:12:10] ================================================================================
[13:12:10] TRIAL 249 - Starting
[13:12:10] ================================================================================
[13:12:10] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:10] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:10] ❌ Trial 249 failed with error: expected scalar type Long but found Float
[13:12:10] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:10] ✓ Results saved to hyperparameter_results.json
[13:12:10] ================================================================================
[13:12:10] TRIAL 250 - Starting
[13:12:10] ================================================================================
[13:12:10] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:10] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:10] ❌ Trial 250 failed with error: expected scalar type Long but found Float
[13:12:10] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:10] ✓ Results saved to hyperparameter_results.json
[13:12:10] 
================================================================================
[13:12:10] PROGRESS UPDATE - Completed 250 trials
[13:12:10] Best so far: 0.0000
[13:12:10] ================================================================================

[13:12:10] ================================================================================
[13:12:10] TRIAL 251 - Starting
[13:12:10] ================================================================================
[13:12:10] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:10] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:10] ❌ Trial 251 failed with error: expected scalar type Long but found Float
[13:12:10] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:10] ✓ Results saved to hyperparameter_results.json
[13:12:10] ================================================================================
[13:12:10] TRIAL 252 - Starting
[13:12:10] ================================================================================
[13:12:10] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:10] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:10] ❌ Trial 252 failed with error: expected scalar type Long but found Float
[13:12:10] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:10] ✓ Results saved to hyperparameter_results.json
[13:12:10] ================================================================================
[13:12:10] TRIAL 253 - Starting
[13:12:10] ================================================================================
[13:12:10] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:10] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:11] ❌ Trial 253 failed with error: expected scalar type Long but found Float
[13:12:11] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:11] ✓ Results saved to hyperparameter_results.json
[13:12:11] ================================================================================
[13:12:11] TRIAL 254 - Starting
[13:12:11] ================================================================================
[13:12:11] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:11] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:11] ❌ Trial 254 failed with error: expected scalar type Long but found Float
[13:12:11] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:11] ✓ Results saved to hyperparameter_results.json
[13:12:11] ================================================================================
[13:12:11] TRIAL 255 - Starting
[13:12:11] ================================================================================
[13:12:11] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:11] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:11] ❌ Trial 255 failed with error: expected scalar type Long but found Float
[13:12:11] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:11] ✓ Results saved to hyperparameter_results.json
[13:12:11] 
================================================================================
[13:12:11] PROGRESS UPDATE - Completed 255 trials
[13:12:11] Best so far: 0.0000
[13:12:11] ================================================================================

[13:12:11] ================================================================================
[13:12:11] TRIAL 256 - Starting
[13:12:11] ================================================================================
[13:12:11] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:11] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:11] ❌ Trial 256 failed with error: expected scalar type Long but found Float
[13:12:11] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:11] ✓ Results saved to hyperparameter_results.json
[13:12:11] ================================================================================
[13:12:11] TRIAL 257 - Starting
[13:12:11] ================================================================================
[13:12:11] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:11] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:11] ❌ Trial 257 failed with error: expected scalar type Long but found Float
[13:12:11] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:11] ✓ Results saved to hyperparameter_results.json
[13:12:11] ================================================================================
[13:12:11] TRIAL 258 - Starting
[13:12:11] ================================================================================
[13:12:11] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:11] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:11] ❌ Trial 258 failed with error: expected scalar type Long but found Float
[13:12:11] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:11] ✓ Results saved to hyperparameter_results.json
[13:12:11] ================================================================================
[13:12:11] TRIAL 259 - Starting
[13:12:11] ================================================================================
[13:12:11] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:11] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:11] ❌ Trial 259 failed with error: expected scalar type Long but found Float
[13:12:11] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:11] ✓ Results saved to hyperparameter_results.json
[13:12:11] ================================================================================
[13:12:11] TRIAL 260 - Starting
[13:12:11] ================================================================================
[13:12:11] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:11] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:11] ❌ Trial 260 failed with error: expected scalar type Long but found Float
[13:12:11] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:11] ✓ Results saved to hyperparameter_results.json
[13:12:11] 
================================================================================
[13:12:11] PROGRESS UPDATE - Completed 260 trials
[13:12:11] Best so far: 0.0000
[13:12:11] ================================================================================

[13:12:11] ================================================================================
[13:12:11] TRIAL 261 - Starting
[13:12:11] ================================================================================
[13:12:11] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:11] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:12] ❌ Trial 261 failed with error: expected scalar type Long but found Float
[13:12:12] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:12] ✓ Results saved to hyperparameter_results.json
[13:12:12] ================================================================================
[13:12:12] TRIAL 262 - Starting
[13:12:12] ================================================================================
[13:12:12] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:12] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:12] ❌ Trial 262 failed with error: expected scalar type Long but found Float
[13:12:12] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:12] ✓ Results saved to hyperparameter_results.json
[13:12:12] ================================================================================
[13:12:12] TRIAL 263 - Starting
[13:12:12] ================================================================================
[13:12:12] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:12] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:12] ❌ Trial 263 failed with error: expected scalar type Long but found Float
[13:12:12] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:12] ✓ Results saved to hyperparameter_results.json
[13:12:12] ================================================================================
[13:12:12] TRIAL 264 - Starting
[13:12:12] ================================================================================
[13:12:12] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:12] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:12] ❌ Trial 264 failed with error: expected scalar type Long but found Float
[13:12:12] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:12] ✓ Results saved to hyperparameter_results.json
[13:12:12] ================================================================================
[13:12:12] TRIAL 265 - Starting
[13:12:12] ================================================================================
[13:12:12] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:12] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:12] ❌ Trial 265 failed with error: expected scalar type Long but found Float
[13:12:12] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:12] ✓ Results saved to hyperparameter_results.json
[13:12:12] 
================================================================================
[13:12:12] PROGRESS UPDATE - Completed 265 trials
[13:12:12] Best so far: 0.0000
[13:12:12] ================================================================================

[13:12:12] ================================================================================
[13:12:12] TRIAL 266 - Starting
[13:12:12] ================================================================================
[13:12:12] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:12] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:12] ❌ Trial 266 failed with error: expected scalar type Long but found Float
[13:12:12] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:12] ✓ Results saved to hyperparameter_results.json
[13:12:12] ================================================================================
[13:12:12] TRIAL 267 - Starting
[13:12:12] ================================================================================
[13:12:12] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:12] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:12] ❌ Trial 267 failed with error: expected scalar type Long but found Float
[13:12:12] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:12] ✓ Results saved to hyperparameter_results.json
[13:12:12] ================================================================================
[13:12:12] TRIAL 268 - Starting
[13:12:12] ================================================================================
[13:12:12] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:12] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:12] ❌ Trial 268 failed with error: expected scalar type Long but found Float
[13:12:12] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:12] ✓ Results saved to hyperparameter_results.json
[13:12:12] ================================================================================
[13:12:12] TRIAL 269 - Starting
[13:12:12] ================================================================================
[13:12:12] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:12] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:12] ❌ Trial 269 failed with error: expected scalar type Long but found Float
[13:12:12] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:13] ✓ Results saved to hyperparameter_results.json
[13:12:13] ================================================================================
[13:12:13] TRIAL 270 - Starting
[13:12:13] ================================================================================
[13:12:13] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:13] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:13] ❌ Trial 270 failed with error: expected scalar type Long but found Float
[13:12:13] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:13] ✓ Results saved to hyperparameter_results.json
[13:12:13] 
================================================================================
[13:12:13] PROGRESS UPDATE - Completed 270 trials
[13:12:13] Best so far: 0.0000
[13:12:13] ================================================================================

[13:12:13] ================================================================================
[13:12:13] TRIAL 271 - Starting
[13:12:13] ================================================================================
[13:12:13] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:13] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:13] ❌ Trial 271 failed with error: expected scalar type Long but found Float
[13:12:13] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:13] ✓ Results saved to hyperparameter_results.json
[13:12:13] ================================================================================
[13:12:13] TRIAL 272 - Starting
[13:12:13] ================================================================================
[13:12:13] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:13] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:13] ❌ Trial 272 failed with error: expected scalar type Long but found Float
[13:12:13] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:13] ✓ Results saved to hyperparameter_results.json
[13:12:13] ================================================================================
[13:12:13] TRIAL 273 - Starting
[13:12:13] ================================================================================
[13:12:13] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:13] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:13] ❌ Trial 273 failed with error: expected scalar type Long but found Float
[13:12:13] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:13] ✓ Results saved to hyperparameter_results.json
[13:12:13] ================================================================================
[13:12:13] TRIAL 274 - Starting
[13:12:13] ================================================================================
[13:12:13] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:13] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:13] ❌ Trial 274 failed with error: expected scalar type Long but found Float
[13:12:13] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:13] ✓ Results saved to hyperparameter_results.json
[13:12:13] ================================================================================
[13:12:13] TRIAL 275 - Starting
[13:12:13] ================================================================================
[13:12:13] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:13] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:13] ❌ Trial 275 failed with error: expected scalar type Long but found Float
[13:12:13] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:14] ✓ Results saved to hyperparameter_results.json
[13:12:14] 
================================================================================
[13:12:14] PROGRESS UPDATE - Completed 275 trials
[13:12:14] Best so far: 0.0000
[13:12:14] ================================================================================

[13:12:14] ================================================================================
[13:12:14] TRIAL 276 - Starting
[13:12:14] ================================================================================
[13:12:14] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:14] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:14] ❌ Trial 276 failed with error: expected scalar type Long but found Float
[13:12:14] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:14] ✓ Results saved to hyperparameter_results.json
[13:12:14] ================================================================================
[13:12:14] TRIAL 277 - Starting
[13:12:14] ================================================================================
[13:12:14] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:14] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:14] ❌ Trial 277 failed with error: expected scalar type Long but found Float
[13:12:14] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:14] ✓ Results saved to hyperparameter_results.json
[13:12:14] ================================================================================
[13:12:14] TRIAL 278 - Starting
[13:12:14] ================================================================================
[13:12:14] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:14] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:14] ❌ Trial 278 failed with error: expected scalar type Long but found Float
[13:12:14] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:14] ✓ Results saved to hyperparameter_results.json
[13:12:14] ================================================================================
[13:12:14] TRIAL 279 - Starting
[13:12:14] ================================================================================
[13:12:14] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:14] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:14] ❌ Trial 279 failed with error: expected scalar type Long but found Float
[13:12:14] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:14] ✓ Results saved to hyperparameter_results.json
[13:12:14] ================================================================================
[13:12:14] TRIAL 280 - Starting
[13:12:14] ================================================================================
[13:12:14] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:14] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:14] ❌ Trial 280 failed with error: expected scalar type Long but found Float
[13:12:14] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:14] ✓ Results saved to hyperparameter_results.json
[13:12:14] 
================================================================================
[13:12:14] PROGRESS UPDATE - Completed 280 trials
[13:12:14] Best so far: 0.0000
[13:12:14] ================================================================================

[13:12:14] ================================================================================
[13:12:14] TRIAL 281 - Starting
[13:12:14] ================================================================================
[13:12:14] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:14] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:14] ❌ Trial 281 failed with error: expected scalar type Long but found Float
[13:12:14] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:14] ✓ Results saved to hyperparameter_results.json
[13:12:14] ================================================================================
[13:12:14] TRIAL 282 - Starting
[13:12:14] ================================================================================
[13:12:14] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:14] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:14] ❌ Trial 282 failed with error: expected scalar type Long but found Float
[13:12:14] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:14] ✓ Results saved to hyperparameter_results.json
[13:12:14] ================================================================================
[13:12:14] TRIAL 283 - Starting
[13:12:14] ================================================================================
[13:12:14] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:14] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:15] ❌ Trial 283 failed with error: expected scalar type Long but found Float
[13:12:15] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:15] ✓ Results saved to hyperparameter_results.json
[13:12:15] ================================================================================
[13:12:15] TRIAL 284 - Starting
[13:12:15] ================================================================================
[13:12:15] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:15] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:15] ❌ Trial 284 failed with error: expected scalar type Long but found Float
[13:12:15] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:15] ✓ Results saved to hyperparameter_results.json
[13:12:15] ================================================================================
[13:12:15] TRIAL 285 - Starting
[13:12:15] ================================================================================
[13:12:15] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:15] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:15] ❌ Trial 285 failed with error: expected scalar type Long but found Float
[13:12:15] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:15] ✓ Results saved to hyperparameter_results.json
[13:12:15] 
================================================================================
[13:12:15] PROGRESS UPDATE - Completed 285 trials
[13:12:15] Best so far: 0.0000
[13:12:15] ================================================================================

[13:12:15] ================================================================================
[13:12:15] TRIAL 286 - Starting
[13:12:15] ================================================================================
[13:12:15] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:15] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:15] ❌ Trial 286 failed with error: expected scalar type Long but found Float
[13:12:15] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:15] ✓ Results saved to hyperparameter_results.json
[13:12:15] ================================================================================
[13:12:15] TRIAL 287 - Starting
[13:12:15] ================================================================================
[13:12:15] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:15] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:15] ❌ Trial 287 failed with error: expected scalar type Long but found Float
[13:12:15] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:15] ✓ Results saved to hyperparameter_results.json
[13:12:15] ================================================================================
[13:12:15] TRIAL 288 - Starting
[13:12:15] ================================================================================
[13:12:15] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:15] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:15] ❌ Trial 288 failed with error: expected scalar type Long but found Float
[13:12:15] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:15] ✓ Results saved to hyperparameter_results.json
[13:12:15] ================================================================================
[13:12:15] TRIAL 289 - Starting
[13:12:15] ================================================================================
[13:12:15] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:15] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:15] ❌ Trial 289 failed with error: expected scalar type Long but found Float
[13:12:15] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:15] ✓ Results saved to hyperparameter_results.json
[13:12:15] ================================================================================
[13:12:15] TRIAL 290 - Starting
[13:12:15] ================================================================================
[13:12:15] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:15] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:15] ❌ Trial 290 failed with error: expected scalar type Long but found Float
[13:12:15] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:15] ✓ Results saved to hyperparameter_results.json
[13:12:15] 
================================================================================
[13:12:15] PROGRESS UPDATE - Completed 290 trials
[13:12:15] Best so far: 0.0000
[13:12:15] ================================================================================

[13:12:15] ================================================================================
[13:12:15] TRIAL 291 - Starting
[13:12:15] ================================================================================
[13:12:15] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:15] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:15] ❌ Trial 291 failed with error: expected scalar type Long but found Float
[13:12:15] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:15] ✓ Results saved to hyperparameter_results.json
[13:12:15] ================================================================================
[13:12:15] TRIAL 292 - Starting
[13:12:15] ================================================================================
[13:12:15] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:15] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:16] ❌ Trial 292 failed with error: expected scalar type Long but found Float
[13:12:16] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:16] ✓ Results saved to hyperparameter_results.json
[13:12:16] ================================================================================
[13:12:16] TRIAL 293 - Starting
[13:12:16] ================================================================================
[13:12:16] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:16] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:16] ❌ Trial 293 failed with error: expected scalar type Long but found Float
[13:12:16] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:16] ✓ Results saved to hyperparameter_results.json
[13:12:16] ================================================================================
[13:12:16] TRIAL 294 - Starting
[13:12:16] ================================================================================
[13:12:16] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:16] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:16] ❌ Trial 294 failed with error: expected scalar type Long but found Float
[13:12:16] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:16] ✓ Results saved to hyperparameter_results.json
[13:12:16] ================================================================================
[13:12:16] TRIAL 295 - Starting
[13:12:16] ================================================================================
[13:12:16] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:16] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:16] ❌ Trial 295 failed with error: expected scalar type Long but found Float
[13:12:16] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:16] ✓ Results saved to hyperparameter_results.json
[13:12:16] 
================================================================================
[13:12:16] PROGRESS UPDATE - Completed 295 trials
[13:12:16] Best so far: 0.0000
[13:12:16] ================================================================================

[13:12:16] ================================================================================
[13:12:16] TRIAL 296 - Starting
[13:12:16] ================================================================================
[13:12:16] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:16] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:16] ❌ Trial 296 failed with error: expected scalar type Long but found Float
[13:12:16] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:16] ✓ Results saved to hyperparameter_results.json
[13:12:16] ================================================================================
[13:12:16] TRIAL 297 - Starting
[13:12:16] ================================================================================
[13:12:16] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:16] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:16] ❌ Trial 297 failed with error: expected scalar type Long but found Float
[13:12:16] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:16] ✓ Results saved to hyperparameter_results.json
[13:12:16] ================================================================================
[13:12:16] TRIAL 298 - Starting
[13:12:16] ================================================================================
[13:12:16] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:16] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:16] ❌ Trial 298 failed with error: expected scalar type Long but found Float
[13:12:16] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:16] ✓ Results saved to hyperparameter_results.json
[13:12:16] ================================================================================
[13:12:16] TRIAL 299 - Starting
[13:12:16] ================================================================================
[13:12:16] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:16] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:17] ❌ Trial 299 failed with error: expected scalar type Long but found Float
[13:12:17] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:17] ✓ Results saved to hyperparameter_results.json
[13:12:17] ================================================================================
[13:12:17] TRIAL 300 - Starting
[13:12:17] ================================================================================
[13:12:17] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:17] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:17] ❌ Trial 300 failed with error: expected scalar type Long but found Float
[13:12:17] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:17] ✓ Results saved to hyperparameter_results.json
[13:12:17] 
================================================================================
[13:12:17] PROGRESS UPDATE - Completed 300 trials
[13:12:17] Best so far: 0.0000
[13:12:17] ================================================================================

[13:12:17] ================================================================================
[13:12:17] TRIAL 301 - Starting
[13:12:17] ================================================================================
[13:12:17] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:17] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:17] ❌ Trial 301 failed with error: expected scalar type Long but found Float
[13:12:17] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:17] ✓ Results saved to hyperparameter_results.json
[13:12:17] ================================================================================
[13:12:17] TRIAL 302 - Starting
[13:12:17] ================================================================================
[13:12:17] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:17] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:17] ❌ Trial 302 failed with error: expected scalar type Long but found Float
[13:12:17] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:17] ✓ Results saved to hyperparameter_results.json
[13:12:17] ================================================================================
[13:12:17] TRIAL 303 - Starting
[13:12:17] ================================================================================
[13:12:17] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:17] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:17] ❌ Trial 303 failed with error: expected scalar type Long but found Float
[13:12:17] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:17] ✓ Results saved to hyperparameter_results.json
[13:12:17] ================================================================================
[13:12:17] TRIAL 304 - Starting
[13:12:17] ================================================================================
[13:12:17] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:17] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:17] ❌ Trial 304 failed with error: expected scalar type Long but found Float
[13:12:17] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:17] ✓ Results saved to hyperparameter_results.json
[13:12:17] ================================================================================
[13:12:17] TRIAL 305 - Starting
[13:12:17] ================================================================================
[13:12:17] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:17] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:17] ❌ Trial 305 failed with error: expected scalar type Long but found Float
[13:12:17] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:17] ✓ Results saved to hyperparameter_results.json
[13:12:17] 
================================================================================
[13:12:17] PROGRESS UPDATE - Completed 305 trials
[13:12:17] Best so far: 0.0000
[13:12:17] ================================================================================

[13:12:17] ================================================================================
[13:12:17] TRIAL 306 - Starting
[13:12:17] ================================================================================
[13:12:17] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:18] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:18] ❌ Trial 306 failed with error: expected scalar type Long but found Float
[13:12:18] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:18] ✓ Results saved to hyperparameter_results.json
[13:12:18] ================================================================================
[13:12:18] TRIAL 307 - Starting
[13:12:18] ================================================================================
[13:12:18] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:18] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:18] ❌ Trial 307 failed with error: expected scalar type Long but found Float
[13:12:18] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:18] ✓ Results saved to hyperparameter_results.json
[13:12:18] ================================================================================
[13:12:18] TRIAL 308 - Starting
[13:12:18] ================================================================================
[13:12:18] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:18] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:18] ❌ Trial 308 failed with error: expected scalar type Long but found Float
[13:12:18] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:18] ✓ Results saved to hyperparameter_results.json
[13:12:18] ================================================================================
[13:12:18] TRIAL 309 - Starting
[13:12:18] ================================================================================
[13:12:18] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:18] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:18] ❌ Trial 309 failed with error: expected scalar type Long but found Float
[13:12:18] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:18] ✓ Results saved to hyperparameter_results.json
[13:12:18] ================================================================================
[13:12:18] TRIAL 310 - Starting
[13:12:18] ================================================================================
[13:12:18] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:18] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:18] ❌ Trial 310 failed with error: expected scalar type Long but found Float
[13:12:18] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:18] ✓ Results saved to hyperparameter_results.json
[13:12:18] 
================================================================================
[13:12:18] PROGRESS UPDATE - Completed 310 trials
[13:12:18] Best so far: 0.0000
[13:12:18] ================================================================================

[13:12:18] ================================================================================
[13:12:18] TRIAL 311 - Starting
[13:12:18] ================================================================================
[13:12:18] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:18] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:18] ❌ Trial 311 failed with error: expected scalar type Long but found Float
[13:12:18] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:18] ✓ Results saved to hyperparameter_results.json
[13:12:18] ================================================================================
[13:12:18] TRIAL 312 - Starting
[13:12:18] ================================================================================
[13:12:18] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:18] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:18] ❌ Trial 312 failed with error: expected scalar type Long but found Float
[13:12:18] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:18] ✓ Results saved to hyperparameter_results.json
[13:12:18] ================================================================================
[13:12:18] TRIAL 313 - Starting
[13:12:18] ================================================================================
[13:12:18] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:18] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:19] ❌ Trial 313 failed with error: expected scalar type Long but found Float
[13:12:19] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:19] ✓ Results saved to hyperparameter_results.json
[13:12:19] ================================================================================
[13:12:19] TRIAL 314 - Starting
[13:12:19] ================================================================================
[13:12:19] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:19] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:19] ❌ Trial 314 failed with error: expected scalar type Long but found Float
[13:12:19] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:19] ✓ Results saved to hyperparameter_results.json
[13:12:19] ================================================================================
[13:12:19] TRIAL 315 - Starting
[13:12:19] ================================================================================
[13:12:19] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:19] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:19] ❌ Trial 315 failed with error: expected scalar type Long but found Float
[13:12:19] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:19] ✓ Results saved to hyperparameter_results.json
[13:12:19] 
================================================================================
[13:12:19] PROGRESS UPDATE - Completed 315 trials
[13:12:19] Best so far: 0.0000
[13:12:19] ================================================================================

[13:12:19] ================================================================================
[13:12:19] TRIAL 316 - Starting
[13:12:19] ================================================================================
[13:12:19] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:19] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:19] ❌ Trial 316 failed with error: expected scalar type Long but found Float
[13:12:19] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:19] ✓ Results saved to hyperparameter_results.json
[13:12:19] ================================================================================
[13:12:19] TRIAL 317 - Starting
[13:12:19] ================================================================================
[13:12:19] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:19] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:19] ❌ Trial 317 failed with error: expected scalar type Long but found Float
[13:12:19] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:19] ✓ Results saved to hyperparameter_results.json
[13:12:19] ================================================================================
[13:12:19] TRIAL 318 - Starting
[13:12:19] ================================================================================
[13:12:19] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:19] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:19] ❌ Trial 318 failed with error: expected scalar type Long but found Float
[13:12:19] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:19] ✓ Results saved to hyperparameter_results.json
[13:12:19] ================================================================================
[13:12:19] TRIAL 319 - Starting
[13:12:19] ================================================================================
[13:12:19] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:19] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:19] ❌ Trial 319 failed with error: expected scalar type Long but found Float
[13:12:19] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:19] ✓ Results saved to hyperparameter_results.json
[13:12:19] ================================================================================
[13:12:19] TRIAL 320 - Starting
[13:12:19] ================================================================================
[13:12:19] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:19] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:19] ❌ Trial 320 failed with error: expected scalar type Long but found Float
[13:12:19] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:20] ✓ Results saved to hyperparameter_results.json
[13:12:20] 
================================================================================
[13:12:20] PROGRESS UPDATE - Completed 320 trials
[13:12:20] Best so far: 0.0000
[13:12:20] ================================================================================

[13:12:20] ================================================================================
[13:12:20] TRIAL 321 - Starting
[13:12:20] ================================================================================
[13:12:20] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:20] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:20] ❌ Trial 321 failed with error: expected scalar type Long but found Float
[13:12:20] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:20] ✓ Results saved to hyperparameter_results.json
[13:12:20] ================================================================================
[13:12:20] TRIAL 322 - Starting
[13:12:20] ================================================================================
[13:12:20] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:20] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:20] ❌ Trial 322 failed with error: expected scalar type Long but found Float
[13:12:20] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:20] ✓ Results saved to hyperparameter_results.json
[13:12:20] ================================================================================
[13:12:20] TRIAL 323 - Starting
[13:12:20] ================================================================================
[13:12:20] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:20] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:20] ❌ Trial 323 failed with error: expected scalar type Long but found Float
[13:12:20] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:20] ✓ Results saved to hyperparameter_results.json
[13:12:20] ================================================================================
[13:12:20] TRIAL 324 - Starting
[13:12:20] ================================================================================
[13:12:20] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:20] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:20] ❌ Trial 324 failed with error: expected scalar type Long but found Float
[13:12:20] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:20] ✓ Results saved to hyperparameter_results.json
[13:12:20] ================================================================================
[13:12:20] TRIAL 325 - Starting
[13:12:20] ================================================================================
[13:12:20] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:20] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:20] ❌ Trial 325 failed with error: expected scalar type Long but found Float
[13:12:20] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:20] ✓ Results saved to hyperparameter_results.json
[13:12:20] 
================================================================================
[13:12:20] PROGRESS UPDATE - Completed 325 trials
[13:12:20] Best so far: 0.0000
[13:12:20] ================================================================================

[13:12:20] ================================================================================
[13:12:20] TRIAL 326 - Starting
[13:12:20] ================================================================================
[13:12:20] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:20] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:20] ❌ Trial 326 failed with error: expected scalar type Long but found Float
[13:12:20] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:20] ✓ Results saved to hyperparameter_results.json
[13:12:20] ================================================================================
[13:12:20] TRIAL 327 - Starting
[13:12:20] ================================================================================
[13:12:20] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:20] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:20] ❌ Trial 327 failed with error: expected scalar type Long but found Float
[13:12:20] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:20] ✓ Results saved to hyperparameter_results.json
[13:12:20] ================================================================================
[13:12:20] TRIAL 328 - Starting
[13:12:20] ================================================================================
[13:12:20] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:20] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:21] ❌ Trial 328 failed with error: expected scalar type Long but found Float
[13:12:21] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:21] ✓ Results saved to hyperparameter_results.json
[13:12:21] ================================================================================
[13:12:21] TRIAL 329 - Starting
[13:12:21] ================================================================================
[13:12:21] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:21] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:21] ❌ Trial 329 failed with error: expected scalar type Long but found Float
[13:12:21] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:21] ✓ Results saved to hyperparameter_results.json
[13:12:21] ================================================================================
[13:12:21] TRIAL 330 - Starting
[13:12:21] ================================================================================
[13:12:21] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:21] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:21] ❌ Trial 330 failed with error: expected scalar type Long but found Float
[13:12:21] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:21] ✓ Results saved to hyperparameter_results.json
[13:12:21] 
================================================================================
[13:12:21] PROGRESS UPDATE - Completed 330 trials
[13:12:21] Best so far: 0.0000
[13:12:21] ================================================================================

[13:12:21] ================================================================================
[13:12:21] TRIAL 331 - Starting
[13:12:21] ================================================================================
[13:12:21] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:21] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:21] ❌ Trial 331 failed with error: expected scalar type Long but found Float
[13:12:21] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:21] ✓ Results saved to hyperparameter_results.json
[13:12:21] ================================================================================
[13:12:21] TRIAL 332 - Starting
[13:12:21] ================================================================================
[13:12:21] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:21] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:21] ❌ Trial 332 failed with error: expected scalar type Long but found Float
[13:12:21] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:21] ✓ Results saved to hyperparameter_results.json
[13:12:21] ================================================================================
[13:12:21] TRIAL 333 - Starting
[13:12:21] ================================================================================
[13:12:21] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:21] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:21] ❌ Trial 333 failed with error: expected scalar type Long but found Float
[13:12:21] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:21] ✓ Results saved to hyperparameter_results.json
[13:12:21] ================================================================================
[13:12:21] TRIAL 334 - Starting
[13:12:21] ================================================================================
[13:12:21] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:21] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:21] ❌ Trial 334 failed with error: expected scalar type Long but found Float
[13:12:21] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:21] ✓ Results saved to hyperparameter_results.json
[13:12:21] ================================================================================
[13:12:21] TRIAL 335 - Starting
[13:12:21] ================================================================================
[13:12:21] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:21] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:21] ❌ Trial 335 failed with error: expected scalar type Long but found Float
[13:12:21] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:21] ✓ Results saved to hyperparameter_results.json
[13:12:21] 
================================================================================
[13:12:21] PROGRESS UPDATE - Completed 335 trials
[13:12:21] Best so far: 0.0000
[13:12:21] ================================================================================

[13:12:21] ================================================================================
[13:12:21] TRIAL 336 - Starting
[13:12:21] ================================================================================
[13:12:21] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:22] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:22] ❌ Trial 336 failed with error: expected scalar type Long but found Float
[13:12:22] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:22] ✓ Results saved to hyperparameter_results.json
[13:12:22] ================================================================================
[13:12:22] TRIAL 337 - Starting
[13:12:22] ================================================================================
[13:12:22] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:22] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:22] ❌ Trial 337 failed with error: expected scalar type Long but found Float
[13:12:22] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:22] ✓ Results saved to hyperparameter_results.json
[13:12:22] ================================================================================
[13:12:22] TRIAL 338 - Starting
[13:12:22] ================================================================================
[13:12:22] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:22] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:22] ❌ Trial 338 failed with error: expected scalar type Long but found Float
[13:12:22] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:22] ✓ Results saved to hyperparameter_results.json
[13:12:22] ================================================================================
[13:12:22] TRIAL 339 - Starting
[13:12:22] ================================================================================
[13:12:22] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:22] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:22] ❌ Trial 339 failed with error: expected scalar type Long but found Float
[13:12:22] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:22] ✓ Results saved to hyperparameter_results.json
[13:12:22] ================================================================================
[13:12:22] TRIAL 340 - Starting
[13:12:22] ================================================================================
[13:12:22] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:22] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:22] ❌ Trial 340 failed with error: expected scalar type Long but found Float
[13:12:22] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:22] ✓ Results saved to hyperparameter_results.json
[13:12:22] 
================================================================================
[13:12:22] PROGRESS UPDATE - Completed 340 trials
[13:12:22] Best so far: 0.0000
[13:12:22] ================================================================================

[13:12:22] ================================================================================
[13:12:22] TRIAL 341 - Starting
[13:12:22] ================================================================================
[13:12:22] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:22] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:22] ❌ Trial 341 failed with error: expected scalar type Long but found Float
[13:12:22] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:22] ✓ Results saved to hyperparameter_results.json
[13:12:22] ================================================================================
[13:12:22] TRIAL 342 - Starting
[13:12:22] ================================================================================
[13:12:22] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:22] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:22] ❌ Trial 342 failed with error: expected scalar type Long but found Float
[13:12:22] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:22] ✓ Results saved to hyperparameter_results.json
[13:12:22] ================================================================================
[13:12:22] TRIAL 343 - Starting
[13:12:22] ================================================================================
[13:12:22] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:22] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:22] ❌ Trial 343 failed with error: expected scalar type Long but found Float
[13:12:22] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:22] ✓ Results saved to hyperparameter_results.json
[13:12:22] ================================================================================
[13:12:22] TRIAL 344 - Starting
[13:12:22] ================================================================================
[13:12:22] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:22] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:23] ❌ Trial 344 failed with error: expected scalar type Long but found Float
[13:12:23] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:23] ✓ Results saved to hyperparameter_results.json
[13:12:23] ================================================================================
[13:12:23] TRIAL 345 - Starting
[13:12:23] ================================================================================
[13:12:23] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:23] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:23] ❌ Trial 345 failed with error: expected scalar type Long but found Float
[13:12:23] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:23] ✓ Results saved to hyperparameter_results.json
[13:12:23] 
================================================================================
[13:12:23] PROGRESS UPDATE - Completed 345 trials
[13:12:23] Best so far: 0.0000
[13:12:23] ================================================================================

[13:12:23] ================================================================================
[13:12:23] TRIAL 346 - Starting
[13:12:23] ================================================================================
[13:12:23] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:23] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:23] ❌ Trial 346 failed with error: expected scalar type Long but found Float
[13:12:23] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:23] ✓ Results saved to hyperparameter_results.json
[13:12:23] ================================================================================
[13:12:23] TRIAL 347 - Starting
[13:12:23] ================================================================================
[13:12:23] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:23] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:23] ❌ Trial 347 failed with error: expected scalar type Long but found Float
[13:12:23] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:23] ✓ Results saved to hyperparameter_results.json
[13:12:23] ================================================================================
[13:12:23] TRIAL 348 - Starting
[13:12:23] ================================================================================
[13:12:23] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:23] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:23] ❌ Trial 348 failed with error: expected scalar type Long but found Float
[13:12:23] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:23] ✓ Results saved to hyperparameter_results.json
[13:12:23] ================================================================================
[13:12:23] TRIAL 349 - Starting
[13:12:23] ================================================================================
[13:12:23] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:23] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:23] ❌ Trial 349 failed with error: expected scalar type Long but found Float
[13:12:23] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:23] ✓ Results saved to hyperparameter_results.json
[13:12:23] ================================================================================
[13:12:23] TRIAL 350 - Starting
[13:12:23] ================================================================================
[13:12:23] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:23] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:23] ❌ Trial 350 failed with error: expected scalar type Long but found Float
[13:12:23] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:23] ✓ Results saved to hyperparameter_results.json
[13:12:23] 
================================================================================
[13:12:23] PROGRESS UPDATE - Completed 350 trials
[13:12:23] Best so far: 0.0000
[13:12:23] ================================================================================

[13:12:23] ================================================================================
[13:12:23] TRIAL 351 - Starting
[13:12:23] ================================================================================
[13:12:23] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:23] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:23] ❌ Trial 351 failed with error: expected scalar type Long but found Float
[13:12:23] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:23] ✓ Results saved to hyperparameter_results.json
[13:12:23] ================================================================================
[13:12:23] TRIAL 352 - Starting
[13:12:23] ================================================================================
[13:12:23] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:23] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:24] ❌ Trial 352 failed with error: expected scalar type Long but found Float
[13:12:24] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:24] ✓ Results saved to hyperparameter_results.json
[13:12:24] ================================================================================
[13:12:24] TRIAL 353 - Starting
[13:12:24] ================================================================================
[13:12:24] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:24] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:24] ❌ Trial 353 failed with error: expected scalar type Long but found Float
[13:12:24] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:24] ✓ Results saved to hyperparameter_results.json
[13:12:24] ================================================================================
[13:12:24] TRIAL 354 - Starting
[13:12:24] ================================================================================
[13:12:24] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:24] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:24] ❌ Trial 354 failed with error: expected scalar type Long but found Float
[13:12:24] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:24] ✓ Results saved to hyperparameter_results.json
[13:12:24] ================================================================================
[13:12:24] TRIAL 355 - Starting
[13:12:24] ================================================================================
[13:12:24] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:24] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:24] ❌ Trial 355 failed with error: expected scalar type Long but found Float
[13:12:24] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:24] ✓ Results saved to hyperparameter_results.json
[13:12:24] 
================================================================================
[13:12:24] PROGRESS UPDATE - Completed 355 trials
[13:12:24] Best so far: 0.0000
[13:12:24] ================================================================================

[13:12:24] ================================================================================
[13:12:24] TRIAL 356 - Starting
[13:12:24] ================================================================================
[13:12:24] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:24] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:24] ❌ Trial 356 failed with error: expected scalar type Long but found Float
[13:12:24] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:24] ✓ Results saved to hyperparameter_results.json
[13:12:24] ================================================================================
[13:12:24] TRIAL 357 - Starting
[13:12:24] ================================================================================
[13:12:24] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:24] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:24] ❌ Trial 357 failed with error: expected scalar type Long but found Float
[13:12:24] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:24] ✓ Results saved to hyperparameter_results.json
[13:12:24] ================================================================================
[13:12:24] TRIAL 358 - Starting
[13:12:24] ================================================================================
[13:12:24] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:24] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:24] ❌ Trial 358 failed with error: expected scalar type Long but found Float
[13:12:24] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:24] ✓ Results saved to hyperparameter_results.json
[13:12:24] ================================================================================
[13:12:24] TRIAL 359 - Starting
[13:12:24] ================================================================================
[13:12:24] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:24] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:24] ❌ Trial 359 failed with error: expected scalar type Long but found Float
[13:12:24] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:24] ✓ Results saved to hyperparameter_results.json
[13:12:24] ================================================================================
[13:12:24] TRIAL 360 - Starting
[13:12:24] ================================================================================
[13:12:24] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:24] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:25] ❌ Trial 360 failed with error: expected scalar type Long but found Float
[13:12:25] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:25] ✓ Results saved to hyperparameter_results.json
[13:12:25] 
================================================================================
[13:12:25] PROGRESS UPDATE - Completed 360 trials
[13:12:25] Best so far: 0.0000
[13:12:25] ================================================================================

[13:12:25] ================================================================================
[13:12:25] TRIAL 361 - Starting
[13:12:25] ================================================================================
[13:12:25] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:25] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:25] ❌ Trial 361 failed with error: expected scalar type Long but found Float
[13:12:25] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:25] ✓ Results saved to hyperparameter_results.json
[13:12:25] ================================================================================
[13:12:25] TRIAL 362 - Starting
[13:12:25] ================================================================================
[13:12:25] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:25] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:25] ❌ Trial 362 failed with error: expected scalar type Long but found Float
[13:12:25] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:25] ✓ Results saved to hyperparameter_results.json
[13:12:25] ================================================================================
[13:12:25] TRIAL 363 - Starting
[13:12:25] ================================================================================
[13:12:25] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:25] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:25] ❌ Trial 363 failed with error: expected scalar type Long but found Float
[13:12:25] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:25] ✓ Results saved to hyperparameter_results.json
[13:12:25] ================================================================================
[13:12:25] TRIAL 364 - Starting
[13:12:25] ================================================================================
[13:12:25] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:25] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:25] ❌ Trial 364 failed with error: expected scalar type Long but found Float
[13:12:25] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:25] ✓ Results saved to hyperparameter_results.json
[13:12:25] ================================================================================
[13:12:25] TRIAL 365 - Starting
[13:12:25] ================================================================================
[13:12:25] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:25] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:25] ❌ Trial 365 failed with error: expected scalar type Long but found Float
[13:12:25] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:25] ✓ Results saved to hyperparameter_results.json
[13:12:25] 
================================================================================
[13:12:25] PROGRESS UPDATE - Completed 365 trials
[13:12:25] Best so far: 0.0000
[13:12:25] ================================================================================

[13:12:25] ================================================================================
[13:12:25] TRIAL 366 - Starting
[13:12:25] ================================================================================
[13:12:25] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:25] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:25] ❌ Trial 366 failed with error: expected scalar type Long but found Float
[13:12:25] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:25] ✓ Results saved to hyperparameter_results.json
[13:12:25] ================================================================================
[13:12:25] TRIAL 367 - Starting
[13:12:25] ================================================================================
[13:12:25] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:25] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:25] ❌ Trial 367 failed with error: expected scalar type Long but found Float
[13:12:25] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:26] ✓ Results saved to hyperparameter_results.json
[13:12:26] ================================================================================
[13:12:26] TRIAL 368 - Starting
[13:12:26] ================================================================================
[13:12:26] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:26] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:26] ❌ Trial 368 failed with error: expected scalar type Long but found Float
[13:12:26] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:26] ✓ Results saved to hyperparameter_results.json
[13:12:26] ================================================================================
[13:12:26] TRIAL 369 - Starting
[13:12:26] ================================================================================
[13:12:26] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:26] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:26] ❌ Trial 369 failed with error: expected scalar type Long but found Float
[13:12:26] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:26] ✓ Results saved to hyperparameter_results.json
[13:12:26] ================================================================================
[13:12:26] TRIAL 370 - Starting
[13:12:26] ================================================================================
[13:12:26] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:26] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:26] ❌ Trial 370 failed with error: expected scalar type Long but found Float
[13:12:26] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:26] ✓ Results saved to hyperparameter_results.json
[13:12:26] 
================================================================================
[13:12:26] PROGRESS UPDATE - Completed 370 trials
[13:12:26] Best so far: 0.0000
[13:12:26] ================================================================================

[13:12:26] ================================================================================
[13:12:26] TRIAL 371 - Starting
[13:12:26] ================================================================================
[13:12:26] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:26] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:26] ❌ Trial 371 failed with error: expected scalar type Long but found Float
[13:12:26] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:26] ✓ Results saved to hyperparameter_results.json
[13:12:26] ================================================================================
[13:12:26] TRIAL 372 - Starting
[13:12:26] ================================================================================
[13:12:26] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:26] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:26] ❌ Trial 372 failed with error: expected scalar type Long but found Float
[13:12:26] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:26] ✓ Results saved to hyperparameter_results.json
[13:12:26] ================================================================================
[13:12:26] TRIAL 373 - Starting
[13:12:26] ================================================================================
[13:12:26] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:26] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:26] ❌ Trial 373 failed with error: expected scalar type Long but found Float
[13:12:26] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:26] ✓ Results saved to hyperparameter_results.json
[13:12:26] ================================================================================
[13:12:26] TRIAL 374 - Starting
[13:12:26] ================================================================================
[13:12:26] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:26] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:26] ❌ Trial 374 failed with error: expected scalar type Long but found Float
[13:12:26] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:26] ✓ Results saved to hyperparameter_results.json
[13:12:26] ================================================================================
[13:12:26] TRIAL 375 - Starting
[13:12:26] ================================================================================
[13:12:26] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:26] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:27] ❌ Trial 375 failed with error: expected scalar type Long but found Float
[13:12:27] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:27] ✓ Results saved to hyperparameter_results.json
[13:12:27] 
================================================================================
[13:12:27] PROGRESS UPDATE - Completed 375 trials
[13:12:27] Best so far: 0.0000
[13:12:27] ================================================================================

[13:12:27] ================================================================================
[13:12:27] TRIAL 376 - Starting
[13:12:27] ================================================================================
[13:12:27] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:27] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:27] ❌ Trial 376 failed with error: expected scalar type Long but found Float
[13:12:27] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:27] ✓ Results saved to hyperparameter_results.json
[13:12:27] ================================================================================
[13:12:27] TRIAL 377 - Starting
[13:12:27] ================================================================================
[13:12:27] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:27] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:27] ❌ Trial 377 failed with error: expected scalar type Long but found Float
[13:12:27] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:27] ✓ Results saved to hyperparameter_results.json
[13:12:27] ================================================================================
[13:12:27] TRIAL 378 - Starting
[13:12:27] ================================================================================
[13:12:27] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:27] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:27] ❌ Trial 378 failed with error: expected scalar type Long but found Float
[13:12:27] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:27] ✓ Results saved to hyperparameter_results.json
[13:12:27] ================================================================================
[13:12:27] TRIAL 379 - Starting
[13:12:27] ================================================================================
[13:12:27] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:27] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:27] ❌ Trial 379 failed with error: expected scalar type Long but found Float
[13:12:27] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:27] ✓ Results saved to hyperparameter_results.json
[13:12:27] ================================================================================
[13:12:27] TRIAL 380 - Starting
[13:12:27] ================================================================================
[13:12:27] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:27] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:27] ❌ Trial 380 failed with error: expected scalar type Long but found Float
[13:12:27] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:27] ✓ Results saved to hyperparameter_results.json
[13:12:27] 
================================================================================
[13:12:27] PROGRESS UPDATE - Completed 380 trials
[13:12:27] Best so far: 0.0000
[13:12:27] ================================================================================

[13:12:27] ================================================================================
[13:12:27] TRIAL 381 - Starting
[13:12:27] ================================================================================
[13:12:27] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:27] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:27] ❌ Trial 381 failed with error: expected scalar type Long but found Float
[13:12:27] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:27] ✓ Results saved to hyperparameter_results.json
[13:12:27] ================================================================================
[13:12:27] TRIAL 382 - Starting
[13:12:27] ================================================================================
[13:12:27] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:27] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:27] ❌ Trial 382 failed with error: expected scalar type Long but found Float
[13:12:27] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:27] ✓ Results saved to hyperparameter_results.json
[13:12:27] ================================================================================
[13:12:27] TRIAL 383 - Starting
[13:12:27] ================================================================================
[13:12:27] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:27] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:28] ❌ Trial 383 failed with error: expected scalar type Long but found Float
[13:12:28] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:28] ✓ Results saved to hyperparameter_results.json
[13:12:28] ================================================================================
[13:12:28] TRIAL 384 - Starting
[13:12:28] ================================================================================
[13:12:28] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:28] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:28] ❌ Trial 384 failed with error: expected scalar type Long but found Float
[13:12:28] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:28] ✓ Results saved to hyperparameter_results.json
[13:12:28] ================================================================================
[13:12:28] TRIAL 385 - Starting
[13:12:28] ================================================================================
[13:12:28] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:28] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:28] ❌ Trial 385 failed with error: expected scalar type Long but found Float
[13:12:28] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:28] ✓ Results saved to hyperparameter_results.json
[13:12:28] 
================================================================================
[13:12:28] PROGRESS UPDATE - Completed 385 trials
[13:12:28] Best so far: 0.0000
[13:12:28] ================================================================================

[13:12:28] ================================================================================
[13:12:28] TRIAL 386 - Starting
[13:12:28] ================================================================================
[13:12:28] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:28] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:28] ❌ Trial 386 failed with error: expected scalar type Long but found Float
[13:12:28] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:28] ✓ Results saved to hyperparameter_results.json
[13:12:28] ================================================================================
[13:12:28] TRIAL 387 - Starting
[13:12:28] ================================================================================
[13:12:28] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:28] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:28] ❌ Trial 387 failed with error: expected scalar type Long but found Float
[13:12:28] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:28] ✓ Results saved to hyperparameter_results.json
[13:12:28] ================================================================================
[13:12:28] TRIAL 388 - Starting
[13:12:28] ================================================================================
[13:12:28] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:28] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:28] ❌ Trial 388 failed with error: expected scalar type Long but found Float
[13:12:28] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:28] ✓ Results saved to hyperparameter_results.json
[13:12:28] ================================================================================
[13:12:28] TRIAL 389 - Starting
[13:12:28] ================================================================================
[13:12:28] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:28] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:28] ❌ Trial 389 failed with error: expected scalar type Long but found Float
[13:12:28] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:28] ✓ Results saved to hyperparameter_results.json
[13:12:28] ================================================================================
[13:12:28] TRIAL 390 - Starting
[13:12:28] ================================================================================
[13:12:28] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:28] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:29] ❌ Trial 390 failed with error: expected scalar type Long but found Float
[13:12:29] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:29] ✓ Results saved to hyperparameter_results.json
[13:12:29] 
================================================================================
[13:12:29] PROGRESS UPDATE - Completed 390 trials
[13:12:29] Best so far: 0.0000
[13:12:29] ================================================================================

[13:12:29] ================================================================================
[13:12:29] TRIAL 391 - Starting
[13:12:29] ================================================================================
[13:12:29] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:29] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:29] ❌ Trial 391 failed with error: expected scalar type Long but found Float
[13:12:29] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:29] ✓ Results saved to hyperparameter_results.json
[13:12:29] ================================================================================
[13:12:29] TRIAL 392 - Starting
[13:12:29] ================================================================================
[13:12:29] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:29] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:29] ❌ Trial 392 failed with error: expected scalar type Long but found Float
[13:12:29] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:29] ✓ Results saved to hyperparameter_results.json
[13:12:29] ================================================================================
[13:12:29] TRIAL 393 - Starting
[13:12:29] ================================================================================
[13:12:29] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:29] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:29] ❌ Trial 393 failed with error: expected scalar type Long but found Float
[13:12:29] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:29] ✓ Results saved to hyperparameter_results.json
[13:12:29] ================================================================================
[13:12:29] TRIAL 394 - Starting
[13:12:29] ================================================================================
[13:12:29] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:29] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:29] ❌ Trial 394 failed with error: expected scalar type Long but found Float
[13:12:29] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:29] ✓ Results saved to hyperparameter_results.json
[13:12:29] ================================================================================
[13:12:29] TRIAL 395 - Starting
[13:12:29] ================================================================================
[13:12:29] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:29] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:30] ❌ Trial 395 failed with error: expected scalar type Long but found Float
[13:12:30] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:30] ✓ Results saved to hyperparameter_results.json
[13:12:30] 
================================================================================
[13:12:30] PROGRESS UPDATE - Completed 395 trials
[13:12:30] Best so far: 0.0000
[13:12:30] ================================================================================

[13:12:30] ================================================================================
[13:12:30] TRIAL 396 - Starting
[13:12:30] ================================================================================
[13:12:30] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:30] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:30] ❌ Trial 396 failed with error: expected scalar type Long but found Float
[13:12:30] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:30] ✓ Results saved to hyperparameter_results.json
[13:12:30] ================================================================================
[13:12:30] TRIAL 397 - Starting
[13:12:30] ================================================================================
[13:12:30] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:30] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:30] ❌ Trial 397 failed with error: expected scalar type Long but found Float
[13:12:30] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:30] ✓ Results saved to hyperparameter_results.json
[13:12:30] ================================================================================
[13:12:30] TRIAL 398 - Starting
[13:12:30] ================================================================================
[13:12:30] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:30] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:30] ❌ Trial 398 failed with error: expected scalar type Long but found Float
[13:12:30] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:30] ✓ Results saved to hyperparameter_results.json
[13:12:30] ================================================================================
[13:12:30] TRIAL 399 - Starting
[13:12:30] ================================================================================
[13:12:30] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:30] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:30] ❌ Trial 399 failed with error: expected scalar type Long but found Float
[13:12:30] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:30] ✓ Results saved to hyperparameter_results.json
[13:12:30] ================================================================================
[13:12:30] TRIAL 400 - Starting
[13:12:30] ================================================================================
[13:12:30] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:30] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:30] ❌ Trial 400 failed with error: expected scalar type Long but found Float
[13:12:30] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:30] ✓ Results saved to hyperparameter_results.json
[13:12:30] 
================================================================================
[13:12:30] PROGRESS UPDATE - Completed 400 trials
[13:12:30] Best so far: 0.0000
[13:12:30] ================================================================================

[13:12:30] ================================================================================
[13:12:30] TRIAL 401 - Starting
[13:12:30] ================================================================================
[13:12:30] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:30] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:30] ❌ Trial 401 failed with error: expected scalar type Long but found Float
[13:12:30] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:30] ✓ Results saved to hyperparameter_results.json
[13:12:30] ================================================================================
[13:12:30] TRIAL 402 - Starting
[13:12:30] ================================================================================
[13:12:30] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:30] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:30] ❌ Trial 402 failed with error: expected scalar type Long but found Float
[13:12:30] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:30] ✓ Results saved to hyperparameter_results.json
[13:12:30] ================================================================================
[13:12:30] TRIAL 403 - Starting
[13:12:30] ================================================================================
[13:12:30] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:30] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:31] ❌ Trial 403 failed with error: expected scalar type Long but found Float
[13:12:31] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:31] ✓ Results saved to hyperparameter_results.json
[13:12:31] ================================================================================
[13:12:31] TRIAL 404 - Starting
[13:12:31] ================================================================================
[13:12:31] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:31] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:31] ❌ Trial 404 failed with error: expected scalar type Long but found Float
[13:12:31] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:31] ✓ Results saved to hyperparameter_results.json
[13:12:31] ================================================================================
[13:12:31] TRIAL 405 - Starting
[13:12:31] ================================================================================
[13:12:31] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:31] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:31] ❌ Trial 405 failed with error: expected scalar type Long but found Float
[13:12:31] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:31] ✓ Results saved to hyperparameter_results.json
[13:12:31] 
================================================================================
[13:12:31] PROGRESS UPDATE - Completed 405 trials
[13:12:31] Best so far: 0.0000
[13:12:31] ================================================================================

[13:12:31] ================================================================================
[13:12:31] TRIAL 406 - Starting
[13:12:31] ================================================================================
[13:12:31] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:31] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:31] ❌ Trial 406 failed with error: expected scalar type Long but found Float
[13:12:31] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:31] ✓ Results saved to hyperparameter_results.json
[13:12:31] ================================================================================
[13:12:31] TRIAL 407 - Starting
[13:12:31] ================================================================================
[13:12:31] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:31] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:31] ❌ Trial 407 failed with error: expected scalar type Long but found Float
[13:12:31] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:31] ✓ Results saved to hyperparameter_results.json
[13:12:31] ================================================================================
[13:12:31] TRIAL 408 - Starting
[13:12:31] ================================================================================
[13:12:31] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:31] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:31] ❌ Trial 408 failed with error: expected scalar type Long but found Float
[13:12:31] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:31] ✓ Results saved to hyperparameter_results.json
[13:12:31] ================================================================================
[13:12:31] TRIAL 409 - Starting
[13:12:31] ================================================================================
[13:12:31] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:32] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:32] ❌ Trial 409 failed with error: expected scalar type Long but found Float
[13:12:32] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:32] ✓ Results saved to hyperparameter_results.json
[13:12:32] ================================================================================
[13:12:32] TRIAL 410 - Starting
[13:12:32] ================================================================================
[13:12:32] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:32] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:32] ❌ Trial 410 failed with error: expected scalar type Long but found Float
[13:12:32] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:32] ✓ Results saved to hyperparameter_results.json
[13:12:32] 
================================================================================
[13:12:32] PROGRESS UPDATE - Completed 410 trials
[13:12:32] Best so far: 0.0000
[13:12:32] ================================================================================

[13:12:32] ================================================================================
[13:12:32] TRIAL 411 - Starting
[13:12:32] ================================================================================
[13:12:32] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:32] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:32] ❌ Trial 411 failed with error: expected scalar type Long but found Float
[13:12:32] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:32] ✓ Results saved to hyperparameter_results.json
[13:12:32] ================================================================================
[13:12:32] TRIAL 412 - Starting
[13:12:32] ================================================================================
[13:12:32] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:32] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:32] ❌ Trial 412 failed with error: expected scalar type Long but found Float
[13:12:32] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:32] ✓ Results saved to hyperparameter_results.json
[13:12:32] ================================================================================
[13:12:32] TRIAL 413 - Starting
[13:12:32] ================================================================================
[13:12:32] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:32] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:32] ❌ Trial 413 failed with error: expected scalar type Long but found Float
[13:12:32] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:32] ✓ Results saved to hyperparameter_results.json
[13:12:32] ================================================================================
[13:12:32] TRIAL 414 - Starting
[13:12:32] ================================================================================
[13:12:32] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:32] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:32] ❌ Trial 414 failed with error: expected scalar type Long but found Float
[13:12:32] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:32] ✓ Results saved to hyperparameter_results.json
[13:12:32] ================================================================================
[13:12:32] TRIAL 415 - Starting
[13:12:32] ================================================================================
[13:12:32] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:32] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:32] ❌ Trial 415 failed with error: expected scalar type Long but found Float
[13:12:32] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:33] ✓ Results saved to hyperparameter_results.json
[13:12:33] 
================================================================================
[13:12:33] PROGRESS UPDATE - Completed 415 trials
[13:12:33] Best so far: 0.0000
[13:12:33] ================================================================================

[13:12:33] ================================================================================
[13:12:33] TRIAL 416 - Starting
[13:12:33] ================================================================================
[13:12:33] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:33] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:33] ❌ Trial 416 failed with error: expected scalar type Long but found Float
[13:12:33] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:33] ✓ Results saved to hyperparameter_results.json
[13:12:33] ================================================================================
[13:12:33] TRIAL 417 - Starting
[13:12:33] ================================================================================
[13:12:33] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:33] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:33] ❌ Trial 417 failed with error: expected scalar type Long but found Float
[13:12:33] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:33] ✓ Results saved to hyperparameter_results.json
[13:12:33] ================================================================================
[13:12:33] TRIAL 418 - Starting
[13:12:33] ================================================================================
[13:12:33] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:33] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:33] ❌ Trial 418 failed with error: expected scalar type Long but found Float
[13:12:33] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:33] ✓ Results saved to hyperparameter_results.json
[13:12:33] ================================================================================
[13:12:33] TRIAL 419 - Starting
[13:12:33] ================================================================================
[13:12:33] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:33] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:33] ❌ Trial 419 failed with error: expected scalar type Long but found Float
[13:12:33] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:33] ✓ Results saved to hyperparameter_results.json
[13:12:33] ================================================================================
[13:12:33] TRIAL 420 - Starting
[13:12:33] ================================================================================
[13:12:33] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:33] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:33] ❌ Trial 420 failed with error: expected scalar type Long but found Float
[13:12:33] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:33] ✓ Results saved to hyperparameter_results.json
[13:12:33] 
================================================================================
[13:12:33] PROGRESS UPDATE - Completed 420 trials
[13:12:33] Best so far: 0.0000
[13:12:33] ================================================================================

[13:12:33] ================================================================================
[13:12:33] TRIAL 421 - Starting
[13:12:33] ================================================================================
[13:12:33] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:33] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:33] ❌ Trial 421 failed with error: expected scalar type Long but found Float
[13:12:33] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:33] ✓ Results saved to hyperparameter_results.json
[13:12:33] ================================================================================
[13:12:33] TRIAL 422 - Starting
[13:12:33] ================================================================================
[13:12:33] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:33] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:34] ❌ Trial 422 failed with error: expected scalar type Long but found Float
[13:12:34] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:34] ✓ Results saved to hyperparameter_results.json
[13:12:34] ================================================================================
[13:12:34] TRIAL 423 - Starting
[13:12:34] ================================================================================
[13:12:34] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:34] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:34] ❌ Trial 423 failed with error: expected scalar type Long but found Float
[13:12:34] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:34] ✓ Results saved to hyperparameter_results.json
[13:12:34] ================================================================================
[13:12:34] TRIAL 424 - Starting
[13:12:34] ================================================================================
[13:12:34] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:34] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:34] ❌ Trial 424 failed with error: expected scalar type Long but found Float
[13:12:34] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:34] ✓ Results saved to hyperparameter_results.json
[13:12:34] ================================================================================
[13:12:34] TRIAL 425 - Starting
[13:12:34] ================================================================================
[13:12:34] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:34] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:34] ❌ Trial 425 failed with error: expected scalar type Long but found Float
[13:12:34] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:34] ✓ Results saved to hyperparameter_results.json
[13:12:34] 
================================================================================
[13:12:34] PROGRESS UPDATE - Completed 425 trials
[13:12:34] Best so far: 0.0000
[13:12:34] ================================================================================

[13:12:34] ================================================================================
[13:12:34] TRIAL 426 - Starting
[13:12:34] ================================================================================
[13:12:34] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:34] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:34] ❌ Trial 426 failed with error: expected scalar type Long but found Float
[13:12:34] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:34] ✓ Results saved to hyperparameter_results.json
[13:12:34] ================================================================================
[13:12:34] TRIAL 427 - Starting
[13:12:34] ================================================================================
[13:12:34] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:34] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:34] ❌ Trial 427 failed with error: expected scalar type Long but found Float
[13:12:34] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:34] ✓ Results saved to hyperparameter_results.json
[13:12:34] ================================================================================
[13:12:34] TRIAL 428 - Starting
[13:12:34] ================================================================================
[13:12:34] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:34] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:34] ❌ Trial 428 failed with error: expected scalar type Long but found Float
[13:12:34] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:34] ✓ Results saved to hyperparameter_results.json
[13:12:34] ================================================================================
[13:12:34] TRIAL 429 - Starting
[13:12:34] ================================================================================
[13:12:34] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:34] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:35] ❌ Trial 429 failed with error: expected scalar type Long but found Float
[13:12:35] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:35] ✓ Results saved to hyperparameter_results.json
[13:12:35] ================================================================================
[13:12:35] TRIAL 430 - Starting
[13:12:35] ================================================================================
[13:12:35] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:35] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:35] ❌ Trial 430 failed with error: expected scalar type Long but found Float
[13:12:35] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:35] ✓ Results saved to hyperparameter_results.json
[13:12:35] 
================================================================================
[13:12:35] PROGRESS UPDATE - Completed 430 trials
[13:12:35] Best so far: 0.0000
[13:12:35] ================================================================================

[13:12:35] ================================================================================
[13:12:35] TRIAL 431 - Starting
[13:12:35] ================================================================================
[13:12:35] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:35] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:35] ❌ Trial 431 failed with error: expected scalar type Long but found Float
[13:12:35] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:35] ✓ Results saved to hyperparameter_results.json
[13:12:35] ================================================================================
[13:12:35] TRIAL 432 - Starting
[13:12:35] ================================================================================
[13:12:35] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:35] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:35] ❌ Trial 432 failed with error: expected scalar type Long but found Float
[13:12:35] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:35] ✓ Results saved to hyperparameter_results.json
[13:12:35] ================================================================================
[13:12:35] TRIAL 433 - Starting
[13:12:35] ================================================================================
[13:12:35] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:35] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:35] ❌ Trial 433 failed with error: expected scalar type Long but found Float
[13:12:35] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:35] ✓ Results saved to hyperparameter_results.json
[13:12:35] ================================================================================
[13:12:35] TRIAL 434 - Starting
[13:12:35] ================================================================================
[13:12:35] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:35] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:35] ❌ Trial 434 failed with error: expected scalar type Long but found Float
[13:12:35] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:35] ✓ Results saved to hyperparameter_results.json
[13:12:35] ================================================================================
[13:12:35] TRIAL 435 - Starting
[13:12:35] ================================================================================
[13:12:35] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:35] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:35] ❌ Trial 435 failed with error: expected scalar type Long but found Float
[13:12:35] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:35] ✓ Results saved to hyperparameter_results.json
[13:12:35] 
================================================================================
[13:12:35] PROGRESS UPDATE - Completed 435 trials
[13:12:35] Best so far: 0.0000
[13:12:35] ================================================================================

[13:12:35] ================================================================================
[13:12:35] TRIAL 436 - Starting
[13:12:35] ================================================================================
[13:12:35] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:35] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:35] ❌ Trial 436 failed with error: expected scalar type Long but found Float
[13:12:35] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:35] ✓ Results saved to hyperparameter_results.json
[13:12:35] ================================================================================
[13:12:35] TRIAL 437 - Starting
[13:12:35] ================================================================================
[13:12:35] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:35] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:36] ❌ Trial 437 failed with error: expected scalar type Long but found Float
[13:12:36] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:36] ✓ Results saved to hyperparameter_results.json
[13:12:36] ================================================================================
[13:12:36] TRIAL 438 - Starting
[13:12:36] ================================================================================
[13:12:36] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:36] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:36] ❌ Trial 438 failed with error: expected scalar type Long but found Float
[13:12:36] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:36] ✓ Results saved to hyperparameter_results.json
[13:12:36] ================================================================================
[13:12:36] TRIAL 439 - Starting
[13:12:36] ================================================================================
[13:12:36] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:36] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:36] ❌ Trial 439 failed with error: expected scalar type Long but found Float
[13:12:36] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:36] ✓ Results saved to hyperparameter_results.json
[13:12:36] ================================================================================
[13:12:36] TRIAL 440 - Starting
[13:12:36] ================================================================================
[13:12:36] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:36] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:36] ❌ Trial 440 failed with error: expected scalar type Long but found Float
[13:12:36] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:36] ✓ Results saved to hyperparameter_results.json
[13:12:36] 
================================================================================
[13:12:36] PROGRESS UPDATE - Completed 440 trials
[13:12:36] Best so far: 0.0000
[13:12:36] ================================================================================

[13:12:36] ================================================================================
[13:12:36] TRIAL 441 - Starting
[13:12:36] ================================================================================
[13:12:36] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:36] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:36] ❌ Trial 441 failed with error: expected scalar type Long but found Float
[13:12:36] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:36] ✓ Results saved to hyperparameter_results.json
[13:12:36] ================================================================================
[13:12:36] TRIAL 442 - Starting
[13:12:36] ================================================================================
[13:12:36] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:36] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:36] ❌ Trial 442 failed with error: expected scalar type Long but found Float
[13:12:36] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:36] ✓ Results saved to hyperparameter_results.json
[13:12:36] ================================================================================
[13:12:36] TRIAL 443 - Starting
[13:12:36] ================================================================================
[13:12:36] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:36] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:36] ❌ Trial 443 failed with error: expected scalar type Long but found Float
[13:12:36] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:36] ✓ Results saved to hyperparameter_results.json
[13:12:36] ================================================================================
[13:12:36] TRIAL 444 - Starting
[13:12:36] ================================================================================
[13:12:36] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:36] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:37] ❌ Trial 444 failed with error: expected scalar type Long but found Float
[13:12:37] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:37] ✓ Results saved to hyperparameter_results.json
[13:12:37] ================================================================================
[13:12:37] TRIAL 445 - Starting
[13:12:37] ================================================================================
[13:12:37] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:37] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:37] ❌ Trial 445 failed with error: expected scalar type Long but found Float
[13:12:37] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:37] ✓ Results saved to hyperparameter_results.json
[13:12:37] 
================================================================================
[13:12:37] PROGRESS UPDATE - Completed 445 trials
[13:12:37] Best so far: 0.0000
[13:12:37] ================================================================================

[13:12:37] ================================================================================
[13:12:37] TRIAL 446 - Starting
[13:12:37] ================================================================================
[13:12:37] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:37] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:37] ❌ Trial 446 failed with error: expected scalar type Long but found Float
[13:12:37] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:37] ✓ Results saved to hyperparameter_results.json
[13:12:37] ================================================================================
[13:12:37] TRIAL 447 - Starting
[13:12:37] ================================================================================
[13:12:37] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:37] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:37] ❌ Trial 447 failed with error: expected scalar type Long but found Float
[13:12:37] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:37] ✓ Results saved to hyperparameter_results.json
[13:12:37] ================================================================================
[13:12:37] TRIAL 448 - Starting
[13:12:37] ================================================================================
[13:12:37] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:37] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:37] ❌ Trial 448 failed with error: expected scalar type Long but found Float
[13:12:37] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:37] ✓ Results saved to hyperparameter_results.json
[13:12:37] ================================================================================
[13:12:37] TRIAL 449 - Starting
[13:12:37] ================================================================================
[13:12:37] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:37] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:37] ❌ Trial 449 failed with error: expected scalar type Long but found Float
[13:12:37] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:37] ✓ Results saved to hyperparameter_results.json
[13:12:37] ================================================================================
[13:12:37] TRIAL 450 - Starting
[13:12:37] ================================================================================
[13:12:37] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:37] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:37] ❌ Trial 450 failed with error: expected scalar type Long but found Float
[13:12:37] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:37] ✓ Results saved to hyperparameter_results.json
[13:12:37] 
================================================================================
[13:12:37] PROGRESS UPDATE - Completed 450 trials
[13:12:37] Best so far: 0.0000
[13:12:37] ================================================================================

[13:12:37] ================================================================================
[13:12:37] TRIAL 451 - Starting
[13:12:37] ================================================================================
[13:12:37] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:37] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:38] ❌ Trial 451 failed with error: expected scalar type Long but found Float
[13:12:38] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:38] ✓ Results saved to hyperparameter_results.json
[13:12:38] ================================================================================
[13:12:38] TRIAL 452 - Starting
[13:12:38] ================================================================================
[13:12:38] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:38] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:38] ❌ Trial 452 failed with error: expected scalar type Long but found Float
[13:12:38] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:38] ✓ Results saved to hyperparameter_results.json
[13:12:38] ================================================================================
[13:12:38] TRIAL 453 - Starting
[13:12:38] ================================================================================
[13:12:38] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:38] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:38] ❌ Trial 453 failed with error: expected scalar type Long but found Float
[13:12:38] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:38] ✓ Results saved to hyperparameter_results.json
[13:12:38] ================================================================================
[13:12:38] TRIAL 454 - Starting
[13:12:38] ================================================================================
[13:12:38] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:38] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:38] ❌ Trial 454 failed with error: expected scalar type Long but found Float
[13:12:38] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:38] ✓ Results saved to hyperparameter_results.json
[13:12:38] ================================================================================
[13:12:38] TRIAL 455 - Starting
[13:12:38] ================================================================================
[13:12:38] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:38] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:38] ❌ Trial 455 failed with error: expected scalar type Long but found Float
[13:12:38] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:38] ✓ Results saved to hyperparameter_results.json
[13:12:38] 
================================================================================
[13:12:38] PROGRESS UPDATE - Completed 455 trials
[13:12:38] Best so far: 0.0000
[13:12:38] ================================================================================

[13:12:38] ================================================================================
[13:12:38] TRIAL 456 - Starting
[13:12:38] ================================================================================
[13:12:38] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:38] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:38] ❌ Trial 456 failed with error: expected scalar type Long but found Float
[13:12:38] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:38] ✓ Results saved to hyperparameter_results.json
[13:12:38] ================================================================================
[13:12:38] TRIAL 457 - Starting
[13:12:38] ================================================================================
[13:12:38] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:38] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:38] ❌ Trial 457 failed with error: expected scalar type Long but found Float
[13:12:38] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:38] ✓ Results saved to hyperparameter_results.json
[13:12:38] ================================================================================
[13:12:38] TRIAL 458 - Starting
[13:12:38] ================================================================================
[13:12:38] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:39] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:39] ❌ Trial 458 failed with error: expected scalar type Long but found Float
[13:12:39] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:39] ✓ Results saved to hyperparameter_results.json
[13:12:39] ================================================================================
[13:12:39] TRIAL 459 - Starting
[13:12:39] ================================================================================
[13:12:39] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:39] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:39] ❌ Trial 459 failed with error: expected scalar type Long but found Float
[13:12:39] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:39] ✓ Results saved to hyperparameter_results.json
[13:12:39] ================================================================================
[13:12:39] TRIAL 460 - Starting
[13:12:39] ================================================================================
[13:12:39] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:39] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:39] ❌ Trial 460 failed with error: expected scalar type Long but found Float
[13:12:39] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:39] ✓ Results saved to hyperparameter_results.json
[13:12:39] 
================================================================================
[13:12:39] PROGRESS UPDATE - Completed 460 trials
[13:12:39] Best so far: 0.0000
[13:12:39] ================================================================================

[13:12:39] ================================================================================
[13:12:39] TRIAL 461 - Starting
[13:12:39] ================================================================================
[13:12:39] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:39] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:39] ❌ Trial 461 failed with error: expected scalar type Long but found Float
[13:12:39] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:39] ✓ Results saved to hyperparameter_results.json
[13:12:39] ================================================================================
[13:12:39] TRIAL 462 - Starting
[13:12:39] ================================================================================
[13:12:39] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:39] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:39] ❌ Trial 462 failed with error: expected scalar type Long but found Float
[13:12:39] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:39] ✓ Results saved to hyperparameter_results.json
[13:12:39] ================================================================================
[13:12:39] TRIAL 463 - Starting
[13:12:39] ================================================================================
[13:12:39] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:39] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:39] ❌ Trial 463 failed with error: expected scalar type Long but found Float
[13:12:39] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:39] ✓ Results saved to hyperparameter_results.json
[13:12:39] ================================================================================
[13:12:39] TRIAL 464 - Starting
[13:12:39] ================================================================================
[13:12:39] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:39] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:39] ❌ Trial 464 failed with error: expected scalar type Long but found Float
[13:12:39] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:39] ✓ Results saved to hyperparameter_results.json
[13:12:39] ================================================================================
[13:12:39] TRIAL 465 - Starting
[13:12:39] ================================================================================
[13:12:39] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:39] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:40] ❌ Trial 465 failed with error: expected scalar type Long but found Float
[13:12:40] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:40] ✓ Results saved to hyperparameter_results.json
[13:12:40] 
================================================================================
[13:12:40] PROGRESS UPDATE - Completed 465 trials
[13:12:40] Best so far: 0.0000
[13:12:40] ================================================================================

[13:12:40] ================================================================================
[13:12:40] TRIAL 466 - Starting
[13:12:40] ================================================================================
[13:12:40] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:40] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:40] ❌ Trial 466 failed with error: expected scalar type Long but found Float
[13:12:40] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:40] ✓ Results saved to hyperparameter_results.json
[13:12:40] ================================================================================
[13:12:40] TRIAL 467 - Starting
[13:12:40] ================================================================================
[13:12:40] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:40] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:40] ❌ Trial 467 failed with error: expected scalar type Long but found Float
[13:12:40] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:40] ✓ Results saved to hyperparameter_results.json
[13:12:40] ================================================================================
[13:12:40] TRIAL 468 - Starting
[13:12:40] ================================================================================
[13:12:40] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:40] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:40] ❌ Trial 468 failed with error: expected scalar type Long but found Float
[13:12:40] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:40] ✓ Results saved to hyperparameter_results.json
[13:12:40] ================================================================================
[13:12:40] TRIAL 469 - Starting
[13:12:40] ================================================================================
[13:12:40] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:40] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:40] ❌ Trial 469 failed with error: expected scalar type Long but found Float
[13:12:40] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:40] ✓ Results saved to hyperparameter_results.json
[13:12:40] ================================================================================
[13:12:40] TRIAL 470 - Starting
[13:12:40] ================================================================================
[13:12:40] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:40] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:40] ❌ Trial 470 failed with error: expected scalar type Long but found Float
[13:12:40] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:40] ✓ Results saved to hyperparameter_results.json
[13:12:40] 
================================================================================
[13:12:40] PROGRESS UPDATE - Completed 470 trials
[13:12:40] Best so far: 0.0000
[13:12:40] ================================================================================

[13:12:40] ================================================================================
[13:12:40] TRIAL 471 - Starting
[13:12:40] ================================================================================
[13:12:40] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:40] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:40] ❌ Trial 471 failed with error: expected scalar type Long but found Float
[13:12:40] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:40] ✓ Results saved to hyperparameter_results.json
[13:12:40] ================================================================================
[13:12:40] TRIAL 472 - Starting
[13:12:40] ================================================================================
[13:12:40] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:40] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:40] ❌ Trial 472 failed with error: expected scalar type Long but found Float
[13:12:40] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:41] ✓ Results saved to hyperparameter_results.json
[13:12:41] ================================================================================
[13:12:41] TRIAL 473 - Starting
[13:12:41] ================================================================================
[13:12:41] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:41] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:41] ❌ Trial 473 failed with error: expected scalar type Long but found Float
[13:12:41] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:41] ✓ Results saved to hyperparameter_results.json
[13:12:41] ================================================================================
[13:12:41] TRIAL 474 - Starting
[13:12:41] ================================================================================
[13:12:41] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:41] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:41] ❌ Trial 474 failed with error: expected scalar type Long but found Float
[13:12:41] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:41] ✓ Results saved to hyperparameter_results.json
[13:12:41] ================================================================================
[13:12:41] TRIAL 475 - Starting
[13:12:41] ================================================================================
[13:12:41] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:41] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:41] ❌ Trial 475 failed with error: expected scalar type Long but found Float
[13:12:41] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:41] ✓ Results saved to hyperparameter_results.json
[13:12:41] 
================================================================================
[13:12:41] PROGRESS UPDATE - Completed 475 trials
[13:12:41] Best so far: 0.0000
[13:12:41] ================================================================================

[13:12:41] ================================================================================
[13:12:41] TRIAL 476 - Starting
[13:12:41] ================================================================================
[13:12:41] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:41] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:41] ❌ Trial 476 failed with error: expected scalar type Long but found Float
[13:12:41] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:41] ✓ Results saved to hyperparameter_results.json
[13:12:41] ================================================================================
[13:12:41] TRIAL 477 - Starting
[13:12:41] ================================================================================
[13:12:41] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:41] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:41] ❌ Trial 477 failed with error: expected scalar type Long but found Float
[13:12:41] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:41] ✓ Results saved to hyperparameter_results.json
[13:12:41] ================================================================================
[13:12:41] TRIAL 478 - Starting
[13:12:41] ================================================================================
[13:12:41] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:41] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:41] ❌ Trial 478 failed with error: expected scalar type Long but found Float
[13:12:41] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:41] ✓ Results saved to hyperparameter_results.json
[13:12:41] ================================================================================
[13:12:41] TRIAL 479 - Starting
[13:12:41] ================================================================================
[13:12:41] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:41] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:41] ❌ Trial 479 failed with error: expected scalar type Long but found Float
[13:12:41] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:41] ✓ Results saved to hyperparameter_results.json
[13:12:41] ================================================================================
[13:12:41] TRIAL 480 - Starting
[13:12:41] ================================================================================
[13:12:41] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:41] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:42] ❌ Trial 480 failed with error: expected scalar type Long but found Float
[13:12:42] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:42] ✓ Results saved to hyperparameter_results.json
[13:12:42] 
================================================================================
[13:12:42] PROGRESS UPDATE - Completed 480 trials
[13:12:42] Best so far: 0.0000
[13:12:42] ================================================================================

[13:12:42] ================================================================================
[13:12:42] TRIAL 481 - Starting
[13:12:42] ================================================================================
[13:12:42] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:42] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:42] ❌ Trial 481 failed with error: expected scalar type Long but found Float
[13:12:42] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:42] ✓ Results saved to hyperparameter_results.json
[13:12:42] ================================================================================
[13:12:42] TRIAL 482 - Starting
[13:12:42] ================================================================================
[13:12:42] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:42] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:42] ❌ Trial 482 failed with error: expected scalar type Long but found Float
[13:12:42] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:42] ✓ Results saved to hyperparameter_results.json
[13:12:42] ================================================================================
[13:12:42] TRIAL 483 - Starting
[13:12:42] ================================================================================
[13:12:42] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:42] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:42] ❌ Trial 483 failed with error: expected scalar type Long but found Float
[13:12:42] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:42] ✓ Results saved to hyperparameter_results.json
[13:12:42] ================================================================================
[13:12:42] TRIAL 484 - Starting
[13:12:42] ================================================================================
[13:12:42] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:42] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:42] ❌ Trial 484 failed with error: expected scalar type Long but found Float
[13:12:42] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:42] ✓ Results saved to hyperparameter_results.json
[13:12:42] ================================================================================
[13:12:42] TRIAL 485 - Starting
[13:12:42] ================================================================================
[13:12:42] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:42] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:42] ❌ Trial 485 failed with error: expected scalar type Long but found Float
[13:12:42] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:42] ✓ Results saved to hyperparameter_results.json
[13:12:42] 
================================================================================
[13:12:42] PROGRESS UPDATE - Completed 485 trials
[13:12:42] Best so far: 0.0000
[13:12:42] ================================================================================

[13:12:42] ================================================================================
[13:12:42] TRIAL 486 - Starting
[13:12:42] ================================================================================
[13:12:42] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:42] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:42] ❌ Trial 486 failed with error: expected scalar type Long but found Float
[13:12:42] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:42] ✓ Results saved to hyperparameter_results.json
[13:12:42] ================================================================================
[13:12:42] TRIAL 487 - Starting
[13:12:42] ================================================================================
[13:12:42] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:42] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:43] ❌ Trial 487 failed with error: expected scalar type Long but found Float
[13:12:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:43] ✓ Results saved to hyperparameter_results.json
[13:12:43] ================================================================================
[13:12:43] TRIAL 488 - Starting
[13:12:43] ================================================================================
[13:12:43] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:43] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:43] ❌ Trial 488 failed with error: expected scalar type Long but found Float
[13:12:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:43] ✓ Results saved to hyperparameter_results.json
[13:12:43] ================================================================================
[13:12:43] TRIAL 489 - Starting
[13:12:43] ================================================================================
[13:12:43] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:43] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:43] ❌ Trial 489 failed with error: expected scalar type Long but found Float
[13:12:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:43] ✓ Results saved to hyperparameter_results.json
[13:12:43] ================================================================================
[13:12:43] TRIAL 490 - Starting
[13:12:43] ================================================================================
[13:12:43] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:43] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:43] ❌ Trial 490 failed with error: expected scalar type Long but found Float
[13:12:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:43] ✓ Results saved to hyperparameter_results.json
[13:12:43] 
================================================================================
[13:12:43] PROGRESS UPDATE - Completed 490 trials
[13:12:43] Best so far: 0.0000
[13:12:43] ================================================================================

[13:12:43] ================================================================================
[13:12:43] TRIAL 491 - Starting
[13:12:43] ================================================================================
[13:12:43] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:43] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:43] ❌ Trial 491 failed with error: expected scalar type Long but found Float
[13:12:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:43] ✓ Results saved to hyperparameter_results.json
[13:12:43] ================================================================================
[13:12:43] TRIAL 492 - Starting
[13:12:43] ================================================================================
[13:12:43] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:43] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:43] ❌ Trial 492 failed with error: expected scalar type Long but found Float
[13:12:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:43] ✓ Results saved to hyperparameter_results.json
[13:12:43] ================================================================================
[13:12:43] TRIAL 493 - Starting
[13:12:43] ================================================================================
[13:12:43] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:43] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:43] ❌ Trial 493 failed with error: expected scalar type Long but found Float
[13:12:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:43] ✓ Results saved to hyperparameter_results.json
[13:12:43] ================================================================================
[13:12:43] TRIAL 494 - Starting
[13:12:43] ================================================================================
[13:12:43] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:43] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:43] ❌ Trial 494 failed with error: expected scalar type Long but found Float
[13:12:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:43] ✓ Results saved to hyperparameter_results.json
[13:12:43] ================================================================================
[13:12:43] TRIAL 495 - Starting
[13:12:43] ================================================================================
[13:12:43] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:43] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:44] ❌ Trial 495 failed with error: expected scalar type Long but found Float
[13:12:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:44] ✓ Results saved to hyperparameter_results.json
[13:12:44] 
================================================================================
[13:12:44] PROGRESS UPDATE - Completed 495 trials
[13:12:44] Best so far: 0.0000
[13:12:44] ================================================================================

[13:12:44] ================================================================================
[13:12:44] TRIAL 496 - Starting
[13:12:44] ================================================================================
[13:12:44] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:44] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:44] ❌ Trial 496 failed with error: expected scalar type Long but found Float
[13:12:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:44] ✓ Results saved to hyperparameter_results.json
[13:12:44] ================================================================================
[13:12:44] TRIAL 497 - Starting
[13:12:44] ================================================================================
[13:12:44] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:44] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:44] ❌ Trial 497 failed with error: expected scalar type Long but found Float
[13:12:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:44] ✓ Results saved to hyperparameter_results.json
[13:12:44] ================================================================================
[13:12:44] TRIAL 498 - Starting
[13:12:44] ================================================================================
[13:12:44] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:44] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:44] ❌ Trial 498 failed with error: expected scalar type Long but found Float
[13:12:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:44] ✓ Results saved to hyperparameter_results.json
[13:12:44] ================================================================================
[13:12:44] TRIAL 499 - Starting
[13:12:44] ================================================================================
[13:12:44] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:44] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:44] ❌ Trial 499 failed with error: expected scalar type Long but found Float
[13:12:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:44] ✓ Results saved to hyperparameter_results.json
[13:12:44] ================================================================================
[13:12:44] TRIAL 500 - Starting
[13:12:44] ================================================================================
[13:12:44] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:44] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:44] ❌ Trial 500 failed with error: expected scalar type Long but found Float
[13:12:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:44] ✓ Results saved to hyperparameter_results.json
[13:12:44] 
================================================================================
[13:12:44] PROGRESS UPDATE - Completed 500 trials
[13:12:44] Best so far: 0.0000
[13:12:44] ================================================================================

[13:12:44] ================================================================================
[13:12:44] TRIAL 501 - Starting
[13:12:44] ================================================================================
[13:12:44] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:44] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:44] ❌ Trial 501 failed with error: expected scalar type Long but found Float
[13:12:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:44] ✓ Results saved to hyperparameter_results.json
[13:12:44] ================================================================================
[13:12:44] TRIAL 502 - Starting
[13:12:44] ================================================================================
[13:12:44] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:44] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:45] ❌ Trial 502 failed with error: expected scalar type Long but found Float
[13:12:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:45] ✓ Results saved to hyperparameter_results.json
[13:12:45] ================================================================================
[13:12:45] TRIAL 503 - Starting
[13:12:45] ================================================================================
[13:12:45] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:45] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:45] ❌ Trial 503 failed with error: expected scalar type Long but found Float
[13:12:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:45] ✓ Results saved to hyperparameter_results.json
[13:12:45] ================================================================================
[13:12:45] TRIAL 504 - Starting
[13:12:45] ================================================================================
[13:12:45] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:45] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:45] ❌ Trial 504 failed with error: expected scalar type Long but found Float
[13:12:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:45] ✓ Results saved to hyperparameter_results.json
[13:12:45] ================================================================================
[13:12:45] TRIAL 505 - Starting
[13:12:45] ================================================================================
[13:12:45] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:45] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:45] ❌ Trial 505 failed with error: expected scalar type Long but found Float
[13:12:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:45] ✓ Results saved to hyperparameter_results.json
[13:12:45] 
================================================================================
[13:12:45] PROGRESS UPDATE - Completed 505 trials
[13:12:45] Best so far: 0.0000
[13:12:45] ================================================================================

[13:12:45] ================================================================================
[13:12:45] TRIAL 506 - Starting
[13:12:45] ================================================================================
[13:12:45] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:45] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:45] ❌ Trial 506 failed with error: expected scalar type Long but found Float
[13:12:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:45] ✓ Results saved to hyperparameter_results.json
[13:12:45] ================================================================================
[13:12:45] TRIAL 507 - Starting
[13:12:45] ================================================================================
[13:12:45] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:45] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:45] ❌ Trial 507 failed with error: expected scalar type Long but found Float
[13:12:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:45] ✓ Results saved to hyperparameter_results.json
[13:12:45] ================================================================================
[13:12:45] TRIAL 508 - Starting
[13:12:45] ================================================================================
[13:12:45] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:45] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:45] ❌ Trial 508 failed with error: expected scalar type Long but found Float
[13:12:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:45] ✓ Results saved to hyperparameter_results.json
[13:12:45] ================================================================================
[13:12:45] TRIAL 509 - Starting
[13:12:45] ================================================================================
[13:12:45] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:45] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:45] ❌ Trial 509 failed with error: expected scalar type Long but found Float
[13:12:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:46] ✓ Results saved to hyperparameter_results.json
[13:12:46] ================================================================================
[13:12:46] TRIAL 510 - Starting
[13:12:46] ================================================================================
[13:12:46] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:46] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:46] ❌ Trial 510 failed with error: expected scalar type Long but found Float
[13:12:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:46] ✓ Results saved to hyperparameter_results.json
[13:12:46] 
================================================================================
[13:12:46] PROGRESS UPDATE - Completed 510 trials
[13:12:46] Best so far: 0.0000
[13:12:46] ================================================================================

[13:12:46] ================================================================================
[13:12:46] TRIAL 511 - Starting
[13:12:46] ================================================================================
[13:12:46] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:46] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:46] ❌ Trial 511 failed with error: expected scalar type Long but found Float
[13:12:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:46] ✓ Results saved to hyperparameter_results.json
[13:12:46] ================================================================================
[13:12:46] TRIAL 512 - Starting
[13:12:46] ================================================================================
[13:12:46] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:46] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:46] ❌ Trial 512 failed with error: expected scalar type Long but found Float
[13:12:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:46] ✓ Results saved to hyperparameter_results.json
[13:12:46] ================================================================================
[13:12:46] TRIAL 513 - Starting
[13:12:46] ================================================================================
[13:12:46] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:46] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:46] ❌ Trial 513 failed with error: expected scalar type Long but found Float
[13:12:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:46] ✓ Results saved to hyperparameter_results.json
[13:12:46] ================================================================================
[13:12:46] TRIAL 514 - Starting
[13:12:46] ================================================================================
[13:12:46] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:46] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:46] ❌ Trial 514 failed with error: expected scalar type Long but found Float
[13:12:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:46] ✓ Results saved to hyperparameter_results.json
[13:12:46] ================================================================================
[13:12:46] TRIAL 515 - Starting
[13:12:46] ================================================================================
[13:12:46] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:46] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:46] ❌ Trial 515 failed with error: expected scalar type Long but found Float
[13:12:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:46] ✓ Results saved to hyperparameter_results.json
[13:12:46] 
================================================================================
[13:12:46] PROGRESS UPDATE - Completed 515 trials
[13:12:46] Best so far: 0.0000
[13:12:46] ================================================================================

[13:12:46] ================================================================================
[13:12:46] TRIAL 516 - Starting
[13:12:46] ================================================================================
[13:12:46] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:46] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:46] ❌ Trial 516 failed with error: expected scalar type Long but found Float
[13:12:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:46] ✓ Results saved to hyperparameter_results.json
[13:12:46] ================================================================================
[13:12:46] TRIAL 517 - Starting
[13:12:46] ================================================================================
[13:12:46] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:46] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:47] ❌ Trial 517 failed with error: expected scalar type Long but found Float
[13:12:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:47] ✓ Results saved to hyperparameter_results.json
[13:12:47] ================================================================================
[13:12:47] TRIAL 518 - Starting
[13:12:47] ================================================================================
[13:12:47] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:47] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:47] ❌ Trial 518 failed with error: expected scalar type Long but found Float
[13:12:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:47] ✓ Results saved to hyperparameter_results.json
[13:12:47] ================================================================================
[13:12:47] TRIAL 519 - Starting
[13:12:47] ================================================================================
[13:12:47] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:47] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:47] ❌ Trial 519 failed with error: expected scalar type Long but found Float
[13:12:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:47] ✓ Results saved to hyperparameter_results.json
[13:12:47] ================================================================================
[13:12:47] TRIAL 520 - Starting
[13:12:47] ================================================================================
[13:12:47] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:47] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:47] ❌ Trial 520 failed with error: expected scalar type Long but found Float
[13:12:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:47] ✓ Results saved to hyperparameter_results.json
[13:12:47] 
================================================================================
[13:12:47] PROGRESS UPDATE - Completed 520 trials
[13:12:47] Best so far: 0.0000
[13:12:47] ================================================================================

[13:12:47] ================================================================================
[13:12:47] TRIAL 521 - Starting
[13:12:47] ================================================================================
[13:12:47] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:47] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:47] ❌ Trial 521 failed with error: expected scalar type Long but found Float
[13:12:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:47] ✓ Results saved to hyperparameter_results.json
[13:12:47] ================================================================================
[13:12:47] TRIAL 522 - Starting
[13:12:47] ================================================================================
[13:12:47] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:47] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:47] ❌ Trial 522 failed with error: expected scalar type Long but found Float
[13:12:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:47] ✓ Results saved to hyperparameter_results.json
[13:12:47] ================================================================================
[13:12:47] TRIAL 523 - Starting
[13:12:47] ================================================================================
[13:12:47] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:47] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:47] ❌ Trial 523 failed with error: expected scalar type Long but found Float
[13:12:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:47] ✓ Results saved to hyperparameter_results.json
[13:12:47] ================================================================================
[13:12:47] TRIAL 524 - Starting
[13:12:47] ================================================================================
[13:12:47] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:47] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:47] ❌ Trial 524 failed with error: expected scalar type Long but found Float
[13:12:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:48] ✓ Results saved to hyperparameter_results.json
[13:12:48] ================================================================================
[13:12:48] TRIAL 525 - Starting
[13:12:48] ================================================================================
[13:12:48] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:48] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:48] ❌ Trial 525 failed with error: expected scalar type Long but found Float
[13:12:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:48] ✓ Results saved to hyperparameter_results.json
[13:12:48] 
================================================================================
[13:12:48] PROGRESS UPDATE - Completed 525 trials
[13:12:48] Best so far: 0.0000
[13:12:48] ================================================================================

[13:12:48] ================================================================================
[13:12:48] TRIAL 526 - Starting
[13:12:48] ================================================================================
[13:12:48] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:48] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:48] ❌ Trial 526 failed with error: expected scalar type Long but found Float
[13:12:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:48] ✓ Results saved to hyperparameter_results.json
[13:12:48] ================================================================================
[13:12:48] TRIAL 527 - Starting
[13:12:48] ================================================================================
[13:12:48] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:48] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:48] ❌ Trial 527 failed with error: expected scalar type Long but found Float
[13:12:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:48] ✓ Results saved to hyperparameter_results.json
[13:12:48] ================================================================================
[13:12:48] TRIAL 528 - Starting
[13:12:48] ================================================================================
[13:12:48] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:48] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:48] ❌ Trial 528 failed with error: expected scalar type Long but found Float
[13:12:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:48] ✓ Results saved to hyperparameter_results.json
[13:12:48] ================================================================================
[13:12:48] TRIAL 529 - Starting
[13:12:48] ================================================================================
[13:12:48] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:48] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:48] ❌ Trial 529 failed with error: expected scalar type Long but found Float
[13:12:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:48] ✓ Results saved to hyperparameter_results.json
[13:12:48] ================================================================================
[13:12:48] TRIAL 530 - Starting
[13:12:48] ================================================================================
[13:12:48] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:48] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:48] ❌ Trial 530 failed with error: expected scalar type Long but found Float
[13:12:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:48] ✓ Results saved to hyperparameter_results.json
[13:12:48] 
================================================================================
[13:12:48] PROGRESS UPDATE - Completed 530 trials
[13:12:48] Best so far: 0.0000
[13:12:48] ================================================================================

[13:12:48] ================================================================================
[13:12:48] TRIAL 531 - Starting
[13:12:48] ================================================================================
[13:12:48] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:48] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:49] ❌ Trial 531 failed with error: expected scalar type Long but found Float
[13:12:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:49] ✓ Results saved to hyperparameter_results.json
[13:12:49] ================================================================================
[13:12:49] TRIAL 532 - Starting
[13:12:49] ================================================================================
[13:12:49] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:49] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:49] ❌ Trial 532 failed with error: expected scalar type Long but found Float
[13:12:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:49] ✓ Results saved to hyperparameter_results.json
[13:12:49] ================================================================================
[13:12:49] TRIAL 533 - Starting
[13:12:49] ================================================================================
[13:12:49] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:49] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:49] ❌ Trial 533 failed with error: expected scalar type Long but found Float
[13:12:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:49] ✓ Results saved to hyperparameter_results.json
[13:12:49] ================================================================================
[13:12:49] TRIAL 534 - Starting
[13:12:49] ================================================================================
[13:12:49] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:49] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:49] ❌ Trial 534 failed with error: expected scalar type Long but found Float
[13:12:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:49] ✓ Results saved to hyperparameter_results.json
[13:12:49] ================================================================================
[13:12:49] TRIAL 535 - Starting
[13:12:49] ================================================================================
[13:12:49] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:49] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:49] ❌ Trial 535 failed with error: expected scalar type Long but found Float
[13:12:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:49] ✓ Results saved to hyperparameter_results.json
[13:12:49] 
================================================================================
[13:12:49] PROGRESS UPDATE - Completed 535 trials
[13:12:49] Best so far: 0.0000
[13:12:49] ================================================================================

[13:12:49] ================================================================================
[13:12:49] TRIAL 536 - Starting
[13:12:49] ================================================================================
[13:12:49] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:49] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:49] ❌ Trial 536 failed with error: expected scalar type Long but found Float
[13:12:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:49] ✓ Results saved to hyperparameter_results.json
[13:12:49] ================================================================================
[13:12:49] TRIAL 537 - Starting
[13:12:49] ================================================================================
[13:12:49] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:49] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:49] ❌ Trial 537 failed with error: expected scalar type Long but found Float
[13:12:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:49] ✓ Results saved to hyperparameter_results.json
[13:12:49] ================================================================================
[13:12:49] TRIAL 538 - Starting
[13:12:49] ================================================================================
[13:12:49] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:49] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:49] ❌ Trial 538 failed with error: expected scalar type Long but found Float
[13:12:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:50] ✓ Results saved to hyperparameter_results.json
[13:12:50] ================================================================================
[13:12:50] TRIAL 539 - Starting
[13:12:50] ================================================================================
[13:12:50] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:50] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:50] ❌ Trial 539 failed with error: expected scalar type Long but found Float
[13:12:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:50] ✓ Results saved to hyperparameter_results.json
[13:12:50] ================================================================================
[13:12:50] TRIAL 540 - Starting
[13:12:50] ================================================================================
[13:12:50] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:50] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:50] ❌ Trial 540 failed with error: expected scalar type Long but found Float
[13:12:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:50] ✓ Results saved to hyperparameter_results.json
[13:12:50] 
================================================================================
[13:12:50] PROGRESS UPDATE - Completed 540 trials
[13:12:50] Best so far: 0.0000
[13:12:50] ================================================================================

[13:12:50] ================================================================================
[13:12:50] TRIAL 541 - Starting
[13:12:50] ================================================================================
[13:12:50] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:50] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:50] ❌ Trial 541 failed with error: expected scalar type Long but found Float
[13:12:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:50] ✓ Results saved to hyperparameter_results.json
[13:12:50] ================================================================================
[13:12:50] TRIAL 542 - Starting
[13:12:50] ================================================================================
[13:12:50] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:50] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:50] ❌ Trial 542 failed with error: expected scalar type Long but found Float
[13:12:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:50] ✓ Results saved to hyperparameter_results.json
[13:12:50] ================================================================================
[13:12:50] TRIAL 543 - Starting
[13:12:50] ================================================================================
[13:12:50] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:50] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:50] ❌ Trial 543 failed with error: expected scalar type Long but found Float
[13:12:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:50] ✓ Results saved to hyperparameter_results.json
[13:12:50] ================================================================================
[13:12:50] TRIAL 544 - Starting
[13:12:50] ================================================================================
[13:12:50] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:50] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:50] ❌ Trial 544 failed with error: expected scalar type Long but found Float
[13:12:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:50] ✓ Results saved to hyperparameter_results.json
[13:12:50] ================================================================================
[13:12:50] TRIAL 545 - Starting
[13:12:50] ================================================================================
[13:12:50] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:50] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:51] ❌ Trial 545 failed with error: expected scalar type Long but found Float
[13:12:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:51] ✓ Results saved to hyperparameter_results.json
[13:12:51] 
================================================================================
[13:12:51] PROGRESS UPDATE - Completed 545 trials
[13:12:51] Best so far: 0.0000
[13:12:51] ================================================================================

[13:12:51] ================================================================================
[13:12:51] TRIAL 546 - Starting
[13:12:51] ================================================================================
[13:12:51] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:51] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:51] ❌ Trial 546 failed with error: expected scalar type Long but found Float
[13:12:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:51] ✓ Results saved to hyperparameter_results.json
[13:12:51] ================================================================================
[13:12:51] TRIAL 547 - Starting
[13:12:51] ================================================================================
[13:12:51] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:51] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:51] ❌ Trial 547 failed with error: expected scalar type Long but found Float
[13:12:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:51] ✓ Results saved to hyperparameter_results.json
[13:12:51] ================================================================================
[13:12:51] TRIAL 548 - Starting
[13:12:51] ================================================================================
[13:12:51] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:51] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:51] ❌ Trial 548 failed with error: expected scalar type Long but found Float
[13:12:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:51] ✓ Results saved to hyperparameter_results.json
[13:12:51] ================================================================================
[13:12:51] TRIAL 549 - Starting
[13:12:51] ================================================================================
[13:12:51] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:51] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:51] ❌ Trial 549 failed with error: expected scalar type Long but found Float
[13:12:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:51] ✓ Results saved to hyperparameter_results.json
[13:12:51] ================================================================================
[13:12:51] TRIAL 550 - Starting
[13:12:51] ================================================================================
[13:12:51] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:51] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:51] ❌ Trial 550 failed with error: expected scalar type Long but found Float
[13:12:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:51] ✓ Results saved to hyperparameter_results.json
[13:12:51] 
================================================================================
[13:12:51] PROGRESS UPDATE - Completed 550 trials
[13:12:51] Best so far: 0.0000
[13:12:51] ================================================================================

[13:12:51] ================================================================================
[13:12:51] TRIAL 551 - Starting
[13:12:51] ================================================================================
[13:12:51] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:51] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:51] ❌ Trial 551 failed with error: expected scalar type Long but found Float
[13:12:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:51] ✓ Results saved to hyperparameter_results.json
[13:12:51] ================================================================================
[13:12:51] TRIAL 552 - Starting
[13:12:51] ================================================================================
[13:12:51] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:51] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:51] ❌ Trial 552 failed with error: expected scalar type Long but found Float
[13:12:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:52] ✓ Results saved to hyperparameter_results.json
[13:12:52] ================================================================================
[13:12:52] TRIAL 553 - Starting
[13:12:52] ================================================================================
[13:12:52] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:52] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:52] ❌ Trial 553 failed with error: expected scalar type Long but found Float
[13:12:52] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:52] ✓ Results saved to hyperparameter_results.json
[13:12:52] ================================================================================
[13:12:52] TRIAL 554 - Starting
[13:12:52] ================================================================================
[13:12:52] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:52] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:52] ❌ Trial 554 failed with error: expected scalar type Long but found Float
[13:12:52] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:52] ✓ Results saved to hyperparameter_results.json
[13:12:52] ================================================================================
[13:12:52] TRIAL 555 - Starting
[13:12:52] ================================================================================
[13:12:52] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:52] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:52] ❌ Trial 555 failed with error: expected scalar type Long but found Float
[13:12:52] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:52] ✓ Results saved to hyperparameter_results.json
[13:12:52] 
================================================================================
[13:12:52] PROGRESS UPDATE - Completed 555 trials
[13:12:52] Best so far: 0.0000
[13:12:52] ================================================================================

[13:12:52] ================================================================================
[13:12:52] TRIAL 556 - Starting
[13:12:52] ================================================================================
[13:12:52] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:52] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:52] ❌ Trial 556 failed with error: expected scalar type Long but found Float
[13:12:52] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:52] ✓ Results saved to hyperparameter_results.json
[13:12:52] ================================================================================
[13:12:52] TRIAL 557 - Starting
[13:12:52] ================================================================================
[13:12:52] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:52] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:52] ❌ Trial 557 failed with error: expected scalar type Long but found Float
[13:12:52] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:52] ✓ Results saved to hyperparameter_results.json
[13:12:52] ================================================================================
[13:12:52] TRIAL 558 - Starting
[13:12:52] ================================================================================
[13:12:52] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:52] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:52] ❌ Trial 558 failed with error: expected scalar type Long but found Float
[13:12:52] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:53] ✓ Results saved to hyperparameter_results.json
[13:12:53] ================================================================================
[13:12:53] TRIAL 559 - Starting
[13:12:53] ================================================================================
[13:12:53] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:53] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:53] ❌ Trial 559 failed with error: expected scalar type Long but found Float
[13:12:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:53] ✓ Results saved to hyperparameter_results.json
[13:12:53] ================================================================================
[13:12:53] TRIAL 560 - Starting
[13:12:53] ================================================================================
[13:12:53] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:53] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:53] ❌ Trial 560 failed with error: expected scalar type Long but found Float
[13:12:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:53] ✓ Results saved to hyperparameter_results.json
[13:12:53] 
================================================================================
[13:12:53] PROGRESS UPDATE - Completed 560 trials
[13:12:53] Best so far: 0.0000
[13:12:53] ================================================================================

[13:12:53] ================================================================================
[13:12:53] TRIAL 561 - Starting
[13:12:53] ================================================================================
[13:12:53] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:53] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:53] ❌ Trial 561 failed with error: expected scalar type Long but found Float
[13:12:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:53] ✓ Results saved to hyperparameter_results.json
[13:12:53] ================================================================================
[13:12:53] TRIAL 562 - Starting
[13:12:53] ================================================================================
[13:12:53] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:53] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:53] ❌ Trial 562 failed with error: expected scalar type Long but found Float
[13:12:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:53] ✓ Results saved to hyperparameter_results.json
[13:12:53] ================================================================================
[13:12:53] TRIAL 563 - Starting
[13:12:53] ================================================================================
[13:12:53] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:53] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:53] ❌ Trial 563 failed with error: expected scalar type Long but found Float
[13:12:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:53] ✓ Results saved to hyperparameter_results.json
[13:12:53] ================================================================================
[13:12:53] TRIAL 564 - Starting
[13:12:53] ================================================================================
[13:12:53] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:53] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:53] ❌ Trial 564 failed with error: expected scalar type Long but found Float
[13:12:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:53] ✓ Results saved to hyperparameter_results.json
[13:12:53] ================================================================================
[13:12:53] TRIAL 565 - Starting
[13:12:53] ================================================================================
[13:12:53] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:53] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:53] ❌ Trial 565 failed with error: expected scalar type Long but found Float
[13:12:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:54] ✓ Results saved to hyperparameter_results.json
[13:12:54] 
================================================================================
[13:12:54] PROGRESS UPDATE - Completed 565 trials
[13:12:54] Best so far: 0.0000
[13:12:54] ================================================================================

[13:12:54] ================================================================================
[13:12:54] TRIAL 566 - Starting
[13:12:54] ================================================================================
[13:12:54] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:54] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:54] ❌ Trial 566 failed with error: expected scalar type Long but found Float
[13:12:54] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:54] ✓ Results saved to hyperparameter_results.json
[13:12:54] ================================================================================
[13:12:54] TRIAL 567 - Starting
[13:12:54] ================================================================================
[13:12:54] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:54] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:54] ❌ Trial 567 failed with error: expected scalar type Long but found Float
[13:12:54] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:54] ✓ Results saved to hyperparameter_results.json
[13:12:54] ================================================================================
[13:12:54] TRIAL 568 - Starting
[13:12:54] ================================================================================
[13:12:54] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:54] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:54] ❌ Trial 568 failed with error: expected scalar type Long but found Float
[13:12:54] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:54] ✓ Results saved to hyperparameter_results.json
[13:12:54] ================================================================================
[13:12:54] TRIAL 569 - Starting
[13:12:54] ================================================================================
[13:12:54] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:54] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:54] ❌ Trial 569 failed with error: expected scalar type Long but found Float
[13:12:54] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:54] ✓ Results saved to hyperparameter_results.json
[13:12:54] ================================================================================
[13:12:54] TRIAL 570 - Starting
[13:12:54] ================================================================================
[13:12:54] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:54] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:54] ❌ Trial 570 failed with error: expected scalar type Long but found Float
[13:12:54] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:54] ✓ Results saved to hyperparameter_results.json
[13:12:54] 
================================================================================
[13:12:54] PROGRESS UPDATE - Completed 570 trials
[13:12:54] Best so far: 0.0000
[13:12:54] ================================================================================

[13:12:54] ================================================================================
[13:12:54] TRIAL 571 - Starting
[13:12:54] ================================================================================
[13:12:54] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:54] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:54] ❌ Trial 571 failed with error: expected scalar type Long but found Float
[13:12:54] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:55] ✓ Results saved to hyperparameter_results.json
[13:12:55] ================================================================================
[13:12:55] TRIAL 572 - Starting
[13:12:55] ================================================================================
[13:12:55] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:55] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:55] ❌ Trial 572 failed with error: expected scalar type Long but found Float
[13:12:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:55] ✓ Results saved to hyperparameter_results.json
[13:12:55] ================================================================================
[13:12:55] TRIAL 573 - Starting
[13:12:55] ================================================================================
[13:12:55] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:12:55] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:12:55] ❌ Trial 573 failed with error: expected scalar type Long but found Float
[13:12:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:55] ✓ Results saved to hyperparameter_results.json
[13:12:55] ================================================================================
[13:12:55] TRIAL 574 - Starting
[13:12:55] ================================================================================
[13:12:55] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:55] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:55] ❌ Trial 574 failed with error: expected scalar type Long but found Float
[13:12:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:55] ✓ Results saved to hyperparameter_results.json
[13:12:55] ================================================================================
[13:12:55] TRIAL 575 - Starting
[13:12:55] ================================================================================
[13:12:55] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:55] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:55] ❌ Trial 575 failed with error: expected scalar type Long but found Float
[13:12:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:55] ✓ Results saved to hyperparameter_results.json
[13:12:55] 
================================================================================
[13:12:55] PROGRESS UPDATE - Completed 575 trials
[13:12:55] Best so far: 0.0000
[13:12:55] ================================================================================

[13:12:55] ================================================================================
[13:12:55] TRIAL 576 - Starting
[13:12:55] ================================================================================
[13:12:55] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:55] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:55] ❌ Trial 576 failed with error: expected scalar type Long but found Float
[13:12:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:55] ✓ Results saved to hyperparameter_results.json
[13:12:55] ================================================================================
[13:12:55] TRIAL 577 - Starting
[13:12:55] ================================================================================
[13:12:55] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:55] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:55] ❌ Trial 577 failed with error: expected scalar type Long but found Float
[13:12:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:55] ✓ Results saved to hyperparameter_results.json
[13:12:55] ================================================================================
[13:12:55] TRIAL 578 - Starting
[13:12:55] ================================================================================
[13:12:55] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:55] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:55] ❌ Trial 578 failed with error: expected scalar type Long but found Float
[13:12:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:56] ✓ Results saved to hyperparameter_results.json
[13:12:56] ================================================================================
[13:12:56] TRIAL 579 - Starting
[13:12:56] ================================================================================
[13:12:56] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:56] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:56] ❌ Trial 579 failed with error: expected scalar type Long but found Float
[13:12:56] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:56] ✓ Results saved to hyperparameter_results.json
[13:12:56] ================================================================================
[13:12:56] TRIAL 580 - Starting
[13:12:56] ================================================================================
[13:12:56] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:56] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:56] ❌ Trial 580 failed with error: expected scalar type Long but found Float
[13:12:56] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:56] ✓ Results saved to hyperparameter_results.json
[13:12:56] 
================================================================================
[13:12:56] PROGRESS UPDATE - Completed 580 trials
[13:12:56] Best so far: 0.0000
[13:12:56] ================================================================================

[13:12:56] ================================================================================
[13:12:56] TRIAL 581 - Starting
[13:12:56] ================================================================================
[13:12:56] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:56] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:56] ❌ Trial 581 failed with error: expected scalar type Long but found Float
[13:12:56] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:56] ✓ Results saved to hyperparameter_results.json
[13:12:56] ================================================================================
[13:12:56] TRIAL 582 - Starting
[13:12:56] ================================================================================
[13:12:56] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:56] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:56] ❌ Trial 582 failed with error: expected scalar type Long but found Float
[13:12:56] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:56] ✓ Results saved to hyperparameter_results.json
[13:12:56] ================================================================================
[13:12:56] TRIAL 583 - Starting
[13:12:56] ================================================================================
[13:12:56] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:56] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:56] ❌ Trial 583 failed with error: expected scalar type Long but found Float
[13:12:56] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:56] ✓ Results saved to hyperparameter_results.json
[13:12:56] ================================================================================
[13:12:56] TRIAL 584 - Starting
[13:12:56] ================================================================================
[13:12:56] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:56] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:56] ❌ Trial 584 failed with error: expected scalar type Long but found Float
[13:12:56] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:56] ✓ Results saved to hyperparameter_results.json
[13:12:56] ================================================================================
[13:12:56] TRIAL 585 - Starting
[13:12:56] ================================================================================
[13:12:56] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:56] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:57] ❌ Trial 585 failed with error: expected scalar type Long but found Float
[13:12:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:57] ✓ Results saved to hyperparameter_results.json
[13:12:57] 
================================================================================
[13:12:57] PROGRESS UPDATE - Completed 585 trials
[13:12:57] Best so far: 0.0000
[13:12:57] ================================================================================

[13:12:57] ================================================================================
[13:12:57] TRIAL 586 - Starting
[13:12:57] ================================================================================
[13:12:57] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:57] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:57] ❌ Trial 586 failed with error: expected scalar type Long but found Float
[13:12:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:57] ✓ Results saved to hyperparameter_results.json
[13:12:57] ================================================================================
[13:12:57] TRIAL 587 - Starting
[13:12:57] ================================================================================
[13:12:57] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:57] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:57] ❌ Trial 587 failed with error: expected scalar type Long but found Float
[13:12:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:57] ✓ Results saved to hyperparameter_results.json
[13:12:57] ================================================================================
[13:12:57] TRIAL 588 - Starting
[13:12:57] ================================================================================
[13:12:57] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:57] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:57] ❌ Trial 588 failed with error: expected scalar type Long but found Float
[13:12:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:57] ✓ Results saved to hyperparameter_results.json
[13:12:57] ================================================================================
[13:12:57] TRIAL 589 - Starting
[13:12:57] ================================================================================
[13:12:57] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:57] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:57] ❌ Trial 589 failed with error: expected scalar type Long but found Float
[13:12:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:57] ✓ Results saved to hyperparameter_results.json
[13:12:57] ================================================================================
[13:12:57] TRIAL 590 - Starting
[13:12:57] ================================================================================
[13:12:57] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:57] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:57] ❌ Trial 590 failed with error: expected scalar type Long but found Float
[13:12:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:57] ✓ Results saved to hyperparameter_results.json
[13:12:57] 
================================================================================
[13:12:57] PROGRESS UPDATE - Completed 590 trials
[13:12:57] Best so far: 0.0000
[13:12:57] ================================================================================

[13:12:57] ================================================================================
[13:12:57] TRIAL 591 - Starting
[13:12:57] ================================================================================
[13:12:57] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:57] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:57] ❌ Trial 591 failed with error: expected scalar type Long but found Float
[13:12:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:57] ✓ Results saved to hyperparameter_results.json
[13:12:57] ================================================================================
[13:12:57] TRIAL 592 - Starting
[13:12:57] ================================================================================
[13:12:57] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:57] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:57] ❌ Trial 592 failed with error: expected scalar type Long but found Float
[13:12:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:58] ✓ Results saved to hyperparameter_results.json
[13:12:58] ================================================================================
[13:12:58] TRIAL 593 - Starting
[13:12:58] ================================================================================
[13:12:58] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:58] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:58] ❌ Trial 593 failed with error: expected scalar type Long but found Float
[13:12:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:58] ✓ Results saved to hyperparameter_results.json
[13:12:58] ================================================================================
[13:12:58] TRIAL 594 - Starting
[13:12:58] ================================================================================
[13:12:58] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:58] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:58] ❌ Trial 594 failed with error: expected scalar type Long but found Float
[13:12:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:58] ✓ Results saved to hyperparameter_results.json
[13:12:58] ================================================================================
[13:12:58] TRIAL 595 - Starting
[13:12:58] ================================================================================
[13:12:58] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:58] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:58] ❌ Trial 595 failed with error: expected scalar type Long but found Float
[13:12:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:58] ✓ Results saved to hyperparameter_results.json
[13:12:58] 
================================================================================
[13:12:58] PROGRESS UPDATE - Completed 595 trials
[13:12:58] Best so far: 0.0000
[13:12:58] ================================================================================

[13:12:58] ================================================================================
[13:12:58] TRIAL 596 - Starting
[13:12:58] ================================================================================
[13:12:58] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:58] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:58] ❌ Trial 596 failed with error: expected scalar type Long but found Float
[13:12:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:58] ✓ Results saved to hyperparameter_results.json
[13:12:58] ================================================================================
[13:12:58] TRIAL 597 - Starting
[13:12:58] ================================================================================
[13:12:58] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:58] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:58] ❌ Trial 597 failed with error: expected scalar type Long but found Float
[13:12:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:58] ✓ Results saved to hyperparameter_results.json
[13:12:58] ================================================================================
[13:12:58] TRIAL 598 - Starting
[13:12:58] ================================================================================
[13:12:58] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:12:58] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:12:58] ❌ Trial 598 failed with error: expected scalar type Long but found Float
[13:12:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:58] ✓ Results saved to hyperparameter_results.json
[13:12:58] ================================================================================
[13:12:58] TRIAL 599 - Starting
[13:12:58] ================================================================================
[13:12:58] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:12:58] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:12:58] ❌ Trial 599 failed with error: expected scalar type Long but found Float
[13:12:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:58] ✓ Results saved to hyperparameter_results.json
[13:12:58] ================================================================================
[13:12:58] TRIAL 600 - Starting
[13:12:58] ================================================================================
[13:12:58] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:12:59] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:12:59] ❌ Trial 600 failed with error: expected scalar type Long but found Float
[13:12:59] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:59] ✓ Results saved to hyperparameter_results.json
[13:12:59] 
================================================================================
[13:12:59] PROGRESS UPDATE - Completed 600 trials
[13:12:59] Best so far: 0.0000
[13:12:59] ================================================================================

[13:12:59] ================================================================================
[13:12:59] TRIAL 601 - Starting
[13:12:59] ================================================================================
[13:12:59] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:12:59] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:12:59] ❌ Trial 601 failed with error: expected scalar type Long but found Float
[13:12:59] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:59] ✓ Results saved to hyperparameter_results.json
[13:12:59] ================================================================================
[13:12:59] TRIAL 602 - Starting
[13:12:59] ================================================================================
[13:12:59] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:12:59] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:12:59] ❌ Trial 602 failed with error: expected scalar type Long but found Float
[13:12:59] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:59] ✓ Results saved to hyperparameter_results.json
[13:12:59] ================================================================================
[13:12:59] TRIAL 603 - Starting
[13:12:59] ================================================================================
[13:12:59] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:12:59] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:12:59] ❌ Trial 603 failed with error: expected scalar type Long but found Float
[13:12:59] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:59] ✓ Results saved to hyperparameter_results.json
[13:12:59] ================================================================================
[13:12:59] TRIAL 604 - Starting
[13:12:59] ================================================================================
[13:12:59] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:12:59] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:12:59] ❌ Trial 604 failed with error: expected scalar type Long but found Float
[13:12:59] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:12:59] ✓ Results saved to hyperparameter_results.json
[13:12:59] ================================================================================
[13:12:59] TRIAL 605 - Starting
[13:12:59] ================================================================================
[13:12:59] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:12:59] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:12:59] ❌ Trial 605 failed with error: expected scalar type Long but found Float
[13:12:59] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:00] ✓ Results saved to hyperparameter_results.json
[13:13:00] 
================================================================================
[13:13:00] PROGRESS UPDATE - Completed 605 trials
[13:13:00] Best so far: 0.0000
[13:13:00] ================================================================================

[13:13:00] ================================================================================
[13:13:00] TRIAL 606 - Starting
[13:13:00] ================================================================================
[13:13:00] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:00] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:00] ❌ Trial 606 failed with error: expected scalar type Long but found Float
[13:13:00] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:00] ✓ Results saved to hyperparameter_results.json
[13:13:00] ================================================================================
[13:13:00] TRIAL 607 - Starting
[13:13:00] ================================================================================
[13:13:00] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:00] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:00] ❌ Trial 607 failed with error: expected scalar type Long but found Float
[13:13:00] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:00] ✓ Results saved to hyperparameter_results.json
[13:13:00] ================================================================================
[13:13:00] TRIAL 608 - Starting
[13:13:00] ================================================================================
[13:13:00] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:00] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:00] ❌ Trial 608 failed with error: expected scalar type Long but found Float
[13:13:00] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:00] ✓ Results saved to hyperparameter_results.json
[13:13:00] ================================================================================
[13:13:00] TRIAL 609 - Starting
[13:13:00] ================================================================================
[13:13:00] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:00] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:00] ❌ Trial 609 failed with error: expected scalar type Long but found Float
[13:13:00] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:00] ✓ Results saved to hyperparameter_results.json
[13:13:00] ================================================================================
[13:13:00] TRIAL 610 - Starting
[13:13:00] ================================================================================
[13:13:00] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:00] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:00] ❌ Trial 610 failed with error: expected scalar type Long but found Float
[13:13:00] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:00] ✓ Results saved to hyperparameter_results.json
[13:13:00] 
================================================================================
[13:13:00] PROGRESS UPDATE - Completed 610 trials
[13:13:00] Best so far: 0.0000
[13:13:00] ================================================================================

[13:13:00] ================================================================================
[13:13:00] TRIAL 611 - Starting
[13:13:00] ================================================================================
[13:13:00] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:00] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:00] ❌ Trial 611 failed with error: expected scalar type Long but found Float
[13:13:00] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:00] ✓ Results saved to hyperparameter_results.json
[13:13:00] ================================================================================
[13:13:00] TRIAL 612 - Starting
[13:13:00] ================================================================================
[13:13:00] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:00] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:01] ❌ Trial 612 failed with error: expected scalar type Long but found Float
[13:13:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:01] ✓ Results saved to hyperparameter_results.json
[13:13:01] ================================================================================
[13:13:01] TRIAL 613 - Starting
[13:13:01] ================================================================================
[13:13:01] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:01] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:01] ❌ Trial 613 failed with error: expected scalar type Long but found Float
[13:13:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:01] ✓ Results saved to hyperparameter_results.json
[13:13:01] ================================================================================
[13:13:01] TRIAL 614 - Starting
[13:13:01] ================================================================================
[13:13:01] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:01] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:01] ❌ Trial 614 failed with error: expected scalar type Long but found Float
[13:13:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:01] ✓ Results saved to hyperparameter_results.json
[13:13:01] ================================================================================
[13:13:01] TRIAL 615 - Starting
[13:13:01] ================================================================================
[13:13:01] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:01] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:01] ❌ Trial 615 failed with error: expected scalar type Long but found Float
[13:13:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:01] ✓ Results saved to hyperparameter_results.json
[13:13:01] 
================================================================================
[13:13:01] PROGRESS UPDATE - Completed 615 trials
[13:13:01] Best so far: 0.0000
[13:13:01] ================================================================================

[13:13:01] ================================================================================
[13:13:01] TRIAL 616 - Starting
[13:13:01] ================================================================================
[13:13:01] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:01] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:01] ❌ Trial 616 failed with error: expected scalar type Long but found Float
[13:13:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:01] ✓ Results saved to hyperparameter_results.json
[13:13:01] ================================================================================
[13:13:01] TRIAL 617 - Starting
[13:13:01] ================================================================================
[13:13:01] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:01] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:01] ❌ Trial 617 failed with error: expected scalar type Long but found Float
[13:13:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:01] ✓ Results saved to hyperparameter_results.json
[13:13:01] ================================================================================
[13:13:01] TRIAL 618 - Starting
[13:13:01] ================================================================================
[13:13:01] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:01] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:01] ❌ Trial 618 failed with error: expected scalar type Long but found Float
[13:13:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:02] ✓ Results saved to hyperparameter_results.json
[13:13:02] ================================================================================
[13:13:02] TRIAL 619 - Starting
[13:13:02] ================================================================================
[13:13:02] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:02] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:02] ❌ Trial 619 failed with error: expected scalar type Long but found Float
[13:13:02] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:02] ✓ Results saved to hyperparameter_results.json
[13:13:02] ================================================================================
[13:13:02] TRIAL 620 - Starting
[13:13:02] ================================================================================
[13:13:02] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:02] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:02] ❌ Trial 620 failed with error: expected scalar type Long but found Float
[13:13:02] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:02] ✓ Results saved to hyperparameter_results.json
[13:13:02] 
================================================================================
[13:13:02] PROGRESS UPDATE - Completed 620 trials
[13:13:02] Best so far: 0.0000
[13:13:02] ================================================================================

[13:13:02] ================================================================================
[13:13:02] TRIAL 621 - Starting
[13:13:02] ================================================================================
[13:13:02] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:02] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:02] ❌ Trial 621 failed with error: expected scalar type Long but found Float
[13:13:02] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:02] ✓ Results saved to hyperparameter_results.json
[13:13:02] ================================================================================
[13:13:02] TRIAL 622 - Starting
[13:13:02] ================================================================================
[13:13:02] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:02] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:02] ❌ Trial 622 failed with error: expected scalar type Long but found Float
[13:13:02] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:02] ✓ Results saved to hyperparameter_results.json
[13:13:02] ================================================================================
[13:13:02] TRIAL 623 - Starting
[13:13:02] ================================================================================
[13:13:02] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:02] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:02] ❌ Trial 623 failed with error: expected scalar type Long but found Float
[13:13:02] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:02] ✓ Results saved to hyperparameter_results.json
[13:13:02] ================================================================================
[13:13:02] TRIAL 624 - Starting
[13:13:02] ================================================================================
[13:13:02] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:02] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:02] ❌ Trial 624 failed with error: expected scalar type Long but found Float
[13:13:02] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:03] ✓ Results saved to hyperparameter_results.json
[13:13:03] ================================================================================
[13:13:03] TRIAL 625 - Starting
[13:13:03] ================================================================================
[13:13:03] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:03] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:03] ❌ Trial 625 failed with error: expected scalar type Long but found Float
[13:13:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:03] ✓ Results saved to hyperparameter_results.json
[13:13:03] 
================================================================================
[13:13:03] PROGRESS UPDATE - Completed 625 trials
[13:13:03] Best so far: 0.0000
[13:13:03] ================================================================================

[13:13:03] ================================================================================
[13:13:03] TRIAL 626 - Starting
[13:13:03] ================================================================================
[13:13:03] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:03] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:03] ❌ Trial 626 failed with error: expected scalar type Long but found Float
[13:13:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:03] ✓ Results saved to hyperparameter_results.json
[13:13:03] ================================================================================
[13:13:03] TRIAL 627 - Starting
[13:13:03] ================================================================================
[13:13:03] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:03] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:03] ❌ Trial 627 failed with error: expected scalar type Long but found Float
[13:13:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:03] ✓ Results saved to hyperparameter_results.json
[13:13:03] ================================================================================
[13:13:03] TRIAL 628 - Starting
[13:13:03] ================================================================================
[13:13:03] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:03] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:03] ❌ Trial 628 failed with error: expected scalar type Long but found Float
[13:13:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:03] ✓ Results saved to hyperparameter_results.json
[13:13:03] ================================================================================
[13:13:03] TRIAL 629 - Starting
[13:13:03] ================================================================================
[13:13:03] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:03] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:03] ❌ Trial 629 failed with error: expected scalar type Long but found Float
[13:13:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:03] ✓ Results saved to hyperparameter_results.json
[13:13:03] ================================================================================
[13:13:03] TRIAL 630 - Starting
[13:13:03] ================================================================================
[13:13:03] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:03] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:03] ❌ Trial 630 failed with error: expected scalar type Long but found Float
[13:13:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:03] ✓ Results saved to hyperparameter_results.json
[13:13:03] 
================================================================================
[13:13:03] PROGRESS UPDATE - Completed 630 trials
[13:13:03] Best so far: 0.0000
[13:13:03] ================================================================================

[13:13:03] ================================================================================
[13:13:03] TRIAL 631 - Starting
[13:13:03] ================================================================================
[13:13:03] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:03] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:03] ❌ Trial 631 failed with error: expected scalar type Long but found Float
[13:13:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:04] ✓ Results saved to hyperparameter_results.json
[13:13:04] ================================================================================
[13:13:04] TRIAL 632 - Starting
[13:13:04] ================================================================================
[13:13:04] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:04] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:04] ❌ Trial 632 failed with error: expected scalar type Long but found Float
[13:13:04] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:04] ✓ Results saved to hyperparameter_results.json
[13:13:04] ================================================================================
[13:13:04] TRIAL 633 - Starting
[13:13:04] ================================================================================
[13:13:04] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:04] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:04] ❌ Trial 633 failed with error: expected scalar type Long but found Float
[13:13:04] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:04] ✓ Results saved to hyperparameter_results.json
[13:13:04] ================================================================================
[13:13:04] TRIAL 634 - Starting
[13:13:04] ================================================================================
[13:13:04] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:04] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:04] ❌ Trial 634 failed with error: expected scalar type Long but found Float
[13:13:04] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:04] ✓ Results saved to hyperparameter_results.json
[13:13:04] ================================================================================
[13:13:04] TRIAL 635 - Starting
[13:13:04] ================================================================================
[13:13:04] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:04] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:04] ❌ Trial 635 failed with error: expected scalar type Long but found Float
[13:13:04] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:04] ✓ Results saved to hyperparameter_results.json
[13:13:04] 
================================================================================
[13:13:04] PROGRESS UPDATE - Completed 635 trials
[13:13:04] Best so far: 0.0000
[13:13:04] ================================================================================

[13:13:04] ================================================================================
[13:13:04] TRIAL 636 - Starting
[13:13:04] ================================================================================
[13:13:04] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:04] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:04] ❌ Trial 636 failed with error: expected scalar type Long but found Float
[13:13:04] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:04] ✓ Results saved to hyperparameter_results.json
[13:13:04] ================================================================================
[13:13:04] TRIAL 637 - Starting
[13:13:04] ================================================================================
[13:13:04] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:04] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:04] ❌ Trial 637 failed with error: expected scalar type Long but found Float
[13:13:04] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:05] ✓ Results saved to hyperparameter_results.json
[13:13:05] ================================================================================
[13:13:05] TRIAL 638 - Starting
[13:13:05] ================================================================================
[13:13:05] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:05] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:05] ❌ Trial 638 failed with error: expected scalar type Long but found Float
[13:13:05] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:05] ✓ Results saved to hyperparameter_results.json
[13:13:05] ================================================================================
[13:13:05] TRIAL 639 - Starting
[13:13:05] ================================================================================
[13:13:05] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:05] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:05] ❌ Trial 639 failed with error: expected scalar type Long but found Float
[13:13:05] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:05] ✓ Results saved to hyperparameter_results.json
[13:13:05] ================================================================================
[13:13:05] TRIAL 640 - Starting
[13:13:05] ================================================================================
[13:13:05] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:05] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:05] ❌ Trial 640 failed with error: expected scalar type Long but found Float
[13:13:05] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:05] ✓ Results saved to hyperparameter_results.json
[13:13:05] 
================================================================================
[13:13:05] PROGRESS UPDATE - Completed 640 trials
[13:13:05] Best so far: 0.0000
[13:13:05] ================================================================================

[13:13:05] ================================================================================
[13:13:05] TRIAL 641 - Starting
[13:13:05] ================================================================================
[13:13:05] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:05] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:05] ❌ Trial 641 failed with error: expected scalar type Long but found Float
[13:13:05] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:05] ✓ Results saved to hyperparameter_results.json
[13:13:05] ================================================================================
[13:13:05] TRIAL 642 - Starting
[13:13:05] ================================================================================
[13:13:05] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:05] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:05] ❌ Trial 642 failed with error: expected scalar type Long but found Float
[13:13:05] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:05] ✓ Results saved to hyperparameter_results.json
[13:13:05] ================================================================================
[13:13:05] TRIAL 643 - Starting
[13:13:05] ================================================================================
[13:13:05] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:05] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:05] ❌ Trial 643 failed with error: expected scalar type Long but found Float
[13:13:05] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:05] ✓ Results saved to hyperparameter_results.json
[13:13:05] ================================================================================
[13:13:05] TRIAL 644 - Starting
[13:13:05] ================================================================================
[13:13:05] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:05] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:06] ❌ Trial 644 failed with error: expected scalar type Long but found Float
[13:13:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:06] ✓ Results saved to hyperparameter_results.json
[13:13:06] ================================================================================
[13:13:06] TRIAL 645 - Starting
[13:13:06] ================================================================================
[13:13:06] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:06] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:06] ❌ Trial 645 failed with error: expected scalar type Long but found Float
[13:13:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:06] ✓ Results saved to hyperparameter_results.json
[13:13:06] 
================================================================================
[13:13:06] PROGRESS UPDATE - Completed 645 trials
[13:13:06] Best so far: 0.0000
[13:13:06] ================================================================================

[13:13:06] ================================================================================
[13:13:06] TRIAL 646 - Starting
[13:13:06] ================================================================================
[13:13:06] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:06] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:06] ❌ Trial 646 failed with error: expected scalar type Long but found Float
[13:13:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:06] ✓ Results saved to hyperparameter_results.json
[13:13:06] ================================================================================
[13:13:06] TRIAL 647 - Starting
[13:13:06] ================================================================================
[13:13:06] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:06] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:06] ❌ Trial 647 failed with error: expected scalar type Long but found Float
[13:13:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:06] ✓ Results saved to hyperparameter_results.json
[13:13:06] ================================================================================
[13:13:06] TRIAL 648 - Starting
[13:13:06] ================================================================================
[13:13:06] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:06] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:06] ❌ Trial 648 failed with error: expected scalar type Long but found Float
[13:13:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:06] ✓ Results saved to hyperparameter_results.json
[13:13:06] ================================================================================
[13:13:06] TRIAL 649 - Starting
[13:13:06] ================================================================================
[13:13:06] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:06] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:06] ❌ Trial 649 failed with error: expected scalar type Long but found Float
[13:13:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:06] ✓ Results saved to hyperparameter_results.json
[13:13:06] ================================================================================
[13:13:06] TRIAL 650 - Starting
[13:13:06] ================================================================================
[13:13:06] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:06] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:06] ❌ Trial 650 failed with error: expected scalar type Long but found Float
[13:13:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:06] ✓ Results saved to hyperparameter_results.json
[13:13:06] 
================================================================================
[13:13:06] PROGRESS UPDATE - Completed 650 trials
[13:13:06] Best so far: 0.0000
[13:13:06] ================================================================================

[13:13:06] ================================================================================
[13:13:06] TRIAL 651 - Starting
[13:13:06] ================================================================================
[13:13:06] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:06] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:07] ❌ Trial 651 failed with error: expected scalar type Long but found Float
[13:13:07] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:07] ✓ Results saved to hyperparameter_results.json
[13:13:07] ================================================================================
[13:13:07] TRIAL 652 - Starting
[13:13:07] ================================================================================
[13:13:07] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:07] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:07] ❌ Trial 652 failed with error: expected scalar type Long but found Float
[13:13:07] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:07] ✓ Results saved to hyperparameter_results.json
[13:13:07] ================================================================================
[13:13:07] TRIAL 653 - Starting
[13:13:07] ================================================================================
[13:13:07] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:07] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:07] ❌ Trial 653 failed with error: expected scalar type Long but found Float
[13:13:07] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:07] ✓ Results saved to hyperparameter_results.json
[13:13:07] ================================================================================
[13:13:07] TRIAL 654 - Starting
[13:13:07] ================================================================================
[13:13:07] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:07] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:07] ❌ Trial 654 failed with error: expected scalar type Long but found Float
[13:13:07] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:07] ✓ Results saved to hyperparameter_results.json
[13:13:07] ================================================================================
[13:13:07] TRIAL 655 - Starting
[13:13:07] ================================================================================
[13:13:07] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:07] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:07] ❌ Trial 655 failed with error: expected scalar type Long but found Float
[13:13:07] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:07] ✓ Results saved to hyperparameter_results.json
[13:13:07] 
================================================================================
[13:13:07] PROGRESS UPDATE - Completed 655 trials
[13:13:07] Best so far: 0.0000
[13:13:07] ================================================================================

[13:13:07] ================================================================================
[13:13:07] TRIAL 656 - Starting
[13:13:07] ================================================================================
[13:13:07] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:07] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:07] ❌ Trial 656 failed with error: expected scalar type Long but found Float
[13:13:07] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:07] ✓ Results saved to hyperparameter_results.json
[13:13:07] ================================================================================
[13:13:07] TRIAL 657 - Starting
[13:13:07] ================================================================================
[13:13:07] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:07] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:08] ❌ Trial 657 failed with error: expected scalar type Long but found Float
[13:13:08] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:08] ✓ Results saved to hyperparameter_results.json
[13:13:08] ================================================================================
[13:13:08] TRIAL 658 - Starting
[13:13:08] ================================================================================
[13:13:08] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:08] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:08] ❌ Trial 658 failed with error: expected scalar type Long but found Float
[13:13:08] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:08] ✓ Results saved to hyperparameter_results.json
[13:13:08] ================================================================================
[13:13:08] TRIAL 659 - Starting
[13:13:08] ================================================================================
[13:13:08] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:08] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:08] ❌ Trial 659 failed with error: expected scalar type Long but found Float
[13:13:08] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:08] ✓ Results saved to hyperparameter_results.json
[13:13:08] ================================================================================
[13:13:08] TRIAL 660 - Starting
[13:13:08] ================================================================================
[13:13:08] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:08] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:08] ❌ Trial 660 failed with error: expected scalar type Long but found Float
[13:13:08] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:08] ✓ Results saved to hyperparameter_results.json
[13:13:08] 
================================================================================
[13:13:08] PROGRESS UPDATE - Completed 660 trials
[13:13:08] Best so far: 0.0000
[13:13:08] ================================================================================

[13:13:08] ================================================================================
[13:13:08] TRIAL 661 - Starting
[13:13:08] ================================================================================
[13:13:08] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:08] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:08] ❌ Trial 661 failed with error: expected scalar type Long but found Float
[13:13:08] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:08] ✓ Results saved to hyperparameter_results.json
[13:13:08] ================================================================================
[13:13:08] TRIAL 662 - Starting
[13:13:08] ================================================================================
[13:13:08] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:08] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:08] ❌ Trial 662 failed with error: expected scalar type Long but found Float
[13:13:08] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:08] ✓ Results saved to hyperparameter_results.json
[13:13:09] ================================================================================
[13:13:09] TRIAL 663 - Starting
[13:13:09] ================================================================================
[13:13:09] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:09] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:09] ❌ Trial 663 failed with error: expected scalar type Long but found Float
[13:13:09] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:09] ✓ Results saved to hyperparameter_results.json
[13:13:09] ================================================================================
[13:13:09] TRIAL 664 - Starting
[13:13:09] ================================================================================
[13:13:09] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:09] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:09] ❌ Trial 664 failed with error: expected scalar type Long but found Float
[13:13:09] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:09] ✓ Results saved to hyperparameter_results.json
[13:13:09] ================================================================================
[13:13:09] TRIAL 665 - Starting
[13:13:09] ================================================================================
[13:13:09] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:09] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:09] ❌ Trial 665 failed with error: expected scalar type Long but found Float
[13:13:09] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:09] ✓ Results saved to hyperparameter_results.json
[13:13:09] 
================================================================================
[13:13:09] PROGRESS UPDATE - Completed 665 trials
[13:13:09] Best so far: 0.0000
[13:13:09] ================================================================================

[13:13:09] ================================================================================
[13:13:09] TRIAL 666 - Starting
[13:13:09] ================================================================================
[13:13:09] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:09] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:09] ❌ Trial 666 failed with error: expected scalar type Long but found Float
[13:13:09] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:09] ✓ Results saved to hyperparameter_results.json
[13:13:09] ================================================================================
[13:13:09] TRIAL 667 - Starting
[13:13:09] ================================================================================
[13:13:09] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:09] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:09] ❌ Trial 667 failed with error: expected scalar type Long but found Float
[13:13:09] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:09] ✓ Results saved to hyperparameter_results.json
[13:13:09] ================================================================================
[13:13:09] TRIAL 668 - Starting
[13:13:09] ================================================================================
[13:13:09] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:09] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:09] ❌ Trial 668 failed with error: expected scalar type Long but found Float
[13:13:09] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:09] ✓ Results saved to hyperparameter_results.json
[13:13:09] ================================================================================
[13:13:09] TRIAL 669 - Starting
[13:13:09] ================================================================================
[13:13:09] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:09] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:09] ❌ Trial 669 failed with error: expected scalar type Long but found Float
[13:13:09] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:10] ✓ Results saved to hyperparameter_results.json
[13:13:10] ================================================================================
[13:13:10] TRIAL 670 - Starting
[13:13:10] ================================================================================
[13:13:10] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:10] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:10] ❌ Trial 670 failed with error: expected scalar type Long but found Float
[13:13:10] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:10] ✓ Results saved to hyperparameter_results.json
[13:13:10] 
================================================================================
[13:13:10] PROGRESS UPDATE - Completed 670 trials
[13:13:10] Best so far: 0.0000
[13:13:10] ================================================================================

[13:13:10] ================================================================================
[13:13:10] TRIAL 671 - Starting
[13:13:10] ================================================================================
[13:13:10] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:10] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:10] ❌ Trial 671 failed with error: expected scalar type Long but found Float
[13:13:10] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:10] ✓ Results saved to hyperparameter_results.json
[13:13:10] ================================================================================
[13:13:10] TRIAL 672 - Starting
[13:13:10] ================================================================================
[13:13:10] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:10] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:10] ❌ Trial 672 failed with error: expected scalar type Long but found Float
[13:13:10] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:10] ✓ Results saved to hyperparameter_results.json
[13:13:10] ================================================================================
[13:13:10] TRIAL 673 - Starting
[13:13:10] ================================================================================
[13:13:10] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:10] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:10] ❌ Trial 673 failed with error: expected scalar type Long but found Float
[13:13:10] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:10] ✓ Results saved to hyperparameter_results.json
[13:13:10] ================================================================================
[13:13:10] TRIAL 674 - Starting
[13:13:10] ================================================================================
[13:13:10] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:10] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:10] ❌ Trial 674 failed with error: expected scalar type Long but found Float
[13:13:10] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:10] ✓ Results saved to hyperparameter_results.json
[13:13:10] ================================================================================
[13:13:10] TRIAL 675 - Starting
[13:13:10] ================================================================================
[13:13:10] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:10] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:10] ❌ Trial 675 failed with error: expected scalar type Long but found Float
[13:13:10] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:10] ✓ Results saved to hyperparameter_results.json
[13:13:10] 
================================================================================
[13:13:10] PROGRESS UPDATE - Completed 675 trials
[13:13:10] Best so far: 0.0000
[13:13:10] ================================================================================

[13:13:10] ================================================================================
[13:13:10] TRIAL 676 - Starting
[13:13:10] ================================================================================
[13:13:10] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:10] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:10] ❌ Trial 676 failed with error: expected scalar type Long but found Float
[13:13:10] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:11] ✓ Results saved to hyperparameter_results.json
[13:13:11] ================================================================================
[13:13:11] TRIAL 677 - Starting
[13:13:11] ================================================================================
[13:13:11] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:11] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:11] ❌ Trial 677 failed with error: expected scalar type Long but found Float
[13:13:11] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:11] ✓ Results saved to hyperparameter_results.json
[13:13:11] ================================================================================
[13:13:11] TRIAL 678 - Starting
[13:13:11] ================================================================================
[13:13:11] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:11] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:11] ❌ Trial 678 failed with error: expected scalar type Long but found Float
[13:13:11] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:11] ✓ Results saved to hyperparameter_results.json
[13:13:11] ================================================================================
[13:13:11] TRIAL 679 - Starting
[13:13:11] ================================================================================
[13:13:11] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:11] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:11] ❌ Trial 679 failed with error: expected scalar type Long but found Float
[13:13:11] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:11] ✓ Results saved to hyperparameter_results.json
[13:13:11] ================================================================================
[13:13:11] TRIAL 680 - Starting
[13:13:11] ================================================================================
[13:13:11] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:11] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:11] ❌ Trial 680 failed with error: expected scalar type Long but found Float
[13:13:11] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:11] ✓ Results saved to hyperparameter_results.json
[13:13:11] 
================================================================================
[13:13:11] PROGRESS UPDATE - Completed 680 trials
[13:13:11] Best so far: 0.0000
[13:13:11] ================================================================================

[13:13:11] ================================================================================
[13:13:11] TRIAL 681 - Starting
[13:13:11] ================================================================================
[13:13:11] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:11] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:11] ❌ Trial 681 failed with error: expected scalar type Long but found Float
[13:13:11] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:11] ✓ Results saved to hyperparameter_results.json
[13:13:11] ================================================================================
[13:13:11] TRIAL 682 - Starting
[13:13:11] ================================================================================
[13:13:11] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:11] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:11] ❌ Trial 682 failed with error: expected scalar type Long but found Float
[13:13:11] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:11] ✓ Results saved to hyperparameter_results.json
[13:13:11] ================================================================================
[13:13:11] TRIAL 683 - Starting
[13:13:11] ================================================================================
[13:13:11] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:11] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:12] ❌ Trial 683 failed with error: expected scalar type Long but found Float
[13:13:12] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:12] ✓ Results saved to hyperparameter_results.json
[13:13:12] ================================================================================
[13:13:12] TRIAL 684 - Starting
[13:13:12] ================================================================================
[13:13:12] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:12] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:12] ❌ Trial 684 failed with error: expected scalar type Long but found Float
[13:13:12] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:12] ✓ Results saved to hyperparameter_results.json
[13:13:12] ================================================================================
[13:13:12] TRIAL 685 - Starting
[13:13:12] ================================================================================
[13:13:12] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:12] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:12] ❌ Trial 685 failed with error: expected scalar type Long but found Float
[13:13:12] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:12] ✓ Results saved to hyperparameter_results.json
[13:13:12] 
================================================================================
[13:13:12] PROGRESS UPDATE - Completed 685 trials
[13:13:12] Best so far: 0.0000
[13:13:12] ================================================================================

[13:13:12] ================================================================================
[13:13:12] TRIAL 686 - Starting
[13:13:12] ================================================================================
[13:13:12] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:12] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:12] ❌ Trial 686 failed with error: expected scalar type Long but found Float
[13:13:12] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:12] ✓ Results saved to hyperparameter_results.json
[13:13:12] ================================================================================
[13:13:12] TRIAL 687 - Starting
[13:13:12] ================================================================================
[13:13:12] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:12] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:12] ❌ Trial 687 failed with error: expected scalar type Long but found Float
[13:13:12] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:12] ✓ Results saved to hyperparameter_results.json
[13:13:12] ================================================================================
[13:13:12] TRIAL 688 - Starting
[13:13:12] ================================================================================
[13:13:12] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:12] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:12] ❌ Trial 688 failed with error: expected scalar type Long but found Float
[13:13:12] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:12] ✓ Results saved to hyperparameter_results.json
[13:13:12] ================================================================================
[13:13:12] TRIAL 689 - Starting
[13:13:12] ================================================================================
[13:13:12] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:12] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:12] ❌ Trial 689 failed with error: expected scalar type Long but found Float
[13:13:12] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:13] ✓ Results saved to hyperparameter_results.json
[13:13:13] ================================================================================
[13:13:13] TRIAL 690 - Starting
[13:13:13] ================================================================================
[13:13:13] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:13] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:13] ❌ Trial 690 failed with error: expected scalar type Long but found Float
[13:13:13] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:13] ✓ Results saved to hyperparameter_results.json
[13:13:13] 
================================================================================
[13:13:13] PROGRESS UPDATE - Completed 690 trials
[13:13:13] Best so far: 0.0000
[13:13:13] ================================================================================

[13:13:13] ================================================================================
[13:13:13] TRIAL 691 - Starting
[13:13:13] ================================================================================
[13:13:13] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:13] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:13] ❌ Trial 691 failed with error: expected scalar type Long but found Float
[13:13:13] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:13] ✓ Results saved to hyperparameter_results.json
[13:13:13] ================================================================================
[13:13:13] TRIAL 692 - Starting
[13:13:13] ================================================================================
[13:13:13] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:13] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:13] ❌ Trial 692 failed with error: expected scalar type Long but found Float
[13:13:13] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:13] ✓ Results saved to hyperparameter_results.json
[13:13:13] ================================================================================
[13:13:13] TRIAL 693 - Starting
[13:13:13] ================================================================================
[13:13:13] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:13] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:13] ❌ Trial 693 failed with error: expected scalar type Long but found Float
[13:13:13] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:13] ✓ Results saved to hyperparameter_results.json
[13:13:13] ================================================================================
[13:13:13] TRIAL 694 - Starting
[13:13:13] ================================================================================
[13:13:13] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:13] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:13] ❌ Trial 694 failed with error: expected scalar type Long but found Float
[13:13:13] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:13] ✓ Results saved to hyperparameter_results.json
[13:13:13] ================================================================================
[13:13:13] TRIAL 695 - Starting
[13:13:13] ================================================================================
[13:13:13] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:13] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:13] ❌ Trial 695 failed with error: expected scalar type Long but found Float
[13:13:13] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:14] ✓ Results saved to hyperparameter_results.json
[13:13:14] 
================================================================================
[13:13:14] PROGRESS UPDATE - Completed 695 trials
[13:13:14] Best so far: 0.0000
[13:13:14] ================================================================================

[13:13:14] ================================================================================
[13:13:14] TRIAL 696 - Starting
[13:13:14] ================================================================================
[13:13:14] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:14] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:14] ❌ Trial 696 failed with error: expected scalar type Long but found Float
[13:13:14] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:14] ✓ Results saved to hyperparameter_results.json
[13:13:14] ================================================================================
[13:13:14] TRIAL 697 - Starting
[13:13:14] ================================================================================
[13:13:14] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:14] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:14] ❌ Trial 697 failed with error: expected scalar type Long but found Float
[13:13:14] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:14] ✓ Results saved to hyperparameter_results.json
[13:13:14] ================================================================================
[13:13:14] TRIAL 698 - Starting
[13:13:14] ================================================================================
[13:13:14] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:14] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:14] ❌ Trial 698 failed with error: expected scalar type Long but found Float
[13:13:14] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:14] ✓ Results saved to hyperparameter_results.json
[13:13:14] ================================================================================
[13:13:14] TRIAL 699 - Starting
[13:13:14] ================================================================================
[13:13:14] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:14] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:14] ❌ Trial 699 failed with error: expected scalar type Long but found Float
[13:13:14] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:14] ✓ Results saved to hyperparameter_results.json
[13:13:14] ================================================================================
[13:13:14] TRIAL 700 - Starting
[13:13:14] ================================================================================
[13:13:14] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:14] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:14] ❌ Trial 700 failed with error: expected scalar type Long but found Float
[13:13:14] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:14] ✓ Results saved to hyperparameter_results.json
[13:13:14] 
================================================================================
[13:13:14] PROGRESS UPDATE - Completed 700 trials
[13:13:14] Best so far: 0.0000
[13:13:14] ================================================================================

[13:13:14] ================================================================================
[13:13:14] TRIAL 701 - Starting
[13:13:14] ================================================================================
[13:13:14] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:14] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:14] ❌ Trial 701 failed with error: expected scalar type Long but found Float
[13:13:14] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:15] ✓ Results saved to hyperparameter_results.json
[13:13:15] ================================================================================
[13:13:15] TRIAL 702 - Starting
[13:13:15] ================================================================================
[13:13:15] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:15] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:15] ❌ Trial 702 failed with error: expected scalar type Long but found Float
[13:13:15] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:15] ✓ Results saved to hyperparameter_results.json
[13:13:15] ================================================================================
[13:13:15] TRIAL 703 - Starting
[13:13:15] ================================================================================
[13:13:15] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:15] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:15] ❌ Trial 703 failed with error: expected scalar type Long but found Float
[13:13:15] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:15] ✓ Results saved to hyperparameter_results.json
[13:13:15] ================================================================================
[13:13:15] TRIAL 704 - Starting
[13:13:15] ================================================================================
[13:13:15] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:15] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:15] ❌ Trial 704 failed with error: expected scalar type Long but found Float
[13:13:15] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:15] ✓ Results saved to hyperparameter_results.json
[13:13:15] ================================================================================
[13:13:15] TRIAL 705 - Starting
[13:13:15] ================================================================================
[13:13:15] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:15] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:15] ❌ Trial 705 failed with error: expected scalar type Long but found Float
[13:13:15] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:15] ✓ Results saved to hyperparameter_results.json
[13:13:15] 
================================================================================
[13:13:15] PROGRESS UPDATE - Completed 705 trials
[13:13:15] Best so far: 0.0000
[13:13:15] ================================================================================

[13:13:15] ================================================================================
[13:13:15] TRIAL 706 - Starting
[13:13:15] ================================================================================
[13:13:15] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:15] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:15] ❌ Trial 706 failed with error: expected scalar type Long but found Float
[13:13:15] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:15] ✓ Results saved to hyperparameter_results.json
[13:13:15] ================================================================================
[13:13:15] TRIAL 707 - Starting
[13:13:15] ================================================================================
[13:13:15] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:15] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:15] ❌ Trial 707 failed with error: expected scalar type Long but found Float
[13:13:15] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:15] ✓ Results saved to hyperparameter_results.json
[13:13:15] ================================================================================
[13:13:15] TRIAL 708 - Starting
[13:13:15] ================================================================================
[13:13:15] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:15] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:16] ❌ Trial 708 failed with error: expected scalar type Long but found Float
[13:13:16] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:16] ✓ Results saved to hyperparameter_results.json
[13:13:16] ================================================================================
[13:13:16] TRIAL 709 - Starting
[13:13:16] ================================================================================
[13:13:16] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:16] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:16] ❌ Trial 709 failed with error: expected scalar type Long but found Float
[13:13:16] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:16] ✓ Results saved to hyperparameter_results.json
[13:13:16] ================================================================================
[13:13:16] TRIAL 710 - Starting
[13:13:16] ================================================================================
[13:13:16] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:16] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:16] ❌ Trial 710 failed with error: expected scalar type Long but found Float
[13:13:16] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:16] ✓ Results saved to hyperparameter_results.json
[13:13:16] 
================================================================================
[13:13:16] PROGRESS UPDATE - Completed 710 trials
[13:13:16] Best so far: 0.0000
[13:13:16] ================================================================================

[13:13:16] ================================================================================
[13:13:16] TRIAL 711 - Starting
[13:13:16] ================================================================================
[13:13:16] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:16] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:16] ❌ Trial 711 failed with error: expected scalar type Long but found Float
[13:13:16] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:16] ✓ Results saved to hyperparameter_results.json
[13:13:16] ================================================================================
[13:13:16] TRIAL 712 - Starting
[13:13:16] ================================================================================
[13:13:16] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:16] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:16] ❌ Trial 712 failed with error: expected scalar type Long but found Float
[13:13:16] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:16] ✓ Results saved to hyperparameter_results.json
[13:13:16] ================================================================================
[13:13:16] TRIAL 713 - Starting
[13:13:16] ================================================================================
[13:13:16] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:16] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:16] ❌ Trial 713 failed with error: expected scalar type Long but found Float
[13:13:16] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:16] ✓ Results saved to hyperparameter_results.json
[13:13:16] ================================================================================
[13:13:16] TRIAL 714 - Starting
[13:13:16] ================================================================================
[13:13:16] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:16] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:16] ❌ Trial 714 failed with error: expected scalar type Long but found Float
[13:13:16] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:17] ✓ Results saved to hyperparameter_results.json
[13:13:17] ================================================================================
[13:13:17] TRIAL 715 - Starting
[13:13:17] ================================================================================
[13:13:17] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:17] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:17] ❌ Trial 715 failed with error: expected scalar type Long but found Float
[13:13:17] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:17] ✓ Results saved to hyperparameter_results.json
[13:13:17] 
================================================================================
[13:13:17] PROGRESS UPDATE - Completed 715 trials
[13:13:17] Best so far: 0.0000
[13:13:17] ================================================================================

[13:13:17] ================================================================================
[13:13:17] TRIAL 716 - Starting
[13:13:17] ================================================================================
[13:13:17] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:17] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:17] ❌ Trial 716 failed with error: expected scalar type Long but found Float
[13:13:17] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:17] ✓ Results saved to hyperparameter_results.json
[13:13:17] ================================================================================
[13:13:17] TRIAL 717 - Starting
[13:13:17] ================================================================================
[13:13:17] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:17] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:17] ❌ Trial 717 failed with error: expected scalar type Long but found Float
[13:13:17] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:17] ✓ Results saved to hyperparameter_results.json
[13:13:17] ================================================================================
[13:13:17] TRIAL 718 - Starting
[13:13:17] ================================================================================
[13:13:17] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:17] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:17] ❌ Trial 718 failed with error: expected scalar type Long but found Float
[13:13:17] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:17] ✓ Results saved to hyperparameter_results.json
[13:13:17] ================================================================================
[13:13:17] TRIAL 719 - Starting
[13:13:17] ================================================================================
[13:13:17] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:17] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:17] ❌ Trial 719 failed with error: expected scalar type Long but found Float
[13:13:17] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:17] ✓ Results saved to hyperparameter_results.json
[13:13:17] ================================================================================
[13:13:17] TRIAL 720 - Starting
[13:13:17] ================================================================================
[13:13:17] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:17] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:17] ❌ Trial 720 failed with error: expected scalar type Long but found Float
[13:13:17] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:18] ✓ Results saved to hyperparameter_results.json
[13:13:18] 
================================================================================
[13:13:18] PROGRESS UPDATE - Completed 720 trials
[13:13:18] Best so far: 0.0000
[13:13:18] ================================================================================

[13:13:18] ================================================================================
[13:13:18] TRIAL 721 - Starting
[13:13:18] ================================================================================
[13:13:18] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:18] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:18] ❌ Trial 721 failed with error: expected scalar type Long but found Float
[13:13:18] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:18] ✓ Results saved to hyperparameter_results.json
[13:13:18] ================================================================================
[13:13:18] TRIAL 722 - Starting
[13:13:18] ================================================================================
[13:13:18] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:18] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:18] ❌ Trial 722 failed with error: expected scalar type Long but found Float
[13:13:18] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:18] ✓ Results saved to hyperparameter_results.json
[13:13:18] ================================================================================
[13:13:18] TRIAL 723 - Starting
[13:13:18] ================================================================================
[13:13:18] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:18] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:18] ❌ Trial 723 failed with error: expected scalar type Long but found Float
[13:13:18] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:18] ✓ Results saved to hyperparameter_results.json
[13:13:18] ================================================================================
[13:13:18] TRIAL 724 - Starting
[13:13:18] ================================================================================
[13:13:18] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:18] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:18] ❌ Trial 724 failed with error: expected scalar type Long but found Float
[13:13:18] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:18] ✓ Results saved to hyperparameter_results.json
[13:13:18] ================================================================================
[13:13:18] TRIAL 725 - Starting
[13:13:18] ================================================================================
[13:13:18] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:18] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:18] ❌ Trial 725 failed with error: expected scalar type Long but found Float
[13:13:18] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:18] ✓ Results saved to hyperparameter_results.json
[13:13:18] 
================================================================================
[13:13:18] PROGRESS UPDATE - Completed 725 trials
[13:13:18] Best so far: 0.0000
[13:13:18] ================================================================================

[13:13:18] ================================================================================
[13:13:18] TRIAL 726 - Starting
[13:13:18] ================================================================================
[13:13:18] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:18] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:18] ❌ Trial 726 failed with error: expected scalar type Long but found Float
[13:13:18] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:18] ✓ Results saved to hyperparameter_results.json
[13:13:18] ================================================================================
[13:13:18] TRIAL 727 - Starting
[13:13:18] ================================================================================
[13:13:18] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:18] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:19] ❌ Trial 727 failed with error: expected scalar type Long but found Float
[13:13:19] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:19] ✓ Results saved to hyperparameter_results.json
[13:13:19] ================================================================================
[13:13:19] TRIAL 728 - Starting
[13:13:19] ================================================================================
[13:13:19] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:19] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:19] ❌ Trial 728 failed with error: expected scalar type Long but found Float
[13:13:19] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:19] ✓ Results saved to hyperparameter_results.json
[13:13:19] ================================================================================
[13:13:19] TRIAL 729 - Starting
[13:13:19] ================================================================================
[13:13:19] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:19] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:19] ❌ Trial 729 failed with error: expected scalar type Long but found Float
[13:13:19] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:19] ✓ Results saved to hyperparameter_results.json
[13:13:19] ================================================================================
[13:13:19] TRIAL 730 - Starting
[13:13:19] ================================================================================
[13:13:19] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:19] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:19] ❌ Trial 730 failed with error: expected scalar type Long but found Float
[13:13:19] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:19] ✓ Results saved to hyperparameter_results.json
[13:13:19] 
================================================================================
[13:13:19] PROGRESS UPDATE - Completed 730 trials
[13:13:19] Best so far: 0.0000
[13:13:19] ================================================================================

[13:13:19] ================================================================================
[13:13:19] TRIAL 731 - Starting
[13:13:19] ================================================================================
[13:13:19] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:19] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:19] ❌ Trial 731 failed with error: expected scalar type Long but found Float
[13:13:19] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:19] ✓ Results saved to hyperparameter_results.json
[13:13:19] ================================================================================
[13:13:19] TRIAL 732 - Starting
[13:13:19] ================================================================================
[13:13:19] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:19] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:19] ❌ Trial 732 failed with error: expected scalar type Long but found Float
[13:13:19] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:19] ✓ Results saved to hyperparameter_results.json
[13:13:19] ================================================================================
[13:13:19] TRIAL 733 - Starting
[13:13:19] ================================================================================
[13:13:19] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:19] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:19] ❌ Trial 733 failed with error: expected scalar type Long but found Float
[13:13:19] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:20] ✓ Results saved to hyperparameter_results.json
[13:13:20] ================================================================================
[13:13:20] TRIAL 734 - Starting
[13:13:20] ================================================================================
[13:13:20] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:20] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:20] ❌ Trial 734 failed with error: expected scalar type Long but found Float
[13:13:20] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:20] ✓ Results saved to hyperparameter_results.json
[13:13:20] ================================================================================
[13:13:20] TRIAL 735 - Starting
[13:13:20] ================================================================================
[13:13:20] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:20] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:20] ❌ Trial 735 failed with error: expected scalar type Long but found Float
[13:13:20] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:20] ✓ Results saved to hyperparameter_results.json
[13:13:20] 
================================================================================
[13:13:20] PROGRESS UPDATE - Completed 735 trials
[13:13:20] Best so far: 0.0000
[13:13:20] ================================================================================

[13:13:20] ================================================================================
[13:13:20] TRIAL 736 - Starting
[13:13:20] ================================================================================
[13:13:20] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:20] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:20] ❌ Trial 736 failed with error: expected scalar type Long but found Float
[13:13:20] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:20] ✓ Results saved to hyperparameter_results.json
[13:13:20] ================================================================================
[13:13:20] TRIAL 737 - Starting
[13:13:20] ================================================================================
[13:13:20] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:20] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:20] ❌ Trial 737 failed with error: expected scalar type Long but found Float
[13:13:20] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:20] ✓ Results saved to hyperparameter_results.json
[13:13:20] ================================================================================
[13:13:20] TRIAL 738 - Starting
[13:13:20] ================================================================================
[13:13:20] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:20] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:20] ❌ Trial 738 failed with error: expected scalar type Long but found Float
[13:13:20] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:20] ✓ Results saved to hyperparameter_results.json
[13:13:20] ================================================================================
[13:13:20] TRIAL 739 - Starting
[13:13:20] ================================================================================
[13:13:20] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:20] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:20] ❌ Trial 739 failed with error: expected scalar type Long but found Float
[13:13:20] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:20] ✓ Results saved to hyperparameter_results.json
[13:13:20] ================================================================================
[13:13:20] TRIAL 740 - Starting
[13:13:20] ================================================================================
[13:13:20] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:20] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:21] ❌ Trial 740 failed with error: expected scalar type Long but found Float
[13:13:21] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:21] ✓ Results saved to hyperparameter_results.json
[13:13:21] 
================================================================================
[13:13:21] PROGRESS UPDATE - Completed 740 trials
[13:13:21] Best so far: 0.0000
[13:13:21] ================================================================================

[13:13:21] ================================================================================
[13:13:21] TRIAL 741 - Starting
[13:13:21] ================================================================================
[13:13:21] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:21] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:21] ❌ Trial 741 failed with error: expected scalar type Long but found Float
[13:13:21] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:21] ✓ Results saved to hyperparameter_results.json
[13:13:21] ================================================================================
[13:13:21] TRIAL 742 - Starting
[13:13:21] ================================================================================
[13:13:21] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:21] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:21] ❌ Trial 742 failed with error: expected scalar type Long but found Float
[13:13:21] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:21] ✓ Results saved to hyperparameter_results.json
[13:13:21] ================================================================================
[13:13:21] TRIAL 743 - Starting
[13:13:21] ================================================================================
[13:13:21] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:21] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:21] ❌ Trial 743 failed with error: expected scalar type Long but found Float
[13:13:21] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:21] ✓ Results saved to hyperparameter_results.json
[13:13:21] ================================================================================
[13:13:21] TRIAL 744 - Starting
[13:13:21] ================================================================================
[13:13:21] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:21] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:21] ❌ Trial 744 failed with error: expected scalar type Long but found Float
[13:13:21] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:21] ✓ Results saved to hyperparameter_results.json
[13:13:21] ================================================================================
[13:13:21] TRIAL 745 - Starting
[13:13:21] ================================================================================
[13:13:21] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:21] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:21] ❌ Trial 745 failed with error: expected scalar type Long but found Float
[13:13:21] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:21] ✓ Results saved to hyperparameter_results.json
[13:13:21] 
================================================================================
[13:13:22] PROGRESS UPDATE - Completed 745 trials
[13:13:22] Best so far: 0.0000
[13:13:22] ================================================================================

[13:13:22] ================================================================================
[13:13:22] TRIAL 746 - Starting
[13:13:22] ================================================================================
[13:13:22] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:22] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:22] ❌ Trial 746 failed with error: expected scalar type Long but found Float
[13:13:22] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:22] ✓ Results saved to hyperparameter_results.json
[13:13:22] ================================================================================
[13:13:22] TRIAL 747 - Starting
[13:13:22] ================================================================================
[13:13:22] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:22] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:22] ❌ Trial 747 failed with error: expected scalar type Long but found Float
[13:13:22] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:22] ✓ Results saved to hyperparameter_results.json
[13:13:22] ================================================================================
[13:13:22] TRIAL 748 - Starting
[13:13:22] ================================================================================
[13:13:22] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:22] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:22] ❌ Trial 748 failed with error: expected scalar type Long but found Float
[13:13:22] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:22] ✓ Results saved to hyperparameter_results.json
[13:13:22] ================================================================================
[13:13:22] TRIAL 749 - Starting
[13:13:22] ================================================================================
[13:13:22] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:22] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:22] ❌ Trial 749 failed with error: expected scalar type Long but found Float
[13:13:22] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:22] ✓ Results saved to hyperparameter_results.json
[13:13:22] ================================================================================
[13:13:22] TRIAL 750 - Starting
[13:13:22] ================================================================================
[13:13:22] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:22] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:22] ❌ Trial 750 failed with error: expected scalar type Long but found Float
[13:13:22] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:22] ✓ Results saved to hyperparameter_results.json
[13:13:22] 
================================================================================
[13:13:22] PROGRESS UPDATE - Completed 750 trials
[13:13:22] Best so far: 0.0000
[13:13:22] ================================================================================

[13:13:22] ================================================================================
[13:13:22] TRIAL 751 - Starting
[13:13:22] ================================================================================
[13:13:22] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:22] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:22] ❌ Trial 751 failed with error: expected scalar type Long but found Float
[13:13:22] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:23] ✓ Results saved to hyperparameter_results.json
[13:13:23] ================================================================================
[13:13:23] TRIAL 752 - Starting
[13:13:23] ================================================================================
[13:13:23] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:23] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:23] ❌ Trial 752 failed with error: expected scalar type Long but found Float
[13:13:23] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:23] ✓ Results saved to hyperparameter_results.json
[13:13:23] ================================================================================
[13:13:23] TRIAL 753 - Starting
[13:13:23] ================================================================================
[13:13:23] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:23] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:23] ❌ Trial 753 failed with error: expected scalar type Long but found Float
[13:13:23] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:23] ✓ Results saved to hyperparameter_results.json
[13:13:23] ================================================================================
[13:13:23] TRIAL 754 - Starting
[13:13:23] ================================================================================
[13:13:23] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:23] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:23] ❌ Trial 754 failed with error: expected scalar type Long but found Float
[13:13:23] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:23] ✓ Results saved to hyperparameter_results.json
[13:13:23] ================================================================================
[13:13:23] TRIAL 755 - Starting
[13:13:23] ================================================================================
[13:13:23] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:23] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:23] ❌ Trial 755 failed with error: expected scalar type Long but found Float
[13:13:23] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:23] ✓ Results saved to hyperparameter_results.json
[13:13:23] 
================================================================================
[13:13:23] PROGRESS UPDATE - Completed 755 trials
[13:13:23] Best so far: 0.0000
[13:13:23] ================================================================================

[13:13:23] ================================================================================
[13:13:23] TRIAL 756 - Starting
[13:13:23] ================================================================================
[13:13:23] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:23] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:23] ❌ Trial 756 failed with error: expected scalar type Long but found Float
[13:13:23] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:23] ✓ Results saved to hyperparameter_results.json
[13:13:23] ================================================================================
[13:13:23] TRIAL 757 - Starting
[13:13:23] ================================================================================
[13:13:23] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:23] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:23] ❌ Trial 757 failed with error: expected scalar type Long but found Float
[13:13:23] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:23] ✓ Results saved to hyperparameter_results.json
[13:13:23] ================================================================================
[13:13:23] TRIAL 758 - Starting
[13:13:24] ================================================================================
[13:13:24] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:24] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:24] ❌ Trial 758 failed with error: expected scalar type Long but found Float
[13:13:24] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:24] ✓ Results saved to hyperparameter_results.json
[13:13:24] ================================================================================
[13:13:24] TRIAL 759 - Starting
[13:13:24] ================================================================================
[13:13:24] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:24] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:24] ❌ Trial 759 failed with error: expected scalar type Long but found Float
[13:13:24] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:24] ✓ Results saved to hyperparameter_results.json
[13:13:24] ================================================================================
[13:13:24] TRIAL 760 - Starting
[13:13:24] ================================================================================
[13:13:24] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:24] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:24] ❌ Trial 760 failed with error: expected scalar type Long but found Float
[13:13:24] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:24] ✓ Results saved to hyperparameter_results.json
[13:13:24] 
================================================================================
[13:13:24] PROGRESS UPDATE - Completed 760 trials
[13:13:24] Best so far: 0.0000
[13:13:24] ================================================================================

[13:13:24] ================================================================================
[13:13:24] TRIAL 761 - Starting
[13:13:24] ================================================================================
[13:13:24] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:24] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:24] ❌ Trial 761 failed with error: expected scalar type Long but found Float
[13:13:24] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:24] ✓ Results saved to hyperparameter_results.json
[13:13:24] ================================================================================
[13:13:24] TRIAL 762 - Starting
[13:13:24] ================================================================================
[13:13:24] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:24] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:24] ❌ Trial 762 failed with error: expected scalar type Long but found Float
[13:13:24] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:24] ✓ Results saved to hyperparameter_results.json
[13:13:24] ================================================================================
[13:13:24] TRIAL 763 - Starting
[13:13:24] ================================================================================
[13:13:24] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:24] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:25] ❌ Trial 763 failed with error: expected scalar type Long but found Float
[13:13:25] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:25] ✓ Results saved to hyperparameter_results.json
[13:13:25] ================================================================================
[13:13:25] TRIAL 764 - Starting
[13:13:25] ================================================================================
[13:13:25] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:25] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:25] ❌ Trial 764 failed with error: expected scalar type Long but found Float
[13:13:25] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:25] ✓ Results saved to hyperparameter_results.json
[13:13:25] ================================================================================
[13:13:25] TRIAL 765 - Starting
[13:13:25] ================================================================================
[13:13:25] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:25] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:25] ❌ Trial 765 failed with error: expected scalar type Long but found Float
[13:13:25] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:25] ✓ Results saved to hyperparameter_results.json
[13:13:25] 
================================================================================
[13:13:25] PROGRESS UPDATE - Completed 765 trials
[13:13:25] Best so far: 0.0000
[13:13:25] ================================================================================

[13:13:25] ================================================================================
[13:13:25] TRIAL 766 - Starting
[13:13:25] ================================================================================
[13:13:25] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:25] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:25] ❌ Trial 766 failed with error: expected scalar type Long but found Float
[13:13:25] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:25] ✓ Results saved to hyperparameter_results.json
[13:13:25] ================================================================================
[13:13:25] TRIAL 767 - Starting
[13:13:25] ================================================================================
[13:13:25] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:25] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:25] ❌ Trial 767 failed with error: expected scalar type Long but found Float
[13:13:25] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:25] ✓ Results saved to hyperparameter_results.json
[13:13:25] ================================================================================
[13:13:25] TRIAL 768 - Starting
[13:13:25] ================================================================================
[13:13:25] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:25] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:25] ❌ Trial 768 failed with error: expected scalar type Long but found Float
[13:13:25] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:25] ✓ Results saved to hyperparameter_results.json
[13:13:25] ================================================================================
[13:13:25] TRIAL 769 - Starting
[13:13:25] ================================================================================
[13:13:25] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:25] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:25] ❌ Trial 769 failed with error: expected scalar type Long but found Float
[13:13:25] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:26] ✓ Results saved to hyperparameter_results.json
[13:13:26] ================================================================================
[13:13:26] TRIAL 770 - Starting
[13:13:26] ================================================================================
[13:13:26] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:26] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:26] ❌ Trial 770 failed with error: expected scalar type Long but found Float
[13:13:26] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:26] ✓ Results saved to hyperparameter_results.json
[13:13:26] 
================================================================================
[13:13:26] PROGRESS UPDATE - Completed 770 trials
[13:13:26] Best so far: 0.0000
[13:13:26] ================================================================================

[13:13:26] ================================================================================
[13:13:26] TRIAL 771 - Starting
[13:13:26] ================================================================================
[13:13:26] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:26] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:26] ❌ Trial 771 failed with error: expected scalar type Long but found Float
[13:13:26] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:26] ✓ Results saved to hyperparameter_results.json
[13:13:26] ================================================================================
[13:13:26] TRIAL 772 - Starting
[13:13:26] ================================================================================
[13:13:26] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:26] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:26] ❌ Trial 772 failed with error: expected scalar type Long but found Float
[13:13:26] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:26] ✓ Results saved to hyperparameter_results.json
[13:13:26] ================================================================================
[13:13:26] TRIAL 773 - Starting
[13:13:26] ================================================================================
[13:13:26] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:26] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:26] ❌ Trial 773 failed with error: expected scalar type Long but found Float
[13:13:26] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:26] ✓ Results saved to hyperparameter_results.json
[13:13:26] ================================================================================
[13:13:26] TRIAL 774 - Starting
[13:13:26] ================================================================================
[13:13:26] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:26] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:26] ❌ Trial 774 failed with error: expected scalar type Long but found Float
[13:13:26] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:26] ✓ Results saved to hyperparameter_results.json
[13:13:26] ================================================================================
[13:13:26] TRIAL 775 - Starting
[13:13:26] ================================================================================
[13:13:26] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:26] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:26] ❌ Trial 775 failed with error: expected scalar type Long but found Float
[13:13:26] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:27] ✓ Results saved to hyperparameter_results.json
[13:13:27] 
================================================================================
[13:13:27] PROGRESS UPDATE - Completed 775 trials
[13:13:27] Best so far: 0.0000
[13:13:27] ================================================================================

[13:13:27] ================================================================================
[13:13:27] TRIAL 776 - Starting
[13:13:27] ================================================================================
[13:13:27] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:27] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:27] ❌ Trial 776 failed with error: expected scalar type Long but found Float
[13:13:27] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:27] ✓ Results saved to hyperparameter_results.json
[13:13:27] ================================================================================
[13:13:27] TRIAL 777 - Starting
[13:13:27] ================================================================================
[13:13:27] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:27] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:27] ❌ Trial 777 failed with error: expected scalar type Long but found Float
[13:13:27] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:27] ✓ Results saved to hyperparameter_results.json
[13:13:27] ================================================================================
[13:13:27] TRIAL 778 - Starting
[13:13:27] ================================================================================
[13:13:27] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:27] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:27] ❌ Trial 778 failed with error: expected scalar type Long but found Float
[13:13:27] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:27] ✓ Results saved to hyperparameter_results.json
[13:13:27] ================================================================================
[13:13:27] TRIAL 779 - Starting
[13:13:27] ================================================================================
[13:13:27] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:27] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:27] ❌ Trial 779 failed with error: expected scalar type Long but found Float
[13:13:27] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:27] ✓ Results saved to hyperparameter_results.json
[13:13:27] ================================================================================
[13:13:27] TRIAL 780 - Starting
[13:13:27] ================================================================================
[13:13:27] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:27] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:27] ❌ Trial 780 failed with error: expected scalar type Long but found Float
[13:13:27] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:27] ✓ Results saved to hyperparameter_results.json
[13:13:27] 
================================================================================
[13:13:27] PROGRESS UPDATE - Completed 780 trials
[13:13:27] Best so far: 0.0000
[13:13:27] ================================================================================

[13:13:27] ================================================================================
[13:13:27] TRIAL 781 - Starting
[13:13:27] ================================================================================
[13:13:27] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:28] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:28] ❌ Trial 781 failed with error: expected scalar type Long but found Float
[13:13:28] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:28] ✓ Results saved to hyperparameter_results.json
[13:13:28] ================================================================================
[13:13:28] TRIAL 782 - Starting
[13:13:28] ================================================================================
[13:13:28] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:28] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:28] ❌ Trial 782 failed with error: expected scalar type Long but found Float
[13:13:28] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:28] ✓ Results saved to hyperparameter_results.json
[13:13:28] ================================================================================
[13:13:28] TRIAL 783 - Starting
[13:13:28] ================================================================================
[13:13:28] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:28] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:28] ❌ Trial 783 failed with error: expected scalar type Long but found Float
[13:13:28] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:28] ✓ Results saved to hyperparameter_results.json
[13:13:28] ================================================================================
[13:13:28] TRIAL 784 - Starting
[13:13:28] ================================================================================
[13:13:28] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:28] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:28] ❌ Trial 784 failed with error: expected scalar type Long but found Float
[13:13:28] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:28] ✓ Results saved to hyperparameter_results.json
[13:13:28] ================================================================================
[13:13:28] TRIAL 785 - Starting
[13:13:28] ================================================================================
[13:13:28] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:28] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:28] ❌ Trial 785 failed with error: expected scalar type Long but found Float
[13:13:28] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:28] ✓ Results saved to hyperparameter_results.json
[13:13:28] 
================================================================================
[13:13:28] PROGRESS UPDATE - Completed 785 trials
[13:13:28] Best so far: 0.0000
[13:13:28] ================================================================================

[13:13:28] ================================================================================
[13:13:28] TRIAL 786 - Starting
[13:13:28] ================================================================================
[13:13:28] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:28] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:28] ❌ Trial 786 failed with error: expected scalar type Long but found Float
[13:13:28] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:28] ✓ Results saved to hyperparameter_results.json
[13:13:28] ================================================================================
[13:13:28] TRIAL 787 - Starting
[13:13:28] ================================================================================
[13:13:28] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:29] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:29] ❌ Trial 787 failed with error: expected scalar type Long but found Float
[13:13:29] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:29] ✓ Results saved to hyperparameter_results.json
[13:13:29] ================================================================================
[13:13:29] TRIAL 788 - Starting
[13:13:29] ================================================================================
[13:13:29] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:29] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:29] ❌ Trial 788 failed with error: expected scalar type Long but found Float
[13:13:29] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:29] ✓ Results saved to hyperparameter_results.json
[13:13:29] ================================================================================
[13:13:29] TRIAL 789 - Starting
[13:13:29] ================================================================================
[13:13:29] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:29] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:29] ❌ Trial 789 failed with error: expected scalar type Long but found Float
[13:13:29] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:29] ✓ Results saved to hyperparameter_results.json
[13:13:29] ================================================================================
[13:13:29] TRIAL 790 - Starting
[13:13:29] ================================================================================
[13:13:29] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:29] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:29] ❌ Trial 790 failed with error: expected scalar type Long but found Float
[13:13:29] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:29] ✓ Results saved to hyperparameter_results.json
[13:13:29] 
================================================================================
[13:13:29] PROGRESS UPDATE - Completed 790 trials
[13:13:29] Best so far: 0.0000
[13:13:29] ================================================================================

[13:13:29] ================================================================================
[13:13:29] TRIAL 791 - Starting
[13:13:29] ================================================================================
[13:13:29] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:29] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:29] ❌ Trial 791 failed with error: expected scalar type Long but found Float
[13:13:29] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:29] ✓ Results saved to hyperparameter_results.json
[13:13:29] ================================================================================
[13:13:29] TRIAL 792 - Starting
[13:13:29] ================================================================================
[13:13:29] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:29] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:29] ❌ Trial 792 failed with error: expected scalar type Long but found Float
[13:13:29] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:29] ✓ Results saved to hyperparameter_results.json
[13:13:29] ================================================================================
[13:13:29] TRIAL 793 - Starting
[13:13:29] ================================================================================
[13:13:30] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:30] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:30] ❌ Trial 793 failed with error: expected scalar type Long but found Float
[13:13:30] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:30] ✓ Results saved to hyperparameter_results.json
[13:13:30] ================================================================================
[13:13:30] TRIAL 794 - Starting
[13:13:30] ================================================================================
[13:13:30] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:30] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:30] ❌ Trial 794 failed with error: expected scalar type Long but found Float
[13:13:30] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:30] ✓ Results saved to hyperparameter_results.json
[13:13:30] ================================================================================
[13:13:30] TRIAL 795 - Starting
[13:13:30] ================================================================================
[13:13:30] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:30] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:30] ❌ Trial 795 failed with error: expected scalar type Long but found Float
[13:13:30] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:30] ✓ Results saved to hyperparameter_results.json
[13:13:30] 
================================================================================
[13:13:30] PROGRESS UPDATE - Completed 795 trials
[13:13:30] Best so far: 0.0000
[13:13:30] ================================================================================

[13:13:30] ================================================================================
[13:13:30] TRIAL 796 - Starting
[13:13:30] ================================================================================
[13:13:30] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:30] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:30] ❌ Trial 796 failed with error: expected scalar type Long but found Float
[13:13:30] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:30] ✓ Results saved to hyperparameter_results.json
[13:13:30] ================================================================================
[13:13:30] TRIAL 797 - Starting
[13:13:30] ================================================================================
[13:13:30] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:30] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:30] ❌ Trial 797 failed with error: expected scalar type Long but found Float
[13:13:30] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:30] ✓ Results saved to hyperparameter_results.json
[13:13:30] ================================================================================
[13:13:30] TRIAL 798 - Starting
[13:13:30] ================================================================================
[13:13:30] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:30] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:30] ❌ Trial 798 failed with error: expected scalar type Long but found Float
[13:13:30] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:31] ✓ Results saved to hyperparameter_results.json
[13:13:31] ================================================================================
[13:13:31] TRIAL 799 - Starting
[13:13:31] ================================================================================
[13:13:31] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:31] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:31] ❌ Trial 799 failed with error: expected scalar type Long but found Float
[13:13:31] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:31] ✓ Results saved to hyperparameter_results.json
[13:13:31] ================================================================================
[13:13:31] TRIAL 800 - Starting
[13:13:31] ================================================================================
[13:13:31] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:31] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:31] ❌ Trial 800 failed with error: expected scalar type Long but found Float
[13:13:31] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:31] ✓ Results saved to hyperparameter_results.json
[13:13:31] 
================================================================================
[13:13:31] PROGRESS UPDATE - Completed 800 trials
[13:13:31] Best so far: 0.0000
[13:13:31] ================================================================================

[13:13:31] ================================================================================
[13:13:31] TRIAL 801 - Starting
[13:13:31] ================================================================================
[13:13:31] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:31] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:31] ❌ Trial 801 failed with error: expected scalar type Long but found Float
[13:13:31] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:31] ✓ Results saved to hyperparameter_results.json
[13:13:31] ================================================================================
[13:13:31] TRIAL 802 - Starting
[13:13:31] ================================================================================
[13:13:31] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:31] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:31] ❌ Trial 802 failed with error: expected scalar type Long but found Float
[13:13:31] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:31] ✓ Results saved to hyperparameter_results.json
[13:13:31] ================================================================================
[13:13:31] TRIAL 803 - Starting
[13:13:31] ================================================================================
[13:13:31] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:31] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:31] ❌ Trial 803 failed with error: expected scalar type Long but found Float
[13:13:31] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:31] ✓ Results saved to hyperparameter_results.json
[13:13:31] ================================================================================
[13:13:31] TRIAL 804 - Starting
[13:13:31] ================================================================================
[13:13:31] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:31] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:31] ❌ Trial 804 failed with error: expected scalar type Long but found Float
[13:13:31] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:32] ✓ Results saved to hyperparameter_results.json
[13:13:32] ================================================================================
[13:13:32] TRIAL 805 - Starting
[13:13:32] ================================================================================
[13:13:32] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:32] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:32] ❌ Trial 805 failed with error: expected scalar type Long but found Float
[13:13:32] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:32] ✓ Results saved to hyperparameter_results.json
[13:13:32] 
================================================================================
[13:13:32] PROGRESS UPDATE - Completed 805 trials
[13:13:32] Best so far: 0.0000
[13:13:32] ================================================================================

[13:13:32] ================================================================================
[13:13:32] TRIAL 806 - Starting
[13:13:32] ================================================================================
[13:13:32] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:32] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:32] ❌ Trial 806 failed with error: expected scalar type Long but found Float
[13:13:32] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:32] ✓ Results saved to hyperparameter_results.json
[13:13:32] ================================================================================
[13:13:32] TRIAL 807 - Starting
[13:13:32] ================================================================================
[13:13:32] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:32] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:32] ❌ Trial 807 failed with error: expected scalar type Long but found Float
[13:13:32] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:32] ✓ Results saved to hyperparameter_results.json
[13:13:32] ================================================================================
[13:13:32] TRIAL 808 - Starting
[13:13:32] ================================================================================
[13:13:32] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:32] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:32] ❌ Trial 808 failed with error: expected scalar type Long but found Float
[13:13:32] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:32] ✓ Results saved to hyperparameter_results.json
[13:13:32] ================================================================================
[13:13:32] TRIAL 809 - Starting
[13:13:32] ================================================================================
[13:13:32] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:32] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:32] ❌ Trial 809 failed with error: expected scalar type Long but found Float
[13:13:32] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:32] ✓ Results saved to hyperparameter_results.json
[13:13:32] ================================================================================
[13:13:32] TRIAL 810 - Starting
[13:13:32] ================================================================================
[13:13:32] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:32] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:33] ❌ Trial 810 failed with error: expected scalar type Long but found Float
[13:13:33] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:33] ✓ Results saved to hyperparameter_results.json
[13:13:33] 
================================================================================
[13:13:33] PROGRESS UPDATE - Completed 810 trials
[13:13:33] Best so far: 0.0000
[13:13:33] ================================================================================

[13:13:33] ================================================================================
[13:13:33] TRIAL 811 - Starting
[13:13:33] ================================================================================
[13:13:33] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:33] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:33] ❌ Trial 811 failed with error: expected scalar type Long but found Float
[13:13:33] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:33] ✓ Results saved to hyperparameter_results.json
[13:13:33] ================================================================================
[13:13:33] TRIAL 812 - Starting
[13:13:33] ================================================================================
[13:13:33] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:33] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:33] ❌ Trial 812 failed with error: expected scalar type Long but found Float
[13:13:33] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:33] ✓ Results saved to hyperparameter_results.json
[13:13:33] ================================================================================
[13:13:33] TRIAL 813 - Starting
[13:13:33] ================================================================================
[13:13:33] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:33] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:33] ❌ Trial 813 failed with error: expected scalar type Long but found Float
[13:13:33] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:33] ✓ Results saved to hyperparameter_results.json
[13:13:33] ================================================================================
[13:13:33] TRIAL 814 - Starting
[13:13:33] ================================================================================
[13:13:33] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:33] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:33] ❌ Trial 814 failed with error: expected scalar type Long but found Float
[13:13:33] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:33] ✓ Results saved to hyperparameter_results.json
[13:13:33] ================================================================================
[13:13:33] TRIAL 815 - Starting
[13:13:33] ================================================================================
[13:13:33] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:33] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:33] ❌ Trial 815 failed with error: expected scalar type Long but found Float
[13:13:33] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:33] ✓ Results saved to hyperparameter_results.json
[13:13:33] 
================================================================================
[13:13:33] PROGRESS UPDATE - Completed 815 trials
[13:13:33] Best so far: 0.0000
[13:13:33] ================================================================================

[13:13:33] ================================================================================
[13:13:33] TRIAL 816 - Starting
[13:13:33] ================================================================================
[13:13:33] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:33] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:34] ❌ Trial 816 failed with error: expected scalar type Long but found Float
[13:13:34] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:34] ✓ Results saved to hyperparameter_results.json
[13:13:34] ================================================================================
[13:13:34] TRIAL 817 - Starting
[13:13:34] ================================================================================
[13:13:34] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:34] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:34] ❌ Trial 817 failed with error: expected scalar type Long but found Float
[13:13:34] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:34] ✓ Results saved to hyperparameter_results.json
[13:13:34] ================================================================================
[13:13:34] TRIAL 818 - Starting
[13:13:34] ================================================================================
[13:13:34] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:34] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:34] ❌ Trial 818 failed with error: expected scalar type Long but found Float
[13:13:34] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:34] ✓ Results saved to hyperparameter_results.json
[13:13:34] ================================================================================
[13:13:34] TRIAL 819 - Starting
[13:13:34] ================================================================================
[13:13:34] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:34] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:34] ❌ Trial 819 failed with error: expected scalar type Long but found Float
[13:13:34] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:34] ✓ Results saved to hyperparameter_results.json
[13:13:34] ================================================================================
[13:13:34] TRIAL 820 - Starting
[13:13:34] ================================================================================
[13:13:34] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:34] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:34] ❌ Trial 820 failed with error: expected scalar type Long but found Float
[13:13:34] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:34] ✓ Results saved to hyperparameter_results.json
[13:13:34] 
================================================================================
[13:13:34] PROGRESS UPDATE - Completed 820 trials
[13:13:34] Best so far: 0.0000
[13:13:34] ================================================================================

[13:13:34] ================================================================================
[13:13:34] TRIAL 821 - Starting
[13:13:34] ================================================================================
[13:13:34] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:34] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:34] ❌ Trial 821 failed with error: expected scalar type Long but found Float
[13:13:34] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:34] ✓ Results saved to hyperparameter_results.json
[13:13:34] ================================================================================
[13:13:34] TRIAL 822 - Starting
[13:13:34] ================================================================================
[13:13:34] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:34] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:34] ❌ Trial 822 failed with error: expected scalar type Long but found Float
[13:13:34] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:35] ✓ Results saved to hyperparameter_results.json
[13:13:35] ================================================================================
[13:13:35] TRIAL 823 - Starting
[13:13:35] ================================================================================
[13:13:35] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:35] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:35] ❌ Trial 823 failed with error: expected scalar type Long but found Float
[13:13:35] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:35] ✓ Results saved to hyperparameter_results.json
[13:13:35] ================================================================================
[13:13:35] TRIAL 824 - Starting
[13:13:35] ================================================================================
[13:13:35] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:35] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:35] ❌ Trial 824 failed with error: expected scalar type Long but found Float
[13:13:35] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:35] ✓ Results saved to hyperparameter_results.json
[13:13:35] ================================================================================
[13:13:35] TRIAL 825 - Starting
[13:13:35] ================================================================================
[13:13:35] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:35] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:35] ❌ Trial 825 failed with error: expected scalar type Long but found Float
[13:13:35] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:35] ✓ Results saved to hyperparameter_results.json
[13:13:35] 
================================================================================
[13:13:35] PROGRESS UPDATE - Completed 825 trials
[13:13:35] Best so far: 0.0000
[13:13:35] ================================================================================

[13:13:35] ================================================================================
[13:13:35] TRIAL 826 - Starting
[13:13:35] ================================================================================
[13:13:35] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:35] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:35] ❌ Trial 826 failed with error: expected scalar type Long but found Float
[13:13:35] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:35] ✓ Results saved to hyperparameter_results.json
[13:13:35] ================================================================================
[13:13:35] TRIAL 827 - Starting
[13:13:35] ================================================================================
[13:13:35] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:35] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:35] ❌ Trial 827 failed with error: expected scalar type Long but found Float
[13:13:35] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:35] ✓ Results saved to hyperparameter_results.json
[13:13:35] ================================================================================
[13:13:35] TRIAL 828 - Starting
[13:13:35] ================================================================================
[13:13:35] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:35] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:35] ❌ Trial 828 failed with error: expected scalar type Long but found Float
[13:13:35] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:36] ✓ Results saved to hyperparameter_results.json
[13:13:36] ================================================================================
[13:13:36] TRIAL 829 - Starting
[13:13:36] ================================================================================
[13:13:36] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:36] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:36] ❌ Trial 829 failed with error: expected scalar type Long but found Float
[13:13:36] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:36] ✓ Results saved to hyperparameter_results.json
[13:13:36] ================================================================================
[13:13:36] TRIAL 830 - Starting
[13:13:36] ================================================================================
[13:13:36] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:36] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:36] ❌ Trial 830 failed with error: expected scalar type Long but found Float
[13:13:36] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:36] ✓ Results saved to hyperparameter_results.json
[13:13:36] 
================================================================================
[13:13:36] PROGRESS UPDATE - Completed 830 trials
[13:13:36] Best so far: 0.0000
[13:13:36] ================================================================================

[13:13:36] ================================================================================
[13:13:36] TRIAL 831 - Starting
[13:13:36] ================================================================================
[13:13:36] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:36] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:36] ❌ Trial 831 failed with error: expected scalar type Long but found Float
[13:13:36] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:36] ✓ Results saved to hyperparameter_results.json
[13:13:36] ================================================================================
[13:13:36] TRIAL 832 - Starting
[13:13:36] ================================================================================
[13:13:36] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:36] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:36] ❌ Trial 832 failed with error: expected scalar type Long but found Float
[13:13:36] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:36] ✓ Results saved to hyperparameter_results.json
[13:13:36] ================================================================================
[13:13:36] TRIAL 833 - Starting
[13:13:36] ================================================================================
[13:13:36] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:36] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:36] ❌ Trial 833 failed with error: expected scalar type Long but found Float
[13:13:36] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:36] ✓ Results saved to hyperparameter_results.json
[13:13:36] ================================================================================
[13:13:36] TRIAL 834 - Starting
[13:13:36] ================================================================================
[13:13:36] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:36] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:36] ❌ Trial 834 failed with error: expected scalar type Long but found Float
[13:13:36] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:37] ✓ Results saved to hyperparameter_results.json
[13:13:37] ================================================================================
[13:13:37] TRIAL 835 - Starting
[13:13:37] ================================================================================
[13:13:37] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:37] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:37] ❌ Trial 835 failed with error: expected scalar type Long but found Float
[13:13:37] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:37] ✓ Results saved to hyperparameter_results.json
[13:13:37] 
================================================================================
[13:13:37] PROGRESS UPDATE - Completed 835 trials
[13:13:37] Best so far: 0.0000
[13:13:37] ================================================================================

[13:13:37] ================================================================================
[13:13:37] TRIAL 836 - Starting
[13:13:37] ================================================================================
[13:13:37] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:37] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:37] ❌ Trial 836 failed with error: expected scalar type Long but found Float
[13:13:37] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:37] ✓ Results saved to hyperparameter_results.json
[13:13:37] ================================================================================
[13:13:37] TRIAL 837 - Starting
[13:13:37] ================================================================================
[13:13:37] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:37] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:37] ❌ Trial 837 failed with error: expected scalar type Long but found Float
[13:13:37] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:37] ✓ Results saved to hyperparameter_results.json
[13:13:37] ================================================================================
[13:13:37] TRIAL 838 - Starting
[13:13:37] ================================================================================
[13:13:37] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:37] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:37] ❌ Trial 838 failed with error: expected scalar type Long but found Float
[13:13:37] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:37] ✓ Results saved to hyperparameter_results.json
[13:13:37] ================================================================================
[13:13:37] TRIAL 839 - Starting
[13:13:37] ================================================================================
[13:13:37] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:37] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:37] ❌ Trial 839 failed with error: expected scalar type Long but found Float
[13:13:37] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:37] ✓ Results saved to hyperparameter_results.json
[13:13:37] ================================================================================
[13:13:37] TRIAL 840 - Starting
[13:13:37] ================================================================================
[13:13:37] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:37] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:38] ❌ Trial 840 failed with error: expected scalar type Long but found Float
[13:13:38] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:38] ✓ Results saved to hyperparameter_results.json
[13:13:38] 
================================================================================
[13:13:38] PROGRESS UPDATE - Completed 840 trials
[13:13:38] Best so far: 0.0000
[13:13:38] ================================================================================

[13:13:38] ================================================================================
[13:13:38] TRIAL 841 - Starting
[13:13:38] ================================================================================
[13:13:38] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:38] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:38] ❌ Trial 841 failed with error: expected scalar type Long but found Float
[13:13:38] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:38] ✓ Results saved to hyperparameter_results.json
[13:13:38] ================================================================================
[13:13:38] TRIAL 842 - Starting
[13:13:38] ================================================================================
[13:13:38] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:38] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:38] ❌ Trial 842 failed with error: expected scalar type Long but found Float
[13:13:38] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:38] ✓ Results saved to hyperparameter_results.json
[13:13:38] ================================================================================
[13:13:38] TRIAL 843 - Starting
[13:13:38] ================================================================================
[13:13:38] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:38] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:38] ❌ Trial 843 failed with error: expected scalar type Long but found Float
[13:13:38] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:38] ✓ Results saved to hyperparameter_results.json
[13:13:38] ================================================================================
[13:13:38] TRIAL 844 - Starting
[13:13:38] ================================================================================
[13:13:38] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:38] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:38] ❌ Trial 844 failed with error: expected scalar type Long but found Float
[13:13:38] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:38] ✓ Results saved to hyperparameter_results.json
[13:13:38] ================================================================================
[13:13:38] TRIAL 845 - Starting
[13:13:38] ================================================================================
[13:13:38] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:38] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:38] ❌ Trial 845 failed with error: expected scalar type Long but found Float
[13:13:38] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:39] ✓ Results saved to hyperparameter_results.json
[13:13:39] 
================================================================================
[13:13:39] PROGRESS UPDATE - Completed 845 trials
[13:13:39] Best so far: 0.0000
[13:13:39] ================================================================================

[13:13:39] ================================================================================
[13:13:39] TRIAL 846 - Starting
[13:13:39] ================================================================================
[13:13:39] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:39] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:39] ❌ Trial 846 failed with error: expected scalar type Long but found Float
[13:13:39] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:39] ✓ Results saved to hyperparameter_results.json
[13:13:39] ================================================================================
[13:13:39] TRIAL 847 - Starting
[13:13:39] ================================================================================
[13:13:39] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:39] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:39] ❌ Trial 847 failed with error: expected scalar type Long but found Float
[13:13:39] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:39] ✓ Results saved to hyperparameter_results.json
[13:13:39] ================================================================================
[13:13:39] TRIAL 848 - Starting
[13:13:39] ================================================================================
[13:13:39] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:39] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:39] ❌ Trial 848 failed with error: expected scalar type Long but found Float
[13:13:39] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:39] ✓ Results saved to hyperparameter_results.json
[13:13:39] ================================================================================
[13:13:39] TRIAL 849 - Starting
[13:13:39] ================================================================================
[13:13:39] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:39] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:39] ❌ Trial 849 failed with error: expected scalar type Long but found Float
[13:13:39] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:39] ✓ Results saved to hyperparameter_results.json
[13:13:39] ================================================================================
[13:13:39] TRIAL 850 - Starting
[13:13:39] ================================================================================
[13:13:39] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:39] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:39] ❌ Trial 850 failed with error: expected scalar type Long but found Float
[13:13:39] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:39] ✓ Results saved to hyperparameter_results.json
[13:13:39] 
================================================================================
[13:13:39] PROGRESS UPDATE - Completed 850 trials
[13:13:39] Best so far: 0.0000
[13:13:39] ================================================================================

[13:13:39] ================================================================================
[13:13:39] TRIAL 851 - Starting
[13:13:39] ================================================================================
[13:13:39] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:40] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:40] ❌ Trial 851 failed with error: expected scalar type Long but found Float
[13:13:40] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:40] ✓ Results saved to hyperparameter_results.json
[13:13:40] ================================================================================
[13:13:40] TRIAL 852 - Starting
[13:13:40] ================================================================================
[13:13:40] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:40] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:40] ❌ Trial 852 failed with error: expected scalar type Long but found Float
[13:13:40] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:40] ✓ Results saved to hyperparameter_results.json
[13:13:40] ================================================================================
[13:13:40] TRIAL 853 - Starting
[13:13:40] ================================================================================
[13:13:40] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:40] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:40] ❌ Trial 853 failed with error: expected scalar type Long but found Float
[13:13:40] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:40] ✓ Results saved to hyperparameter_results.json
[13:13:40] ================================================================================
[13:13:40] TRIAL 854 - Starting
[13:13:40] ================================================================================
[13:13:40] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:40] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:40] ❌ Trial 854 failed with error: expected scalar type Long but found Float
[13:13:40] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:40] ✓ Results saved to hyperparameter_results.json
[13:13:40] ================================================================================
[13:13:40] TRIAL 855 - Starting
[13:13:40] ================================================================================
[13:13:40] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:40] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:41] ❌ Trial 855 failed with error: expected scalar type Long but found Float
[13:13:41] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:41] ✓ Results saved to hyperparameter_results.json
[13:13:41] 
================================================================================
[13:13:41] PROGRESS UPDATE - Completed 855 trials
[13:13:41] Best so far: 0.0000
[13:13:41] ================================================================================

[13:13:41] ================================================================================
[13:13:41] TRIAL 856 - Starting
[13:13:41] ================================================================================
[13:13:41] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:41] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:41] ❌ Trial 856 failed with error: expected scalar type Long but found Float
[13:13:41] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:41] ✓ Results saved to hyperparameter_results.json
[13:13:41] ================================================================================
[13:13:41] TRIAL 857 - Starting
[13:13:41] ================================================================================
[13:13:41] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:41] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:41] ❌ Trial 857 failed with error: expected scalar type Long but found Float
[13:13:41] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:41] ✓ Results saved to hyperparameter_results.json
[13:13:41] ================================================================================
[13:13:41] TRIAL 858 - Starting
[13:13:41] ================================================================================
[13:13:41] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:41] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:41] ❌ Trial 858 failed with error: expected scalar type Long but found Float
[13:13:41] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:41] ✓ Results saved to hyperparameter_results.json
[13:13:41] ================================================================================
[13:13:41] TRIAL 859 - Starting
[13:13:41] ================================================================================
[13:13:41] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:41] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:41] ❌ Trial 859 failed with error: expected scalar type Long but found Float
[13:13:41] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:41] ✓ Results saved to hyperparameter_results.json
[13:13:41] ================================================================================
[13:13:41] TRIAL 860 - Starting
[13:13:41] ================================================================================
[13:13:41] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:41] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:41] ❌ Trial 860 failed with error: expected scalar type Long but found Float
[13:13:41] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:41] ✓ Results saved to hyperparameter_results.json
[13:13:41] 
================================================================================
[13:13:41] PROGRESS UPDATE - Completed 860 trials
[13:13:41] Best so far: 0.0000
[13:13:41] ================================================================================

[13:13:41] ================================================================================
[13:13:41] TRIAL 861 - Starting
[13:13:41] ================================================================================
[13:13:41] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:41] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:42] ❌ Trial 861 failed with error: expected scalar type Long but found Float
[13:13:42] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:42] ✓ Results saved to hyperparameter_results.json
[13:13:42] ================================================================================
[13:13:42] TRIAL 862 - Starting
[13:13:42] ================================================================================
[13:13:42] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:42] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:42] ❌ Trial 862 failed with error: expected scalar type Long but found Float
[13:13:42] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:42] ✓ Results saved to hyperparameter_results.json
[13:13:42] ================================================================================
[13:13:42] TRIAL 863 - Starting
[13:13:42] ================================================================================
[13:13:42] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:42] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:42] ❌ Trial 863 failed with error: expected scalar type Long but found Float
[13:13:42] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:42] ✓ Results saved to hyperparameter_results.json
[13:13:42] ================================================================================
[13:13:42] TRIAL 864 - Starting
[13:13:42] ================================================================================
[13:13:42] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:42] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:42] ❌ Trial 864 failed with error: expected scalar type Long but found Float
[13:13:42] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:42] ✓ Results saved to hyperparameter_results.json
[13:13:42] ================================================================================
[13:13:42] TRIAL 865 - Starting
[13:13:42] ================================================================================
[13:13:42] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:42] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:42] ❌ Trial 865 failed with error: expected scalar type Long but found Float
[13:13:42] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:42] ✓ Results saved to hyperparameter_results.json
[13:13:42] 
================================================================================
[13:13:42] PROGRESS UPDATE - Completed 865 trials
[13:13:42] Best so far: 0.0000
[13:13:42] ================================================================================

[13:13:42] ================================================================================
[13:13:42] TRIAL 866 - Starting
[13:13:42] ================================================================================
[13:13:42] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:42] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:42] ❌ Trial 866 failed with error: expected scalar type Long but found Float
[13:13:42] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:43] ✓ Results saved to hyperparameter_results.json
[13:13:43] ================================================================================
[13:13:43] TRIAL 867 - Starting
[13:13:43] ================================================================================
[13:13:43] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:43] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:43] ❌ Trial 867 failed with error: expected scalar type Long but found Float
[13:13:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:43] ✓ Results saved to hyperparameter_results.json
[13:13:43] ================================================================================
[13:13:43] TRIAL 868 - Starting
[13:13:43] ================================================================================
[13:13:43] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:43] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:43] ❌ Trial 868 failed with error: expected scalar type Long but found Float
[13:13:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:43] ✓ Results saved to hyperparameter_results.json
[13:13:43] ================================================================================
[13:13:43] TRIAL 869 - Starting
[13:13:43] ================================================================================
[13:13:43] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:43] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:43] ❌ Trial 869 failed with error: expected scalar type Long but found Float
[13:13:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:43] ✓ Results saved to hyperparameter_results.json
[13:13:43] ================================================================================
[13:13:43] TRIAL 870 - Starting
[13:13:43] ================================================================================
[13:13:43] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:43] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:43] ❌ Trial 870 failed with error: expected scalar type Long but found Float
[13:13:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:43] ✓ Results saved to hyperparameter_results.json
[13:13:43] 
================================================================================
[13:13:43] PROGRESS UPDATE - Completed 870 trials
[13:13:43] Best so far: 0.0000
[13:13:43] ================================================================================

[13:13:43] ================================================================================
[13:13:43] TRIAL 871 - Starting
[13:13:43] ================================================================================
[13:13:43] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:43] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:43] ❌ Trial 871 failed with error: expected scalar type Long but found Float
[13:13:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:43] ✓ Results saved to hyperparameter_results.json
[13:13:43] ================================================================================
[13:13:43] TRIAL 872 - Starting
[13:13:43] ================================================================================
[13:13:43] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:43] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:43] ❌ Trial 872 failed with error: expected scalar type Long but found Float
[13:13:43] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:44] ✓ Results saved to hyperparameter_results.json
[13:13:44] ================================================================================
[13:13:44] TRIAL 873 - Starting
[13:13:44] ================================================================================
[13:13:44] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:44] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:44] ❌ Trial 873 failed with error: expected scalar type Long but found Float
[13:13:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:44] ✓ Results saved to hyperparameter_results.json
[13:13:44] ================================================================================
[13:13:44] TRIAL 874 - Starting
[13:13:44] ================================================================================
[13:13:44] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:44] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:44] ❌ Trial 874 failed with error: expected scalar type Long but found Float
[13:13:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:44] ✓ Results saved to hyperparameter_results.json
[13:13:44] ================================================================================
[13:13:44] TRIAL 875 - Starting
[13:13:44] ================================================================================
[13:13:44] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:44] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:44] ❌ Trial 875 failed with error: expected scalar type Long but found Float
[13:13:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:44] ✓ Results saved to hyperparameter_results.json
[13:13:44] 
================================================================================
[13:13:44] PROGRESS UPDATE - Completed 875 trials
[13:13:44] Best so far: 0.0000
[13:13:44] ================================================================================

[13:13:44] ================================================================================
[13:13:44] TRIAL 876 - Starting
[13:13:44] ================================================================================
[13:13:44] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:44] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:44] ❌ Trial 876 failed with error: expected scalar type Long but found Float
[13:13:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:44] ✓ Results saved to hyperparameter_results.json
[13:13:44] ================================================================================
[13:13:44] TRIAL 877 - Starting
[13:13:44] ================================================================================
[13:13:44] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:44] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:44] ❌ Trial 877 failed with error: expected scalar type Long but found Float
[13:13:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:44] ✓ Results saved to hyperparameter_results.json
[13:13:44] ================================================================================
[13:13:44] TRIAL 878 - Starting
[13:13:44] ================================================================================
[13:13:44] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:44] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:44] ❌ Trial 878 failed with error: expected scalar type Long but found Float
[13:13:44] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:45] ✓ Results saved to hyperparameter_results.json
[13:13:45] ================================================================================
[13:13:45] TRIAL 879 - Starting
[13:13:45] ================================================================================
[13:13:45] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:45] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:45] ❌ Trial 879 failed with error: expected scalar type Long but found Float
[13:13:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:45] ✓ Results saved to hyperparameter_results.json
[13:13:45] ================================================================================
[13:13:45] TRIAL 880 - Starting
[13:13:45] ================================================================================
[13:13:45] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:45] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:45] ❌ Trial 880 failed with error: expected scalar type Long but found Float
[13:13:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:45] ✓ Results saved to hyperparameter_results.json
[13:13:45] 
================================================================================
[13:13:45] PROGRESS UPDATE - Completed 880 trials
[13:13:45] Best so far: 0.0000
[13:13:45] ================================================================================

[13:13:45] ================================================================================
[13:13:45] TRIAL 881 - Starting
[13:13:45] ================================================================================
[13:13:45] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:45] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:45] ❌ Trial 881 failed with error: expected scalar type Long but found Float
[13:13:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:45] ✓ Results saved to hyperparameter_results.json
[13:13:45] ================================================================================
[13:13:45] TRIAL 882 - Starting
[13:13:45] ================================================================================
[13:13:45] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:45] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:45] ❌ Trial 882 failed with error: expected scalar type Long but found Float
[13:13:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:45] ✓ Results saved to hyperparameter_results.json
[13:13:45] ================================================================================
[13:13:45] TRIAL 883 - Starting
[13:13:45] ================================================================================
[13:13:45] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:45] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:45] ❌ Trial 883 failed with error: expected scalar type Long but found Float
[13:13:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:45] ✓ Results saved to hyperparameter_results.json
[13:13:45] ================================================================================
[13:13:45] TRIAL 884 - Starting
[13:13:45] ================================================================================
[13:13:45] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:45] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:45] ❌ Trial 884 failed with error: expected scalar type Long but found Float
[13:13:45] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:45] ✓ Results saved to hyperparameter_results.json
[13:13:45] ================================================================================
[13:13:45] TRIAL 885 - Starting
[13:13:45] ================================================================================
[13:13:45] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:46] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:46] ❌ Trial 885 failed with error: expected scalar type Long but found Float
[13:13:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:46] ✓ Results saved to hyperparameter_results.json
[13:13:46] 
================================================================================
[13:13:46] PROGRESS UPDATE - Completed 885 trials
[13:13:46] Best so far: 0.0000
[13:13:46] ================================================================================

[13:13:46] ================================================================================
[13:13:46] TRIAL 886 - Starting
[13:13:46] ================================================================================
[13:13:46] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:46] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:46] ❌ Trial 886 failed with error: expected scalar type Long but found Float
[13:13:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:46] ✓ Results saved to hyperparameter_results.json
[13:13:46] ================================================================================
[13:13:46] TRIAL 887 - Starting
[13:13:46] ================================================================================
[13:13:46] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:46] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:46] ❌ Trial 887 failed with error: expected scalar type Long but found Float
[13:13:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:46] ✓ Results saved to hyperparameter_results.json
[13:13:46] ================================================================================
[13:13:46] TRIAL 888 - Starting
[13:13:46] ================================================================================
[13:13:46] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:46] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:46] ❌ Trial 888 failed with error: expected scalar type Long but found Float
[13:13:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:46] ✓ Results saved to hyperparameter_results.json
[13:13:46] ================================================================================
[13:13:46] TRIAL 889 - Starting
[13:13:46] ================================================================================
[13:13:46] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:46] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:46] ❌ Trial 889 failed with error: expected scalar type Long but found Float
[13:13:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:46] ✓ Results saved to hyperparameter_results.json
[13:13:46] ================================================================================
[13:13:46] TRIAL 890 - Starting
[13:13:46] ================================================================================
[13:13:46] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:46] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:46] ❌ Trial 890 failed with error: expected scalar type Long but found Float
[13:13:46] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:47] ✓ Results saved to hyperparameter_results.json
[13:13:47] 
================================================================================
[13:13:47] PROGRESS UPDATE - Completed 890 trials
[13:13:47] Best so far: 0.0000
[13:13:47] ================================================================================

[13:13:47] ================================================================================
[13:13:47] TRIAL 891 - Starting
[13:13:47] ================================================================================
[13:13:47] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:47] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:47] ❌ Trial 891 failed with error: expected scalar type Long but found Float
[13:13:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:47] ✓ Results saved to hyperparameter_results.json
[13:13:47] ================================================================================
[13:13:47] TRIAL 892 - Starting
[13:13:47] ================================================================================
[13:13:47] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:47] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:47] ❌ Trial 892 failed with error: expected scalar type Long but found Float
[13:13:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:47] ✓ Results saved to hyperparameter_results.json
[13:13:47] ================================================================================
[13:13:47] TRIAL 893 - Starting
[13:13:47] ================================================================================
[13:13:47] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:47] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:47] ❌ Trial 893 failed with error: expected scalar type Long but found Float
[13:13:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:47] ✓ Results saved to hyperparameter_results.json
[13:13:47] ================================================================================
[13:13:47] TRIAL 894 - Starting
[13:13:47] ================================================================================
[13:13:47] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:47] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:47] ❌ Trial 894 failed with error: expected scalar type Long but found Float
[13:13:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:47] ✓ Results saved to hyperparameter_results.json
[13:13:47] ================================================================================
[13:13:47] TRIAL 895 - Starting
[13:13:47] ================================================================================
[13:13:47] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:47] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:47] ❌ Trial 895 failed with error: expected scalar type Long but found Float
[13:13:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:47] ✓ Results saved to hyperparameter_results.json
[13:13:47] 
================================================================================
[13:13:47] PROGRESS UPDATE - Completed 895 trials
[13:13:47] Best so far: 0.0000
[13:13:47] ================================================================================

[13:13:47] ================================================================================
[13:13:47] TRIAL 896 - Starting
[13:13:47] ================================================================================
[13:13:47] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:47] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:47] ❌ Trial 896 failed with error: expected scalar type Long but found Float
[13:13:47] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:48] ✓ Results saved to hyperparameter_results.json
[13:13:48] ================================================================================
[13:13:48] TRIAL 897 - Starting
[13:13:48] ================================================================================
[13:13:48] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:48] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:48] ❌ Trial 897 failed with error: expected scalar type Long but found Float
[13:13:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:48] ✓ Results saved to hyperparameter_results.json
[13:13:48] ================================================================================
[13:13:48] TRIAL 898 - Starting
[13:13:48] ================================================================================
[13:13:48] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:48] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:48] ❌ Trial 898 failed with error: expected scalar type Long but found Float
[13:13:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:48] ✓ Results saved to hyperparameter_results.json
[13:13:48] ================================================================================
[13:13:48] TRIAL 899 - Starting
[13:13:48] ================================================================================
[13:13:48] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:48] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:48] ❌ Trial 899 failed with error: expected scalar type Long but found Float
[13:13:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:48] ✓ Results saved to hyperparameter_results.json
[13:13:48] ================================================================================
[13:13:48] TRIAL 900 - Starting
[13:13:48] ================================================================================
[13:13:48] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:48] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:48] ❌ Trial 900 failed with error: expected scalar type Long but found Float
[13:13:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:48] ✓ Results saved to hyperparameter_results.json
[13:13:48] 
================================================================================
[13:13:48] PROGRESS UPDATE - Completed 900 trials
[13:13:48] Best so far: 0.0000
[13:13:48] ================================================================================

[13:13:48] ================================================================================
[13:13:48] TRIAL 901 - Starting
[13:13:48] ================================================================================
[13:13:48] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:48] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:48] ❌ Trial 901 failed with error: expected scalar type Long but found Float
[13:13:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:48] ✓ Results saved to hyperparameter_results.json
[13:13:48] ================================================================================
[13:13:48] TRIAL 902 - Starting
[13:13:48] ================================================================================
[13:13:48] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:48] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:48] ❌ Trial 902 failed with error: expected scalar type Long but found Float
[13:13:48] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:49] ✓ Results saved to hyperparameter_results.json
[13:13:49] ================================================================================
[13:13:49] TRIAL 903 - Starting
[13:13:49] ================================================================================
[13:13:49] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:49] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:49] ❌ Trial 903 failed with error: expected scalar type Long but found Float
[13:13:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:49] ✓ Results saved to hyperparameter_results.json
[13:13:49] ================================================================================
[13:13:49] TRIAL 904 - Starting
[13:13:49] ================================================================================
[13:13:49] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:49] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:49] ❌ Trial 904 failed with error: expected scalar type Long but found Float
[13:13:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:49] ✓ Results saved to hyperparameter_results.json
[13:13:49] ================================================================================
[13:13:49] TRIAL 905 - Starting
[13:13:49] ================================================================================
[13:13:49] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:49] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:49] ❌ Trial 905 failed with error: expected scalar type Long but found Float
[13:13:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:49] ✓ Results saved to hyperparameter_results.json
[13:13:49] 
================================================================================
[13:13:49] PROGRESS UPDATE - Completed 905 trials
[13:13:49] Best so far: 0.0000
[13:13:49] ================================================================================

[13:13:49] ================================================================================
[13:13:49] TRIAL 906 - Starting
[13:13:49] ================================================================================
[13:13:49] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:49] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:49] ❌ Trial 906 failed with error: expected scalar type Long but found Float
[13:13:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:49] ✓ Results saved to hyperparameter_results.json
[13:13:49] ================================================================================
[13:13:49] TRIAL 907 - Starting
[13:13:49] ================================================================================
[13:13:49] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:49] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:49] ❌ Trial 907 failed with error: expected scalar type Long but found Float
[13:13:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:49] ✓ Results saved to hyperparameter_results.json
[13:13:49] ================================================================================
[13:13:49] TRIAL 908 - Starting
[13:13:49] ================================================================================
[13:13:49] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:49] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:49] ❌ Trial 908 failed with error: expected scalar type Long but found Float
[13:13:49] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:50] ✓ Results saved to hyperparameter_results.json
[13:13:50] ================================================================================
[13:13:50] TRIAL 909 - Starting
[13:13:50] ================================================================================
[13:13:50] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:50] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:50] ❌ Trial 909 failed with error: expected scalar type Long but found Float
[13:13:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:50] ✓ Results saved to hyperparameter_results.json
[13:13:50] ================================================================================
[13:13:50] TRIAL 910 - Starting
[13:13:50] ================================================================================
[13:13:50] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:50] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:50] ❌ Trial 910 failed with error: expected scalar type Long but found Float
[13:13:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:50] ✓ Results saved to hyperparameter_results.json
[13:13:50] 
================================================================================
[13:13:50] PROGRESS UPDATE - Completed 910 trials
[13:13:50] Best so far: 0.0000
[13:13:50] ================================================================================

[13:13:50] ================================================================================
[13:13:50] TRIAL 911 - Starting
[13:13:50] ================================================================================
[13:13:50] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:50] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:50] ❌ Trial 911 failed with error: expected scalar type Long but found Float
[13:13:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:50] ✓ Results saved to hyperparameter_results.json
[13:13:50] ================================================================================
[13:13:50] TRIAL 912 - Starting
[13:13:50] ================================================================================
[13:13:50] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:50] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:50] ❌ Trial 912 failed with error: expected scalar type Long but found Float
[13:13:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:50] ✓ Results saved to hyperparameter_results.json
[13:13:50] ================================================================================
[13:13:50] TRIAL 913 - Starting
[13:13:50] ================================================================================
[13:13:50] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:50] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:50] ❌ Trial 913 failed with error: expected scalar type Long but found Float
[13:13:50] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:51] ✓ Results saved to hyperparameter_results.json
[13:13:51] ================================================================================
[13:13:51] TRIAL 914 - Starting
[13:13:51] ================================================================================
[13:13:51] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:51] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:51] ❌ Trial 914 failed with error: expected scalar type Long but found Float
[13:13:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:51] ✓ Results saved to hyperparameter_results.json
[13:13:51] ================================================================================
[13:13:51] TRIAL 915 - Starting
[13:13:51] ================================================================================
[13:13:51] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:51] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:51] ❌ Trial 915 failed with error: expected scalar type Long but found Float
[13:13:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:51] ✓ Results saved to hyperparameter_results.json
[13:13:51] 
================================================================================
[13:13:51] PROGRESS UPDATE - Completed 915 trials
[13:13:51] Best so far: 0.0000
[13:13:51] ================================================================================

[13:13:51] ================================================================================
[13:13:51] TRIAL 916 - Starting
[13:13:51] ================================================================================
[13:13:51] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:51] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:51] ❌ Trial 916 failed with error: expected scalar type Long but found Float
[13:13:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:51] ✓ Results saved to hyperparameter_results.json
[13:13:51] ================================================================================
[13:13:51] TRIAL 917 - Starting
[13:13:51] ================================================================================
[13:13:51] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:51] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:51] ❌ Trial 917 failed with error: expected scalar type Long but found Float
[13:13:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:51] ✓ Results saved to hyperparameter_results.json
[13:13:51] ================================================================================
[13:13:51] TRIAL 918 - Starting
[13:13:51] ================================================================================
[13:13:51] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:51] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:51] ❌ Trial 918 failed with error: expected scalar type Long but found Float
[13:13:51] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:52] ✓ Results saved to hyperparameter_results.json
[13:13:52] ================================================================================
[13:13:52] TRIAL 919 - Starting
[13:13:52] ================================================================================
[13:13:52] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:52] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:52] ❌ Trial 919 failed with error: expected scalar type Long but found Float
[13:13:52] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:52] ✓ Results saved to hyperparameter_results.json
[13:13:52] ================================================================================
[13:13:52] TRIAL 920 - Starting
[13:13:52] ================================================================================
[13:13:52] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:52] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:52] ❌ Trial 920 failed with error: expected scalar type Long but found Float
[13:13:52] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:52] ✓ Results saved to hyperparameter_results.json
[13:13:52] 
================================================================================
[13:13:52] PROGRESS UPDATE - Completed 920 trials
[13:13:52] Best so far: 0.0000
[13:13:52] ================================================================================

[13:13:52] ================================================================================
[13:13:52] TRIAL 921 - Starting
[13:13:52] ================================================================================
[13:13:52] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:52] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:52] ❌ Trial 921 failed with error: expected scalar type Long but found Float
[13:13:52] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:52] ✓ Results saved to hyperparameter_results.json
[13:13:52] ================================================================================
[13:13:52] TRIAL 922 - Starting
[13:13:52] ================================================================================
[13:13:52] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:52] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:52] ❌ Trial 922 failed with error: expected scalar type Long but found Float
[13:13:52] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:52] ✓ Results saved to hyperparameter_results.json
[13:13:52] ================================================================================
[13:13:52] TRIAL 923 - Starting
[13:13:52] ================================================================================
[13:13:52] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:52] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:52] ❌ Trial 923 failed with error: expected scalar type Long but found Float
[13:13:52] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:53] ✓ Results saved to hyperparameter_results.json
[13:13:53] ================================================================================
[13:13:53] TRIAL 924 - Starting
[13:13:53] ================================================================================
[13:13:53] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:53] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:53] ❌ Trial 924 failed with error: expected scalar type Long but found Float
[13:13:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:53] ✓ Results saved to hyperparameter_results.json
[13:13:53] ================================================================================
[13:13:53] TRIAL 925 - Starting
[13:13:53] ================================================================================
[13:13:53] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:53] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:53] ❌ Trial 925 failed with error: expected scalar type Long but found Float
[13:13:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:53] ✓ Results saved to hyperparameter_results.json
[13:13:53] 
================================================================================
[13:13:53] PROGRESS UPDATE - Completed 925 trials
[13:13:53] Best so far: 0.0000
[13:13:53] ================================================================================

[13:13:53] ================================================================================
[13:13:53] TRIAL 926 - Starting
[13:13:53] ================================================================================
[13:13:53] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:53] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:53] ❌ Trial 926 failed with error: expected scalar type Long but found Float
[13:13:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:53] ✓ Results saved to hyperparameter_results.json
[13:13:53] ================================================================================
[13:13:53] TRIAL 927 - Starting
[13:13:53] ================================================================================
[13:13:53] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:53] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:53] ❌ Trial 927 failed with error: expected scalar type Long but found Float
[13:13:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:53] ✓ Results saved to hyperparameter_results.json
[13:13:53] ================================================================================
[13:13:53] TRIAL 928 - Starting
[13:13:53] ================================================================================
[13:13:53] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:53] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:53] ❌ Trial 928 failed with error: expected scalar type Long but found Float
[13:13:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:53] ✓ Results saved to hyperparameter_results.json
[13:13:53] ================================================================================
[13:13:53] TRIAL 929 - Starting
[13:13:53] ================================================================================
[13:13:53] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 5.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:53] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 5.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:53] ❌ Trial 929 failed with error: expected scalar type Long but found Float
[13:13:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:53] ✓ Results saved to hyperparameter_results.json
[13:13:53] ================================================================================
[13:13:53] TRIAL 930 - Starting
[13:13:53] ================================================================================
[13:13:53] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:53] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:53] ❌ Trial 930 failed with error: expected scalar type Long but found Float
[13:13:53] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:53] ✓ Results saved to hyperparameter_results.json
[13:13:53] 
================================================================================
[13:13:53] PROGRESS UPDATE - Completed 930 trials
[13:13:53] Best so far: 0.0000
[13:13:53] ================================================================================

[13:13:53] ================================================================================
[13:13:54] TRIAL 931 - Starting
[13:13:54] ================================================================================
[13:13:54] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:54] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:54] ❌ Trial 931 failed with error: expected scalar type Long but found Float
[13:13:54] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:54] ✓ Results saved to hyperparameter_results.json
[13:13:54] ================================================================================
[13:13:54] TRIAL 932 - Starting
[13:13:54] ================================================================================
[13:13:54] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:54] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:54] ❌ Trial 932 failed with error: expected scalar type Long but found Float
[13:13:54] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:54] ✓ Results saved to hyperparameter_results.json
[13:13:54] ================================================================================
[13:13:54] TRIAL 933 - Starting
[13:13:54] ================================================================================
[13:13:54] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:54] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:54] ❌ Trial 933 failed with error: expected scalar type Long but found Float
[13:13:54] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:54] ✓ Results saved to hyperparameter_results.json
[13:13:54] ================================================================================
[13:13:54] TRIAL 934 - Starting
[13:13:54] ================================================================================
[13:13:54] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:54] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:54] ❌ Trial 934 failed with error: expected scalar type Long but found Float
[13:13:54] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:54] ✓ Results saved to hyperparameter_results.json
[13:13:54] ================================================================================
[13:13:54] TRIAL 935 - Starting
[13:13:54] ================================================================================
[13:13:54] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:54] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:54] ❌ Trial 935 failed with error: expected scalar type Long but found Float
[13:13:54] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:55] ✓ Results saved to hyperparameter_results.json
[13:13:55] 
================================================================================
[13:13:55] PROGRESS UPDATE - Completed 935 trials
[13:13:55] Best so far: 0.0000
[13:13:55] ================================================================================

[13:13:55] ================================================================================
[13:13:55] TRIAL 936 - Starting
[13:13:55] ================================================================================
[13:13:55] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:55] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:55] ❌ Trial 936 failed with error: expected scalar type Long but found Float
[13:13:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:55] ✓ Results saved to hyperparameter_results.json
[13:13:55] ================================================================================
[13:13:55] TRIAL 937 - Starting
[13:13:55] ================================================================================
[13:13:55] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:55] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:55] ❌ Trial 937 failed with error: expected scalar type Long but found Float
[13:13:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:55] ✓ Results saved to hyperparameter_results.json
[13:13:55] ================================================================================
[13:13:55] TRIAL 938 - Starting
[13:13:55] ================================================================================
[13:13:55] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.2,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:55] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.2, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:55] ❌ Trial 938 failed with error: expected scalar type Long but found Float
[13:13:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:55] ✓ Results saved to hyperparameter_results.json
[13:13:55] ================================================================================
[13:13:55] TRIAL 939 - Starting
[13:13:55] ================================================================================
[13:13:55] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:55] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:55] ❌ Trial 939 failed with error: expected scalar type Long but found Float
[13:13:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:55] ✓ Results saved to hyperparameter_results.json
[13:13:55] ================================================================================
[13:13:55] TRIAL 940 - Starting
[13:13:55] ================================================================================
[13:13:55] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:55] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:55] ❌ Trial 940 failed with error: expected scalar type Long but found Float
[13:13:55] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:55] ✓ Results saved to hyperparameter_results.json
[13:13:55] 
================================================================================
[13:13:55] PROGRESS UPDATE - Completed 940 trials
[13:13:55] Best so far: 0.0000
[13:13:55] ================================================================================

[13:13:55] ================================================================================
[13:13:55] TRIAL 941 - Starting
[13:13:55] ================================================================================
[13:13:55] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:55] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:56] ❌ Trial 941 failed with error: expected scalar type Long but found Float
[13:13:56] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:56] ✓ Results saved to hyperparameter_results.json
[13:13:56] ================================================================================
[13:13:56] TRIAL 942 - Starting
[13:13:56] ================================================================================
[13:13:56] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:56] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:56] ❌ Trial 942 failed with error: expected scalar type Long but found Float
[13:13:56] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:56] ✓ Results saved to hyperparameter_results.json
[13:13:56] ================================================================================
[13:13:56] TRIAL 943 - Starting
[13:13:56] ================================================================================
[13:13:56] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:56] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:56] ❌ Trial 943 failed with error: expected scalar type Long but found Float
[13:13:56] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:56] ✓ Results saved to hyperparameter_results.json
[13:13:56] ================================================================================
[13:13:56] TRIAL 944 - Starting
[13:13:56] ================================================================================
[13:13:56] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:56] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:56] ❌ Trial 944 failed with error: expected scalar type Long but found Float
[13:13:56] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:56] ✓ Results saved to hyperparameter_results.json
[13:13:56] ================================================================================
[13:13:56] TRIAL 945 - Starting
[13:13:56] ================================================================================
[13:13:56] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:56] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:56] ❌ Trial 945 failed with error: expected scalar type Long but found Float
[13:13:56] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:56] ✓ Results saved to hyperparameter_results.json
[13:13:56] 
================================================================================
[13:13:56] PROGRESS UPDATE - Completed 945 trials
[13:13:56] Best so far: 0.0000
[13:13:56] ================================================================================

[13:13:56] ================================================================================
[13:13:56] TRIAL 946 - Starting
[13:13:56] ================================================================================
[13:13:56] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:56] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:56] ❌ Trial 946 failed with error: expected scalar type Long but found Float
[13:13:56] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:57] ✓ Results saved to hyperparameter_results.json
[13:13:57] ================================================================================
[13:13:57] TRIAL 947 - Starting
[13:13:57] ================================================================================
[13:13:57] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:57] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:57] ❌ Trial 947 failed with error: expected scalar type Long but found Float
[13:13:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:57] ✓ Results saved to hyperparameter_results.json
[13:13:57] ================================================================================
[13:13:57] TRIAL 948 - Starting
[13:13:57] ================================================================================
[13:13:57] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:57] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:57] ❌ Trial 948 failed with error: expected scalar type Long but found Float
[13:13:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:57] ✓ Results saved to hyperparameter_results.json
[13:13:57] ================================================================================
[13:13:57] TRIAL 949 - Starting
[13:13:57] ================================================================================
[13:13:57] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:13:57] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:13:57] ❌ Trial 949 failed with error: expected scalar type Long but found Float
[13:13:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:57] ✓ Results saved to hyperparameter_results.json
[13:13:57] ================================================================================
[13:13:57] TRIAL 950 - Starting
[13:13:57] ================================================================================
[13:13:57] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:57] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:57] ❌ Trial 950 failed with error: expected scalar type Long but found Float
[13:13:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:57] ✓ Results saved to hyperparameter_results.json
[13:13:57] 
================================================================================
[13:13:57] PROGRESS UPDATE - Completed 950 trials
[13:13:57] Best so far: 0.0000
[13:13:57] ================================================================================

[13:13:57] ================================================================================
[13:13:57] TRIAL 951 - Starting
[13:13:57] ================================================================================
[13:13:57] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:57] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:13:57] ❌ Trial 951 failed with error: expected scalar type Long but found Float
[13:13:57] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:57] ✓ Results saved to hyperparameter_results.json
[13:13:57] ================================================================================
[13:13:57] TRIAL 952 - Starting
[13:13:57] ================================================================================
[13:13:57] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:57] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:58] ❌ Trial 952 failed with error: expected scalar type Long but found Float
[13:13:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:58] ✓ Results saved to hyperparameter_results.json
[13:13:58] ================================================================================
[13:13:58] TRIAL 953 - Starting
[13:13:58] ================================================================================
[13:13:58] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:13:58] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:13:58] ❌ Trial 953 failed with error: expected scalar type Long but found Float
[13:13:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:58] ✓ Results saved to hyperparameter_results.json
[13:13:58] ================================================================================
[13:13:58] TRIAL 954 - Starting
[13:13:58] ================================================================================
[13:13:58] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:13:58] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:13:58] ❌ Trial 954 failed with error: expected scalar type Long but found Float
[13:13:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:58] ✓ Results saved to hyperparameter_results.json
[13:13:58] ================================================================================
[13:13:58] TRIAL 955 - Starting
[13:13:58] ================================================================================
[13:13:58] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:58] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:58] ❌ Trial 955 failed with error: expected scalar type Long but found Float
[13:13:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:58] ✓ Results saved to hyperparameter_results.json
[13:13:58] 
================================================================================
[13:13:58] PROGRESS UPDATE - Completed 955 trials
[13:13:58] Best so far: 0.0000
[13:13:58] ================================================================================

[13:13:58] ================================================================================
[13:13:58] TRIAL 956 - Starting
[13:13:58] ================================================================================
[13:13:58] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:58] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:58] ❌ Trial 956 failed with error: expected scalar type Long but found Float
[13:13:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:58] ✓ Results saved to hyperparameter_results.json
[13:13:58] ================================================================================
[13:13:58] TRIAL 957 - Starting
[13:13:58] ================================================================================
[13:13:58] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:58] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:58] ❌ Trial 957 failed with error: expected scalar type Long but found Float
[13:13:58] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:59] ✓ Results saved to hyperparameter_results.json
[13:13:59] ================================================================================
[13:13:59] TRIAL 958 - Starting
[13:13:59] ================================================================================
[13:13:59] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:13:59] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:13:59] ❌ Trial 958 failed with error: expected scalar type Long but found Float
[13:13:59] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:59] ✓ Results saved to hyperparameter_results.json
[13:13:59] ================================================================================
[13:13:59] TRIAL 959 - Starting
[13:13:59] ================================================================================
[13:13:59] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:13:59] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:13:59] ❌ Trial 959 failed with error: expected scalar type Long but found Float
[13:13:59] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:59] ✓ Results saved to hyperparameter_results.json
[13:13:59] ================================================================================
[13:13:59] TRIAL 960 - Starting
[13:13:59] ================================================================================
[13:13:59] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:13:59] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:13:59] ❌ Trial 960 failed with error: expected scalar type Long but found Float
[13:13:59] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:59] ✓ Results saved to hyperparameter_results.json
[13:13:59] 
================================================================================
[13:13:59] PROGRESS UPDATE - Completed 960 trials
[13:13:59] Best so far: 0.0000
[13:13:59] ================================================================================

[13:13:59] ================================================================================
[13:13:59] TRIAL 961 - Starting
[13:13:59] ================================================================================
[13:13:59] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.1,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:13:59] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.1, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:13:59] ❌ Trial 961 failed with error: expected scalar type Long but found Float
[13:13:59] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:59] ✓ Results saved to hyperparameter_results.json
[13:13:59] ================================================================================
[13:13:59] TRIAL 962 - Starting
[13:13:59] ================================================================================
[13:13:59] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 256,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:13:59] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 256, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:13:59] ❌ Trial 962 failed with error: expected scalar type Long but found Float
[13:13:59] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:13:59] ✓ Results saved to hyperparameter_results.json
[13:13:59] ================================================================================
[13:13:59] TRIAL 963 - Starting
[13:13:59] ================================================================================
[13:13:59] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:13:59] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:14:00] ❌ Trial 963 failed with error: expected scalar type Long but found Float
[13:14:00] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:00] ✓ Results saved to hyperparameter_results.json
[13:14:00] ================================================================================
[13:14:00] TRIAL 964 - Starting
[13:14:00] ================================================================================
[13:14:00] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:14:00] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:14:00] ❌ Trial 964 failed with error: expected scalar type Long but found Float
[13:14:00] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:00] ✓ Results saved to hyperparameter_results.json
[13:14:00] ================================================================================
[13:14:00] TRIAL 965 - Starting
[13:14:00] ================================================================================
[13:14:00] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.3
}
[13:14:00] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.3}
[13:14:00] ❌ Trial 965 failed with error: expected scalar type Long but found Float
[13:14:00] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:00] ✓ Results saved to hyperparameter_results.json
[13:14:00] 
================================================================================
[13:14:00] PROGRESS UPDATE - Completed 965 trials
[13:14:00] Best so far: 0.0000
[13:14:00] ================================================================================

[13:14:00] ================================================================================
[13:14:00] TRIAL 966 - Starting
[13:14:00] ================================================================================
[13:14:00] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:14:00] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:14:00] ❌ Trial 966 failed with error: expected scalar type Long but found Float
[13:14:00] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:00] ✓ Results saved to hyperparameter_results.json
[13:14:00] ================================================================================
[13:14:00] TRIAL 967 - Starting
[13:14:00] ================================================================================
[13:14:00] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 0,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:14:00] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 0, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:14:00] ❌ Trial 967 failed with error: expected scalar type Long but found Float
[13:14:00] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:01] ✓ Results saved to hyperparameter_results.json
[13:14:01] ================================================================================
[13:14:01] TRIAL 968 - Starting
[13:14:01] ================================================================================
[13:14:01] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 2.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:14:01] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 2.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:14:01] ❌ Trial 968 failed with error: expected scalar type Long but found Float
[13:14:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:01] ✓ Results saved to hyperparameter_results.json
[13:14:01] ================================================================================
[13:14:01] TRIAL 969 - Starting
[13:14:01] ================================================================================
[13:14:01] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:14:01] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:14:01] ❌ Trial 969 failed with error: expected scalar type Long but found Float
[13:14:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:01] ✓ Results saved to hyperparameter_results.json
[13:14:01] ================================================================================
[13:14:01] TRIAL 970 - Starting
[13:14:01] ================================================================================
[13:14:01] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.4,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:14:01] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.4, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:14:01] ❌ Trial 970 failed with error: expected scalar type Long but found Float
[13:14:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:01] ✓ Results saved to hyperparameter_results.json
[13:14:01] 
================================================================================
[13:14:01] PROGRESS UPDATE - Completed 970 trials
[13:14:01] Best so far: 0.0000
[13:14:01] ================================================================================

[13:14:01] ================================================================================
[13:14:01] TRIAL 971 - Starting
[13:14:01] ================================================================================
[13:14:01] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.3
}
[13:14:01] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.3}
[13:14:01] ❌ Trial 971 failed with error: expected scalar type Long but found Float
[13:14:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:01] ✓ Results saved to hyperparameter_results.json
[13:14:01] ================================================================================
[13:14:01] TRIAL 972 - Starting
[13:14:01] ================================================================================
[13:14:01] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:14:01] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:14:01] ❌ Trial 972 failed with error: expected scalar type Long but found Float
[13:14:01] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:01] ✓ Results saved to hyperparameter_results.json
[13:14:01] ================================================================================
[13:14:01] TRIAL 973 - Starting
[13:14:01] ================================================================================
[13:14:01] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:14:01] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:14:02] ❌ Trial 973 failed with error: expected scalar type Long but found Float
[13:14:02] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:02] ✓ Results saved to hyperparameter_results.json
[13:14:02] ================================================================================
[13:14:02] TRIAL 974 - Starting
[13:14:02] ================================================================================
[13:14:02] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:14:02] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:14:02] ❌ Trial 974 failed with error: expected scalar type Long but found Float
[13:14:02] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:02] ✓ Results saved to hyperparameter_results.json
[13:14:02] ================================================================================
[13:14:02] TRIAL 975 - Starting
[13:14:02] ================================================================================
[13:14:02] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:14:02] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:14:02] ❌ Trial 975 failed with error: expected scalar type Long but found Float
[13:14:02] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:02] ✓ Results saved to hyperparameter_results.json
[13:14:02] 
================================================================================
[13:14:02] PROGRESS UPDATE - Completed 975 trials
[13:14:02] Best so far: 0.0000
[13:14:02] ================================================================================

[13:14:02] ================================================================================
[13:14:02] TRIAL 976 - Starting
[13:14:02] ================================================================================
[13:14:02] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:14:02] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:14:02] ❌ Trial 976 failed with error: expected scalar type Long but found Float
[13:14:02] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:02] ✓ Results saved to hyperparameter_results.json
[13:14:02] ================================================================================
[13:14:02] TRIAL 977 - Starting
[13:14:02] ================================================================================
[13:14:02] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:14:02] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:14:02] ❌ Trial 977 failed with error: expected scalar type Long but found Float
[13:14:02] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:02] ✓ Results saved to hyperparameter_results.json
[13:14:02] ================================================================================
[13:14:02] TRIAL 978 - Starting
[13:14:02] ================================================================================
[13:14:02] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:14:02] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:14:03] ❌ Trial 978 failed with error: expected scalar type Long but found Float
[13:14:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:03] ✓ Results saved to hyperparameter_results.json
[13:14:03] ================================================================================
[13:14:03] TRIAL 979 - Starting
[13:14:03] ================================================================================
[13:14:03] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.5,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:14:03] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.5, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:14:03] ❌ Trial 979 failed with error: expected scalar type Long but found Float
[13:14:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:03] ✓ Results saved to hyperparameter_results.json
[13:14:03] ================================================================================
[13:14:03] TRIAL 980 - Starting
[13:14:03] ================================================================================
[13:14:03] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 1e-06,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:14:03] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 1e-06, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:14:03] ❌ Trial 980 failed with error: expected scalar type Long but found Float
[13:14:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:03] ✓ Results saved to hyperparameter_results.json
[13:14:03] 
================================================================================
[13:14:03] PROGRESS UPDATE - Completed 980 trials
[13:14:03] Best so far: 0.0000
[13:14:03] ================================================================================

[13:14:03] ================================================================================
[13:14:03] TRIAL 981 - Starting
[13:14:03] ================================================================================
[13:14:03] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:14:03] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:14:03] ❌ Trial 981 failed with error: expected scalar type Long but found Float
[13:14:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:03] ✓ Results saved to hyperparameter_results.json
[13:14:03] ================================================================================
[13:14:03] TRIAL 982 - Starting
[13:14:03] ================================================================================
[13:14:03] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:14:03] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:14:03] ❌ Trial 982 failed with error: expected scalar type Long but found Float
[13:14:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:03] ✓ Results saved to hyperparameter_results.json
[13:14:03] ================================================================================
[13:14:03] TRIAL 983 - Starting
[13:14:03] ================================================================================
[13:14:03] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 4,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "step",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:14:03] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 4, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'step', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:14:03] ❌ Trial 983 failed with error: expected scalar type Long but found Float
[13:14:03] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:04] ✓ Results saved to hyperparameter_results.json
[13:14:04] ================================================================================
[13:14:04] TRIAL 984 - Starting
[13:14:04] ================================================================================
[13:14:04] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.5,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:14:04] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.5, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:14:04] ❌ Trial 984 failed with error: expected scalar type Long but found Float
[13:14:04] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:04] ✓ Results saved to hyperparameter_results.json
[13:14:04] ================================================================================
[13:14:04] TRIAL 985 - Starting
[13:14:04] ================================================================================
[13:14:04] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:14:04] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:14:04] ❌ Trial 985 failed with error: expected scalar type Long but found Float
[13:14:04] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:04] ✓ Results saved to hyperparameter_results.json
[13:14:04] 
================================================================================
[13:14:04] PROGRESS UPDATE - Completed 985 trials
[13:14:04] Best so far: 0.0000
[13:14:04] ================================================================================

[13:14:04] ================================================================================
[13:14:04] TRIAL 986 - Starting
[13:14:04] ================================================================================
[13:14:04] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:14:04] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:14:04] ❌ Trial 986 failed with error: expected scalar type Long but found Float
[13:14:04] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:04] ✓ Results saved to hyperparameter_results.json
[13:14:04] ================================================================================
[13:14:04] TRIAL 987 - Starting
[13:14:04] ================================================================================
[13:14:04] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:14:04] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:14:04] ❌ Trial 987 failed with error: expected scalar type Long but found Float
[13:14:04] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:04] ✓ Results saved to hyperparameter_results.json
[13:14:04] ================================================================================
[13:14:04] TRIAL 988 - Starting
[13:14:04] ================================================================================
[13:14:04] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 2.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:14:04] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 2.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:14:04] ❌ Trial 988 failed with error: expected scalar type Long but found Float
[13:14:04] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:04] ✓ Results saved to hyperparameter_results.json
[13:14:04] ================================================================================
[13:14:04] TRIAL 989 - Starting
[13:14:04] ================================================================================
[13:14:04] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 5.0,
  "dropout": 0.1,
  "weight_decay": 5e-05,
  "hidden_dim": 192,
  "num_layers": 2,
  "batch_size": 8,
  "optimizer": "adam",
  "class_weight_scale": 0.5,
  "scheduler": "plateau",
  "scheduler_patience": 5,
  "scheduler_factor": 0.5
}
[13:14:04] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 5.0, 'dropout': 0.1, 'weight_decay': 5e-05, 'hidden_dim': 192, 'num_layers': 2, 'batch_size': 8, 'optimizer': 'adam', 'class_weight_scale': 0.5, 'scheduler': 'plateau', 'scheduler_patience': 5, 'scheduler_factor': 0.5}
[13:14:04] ❌ Trial 989 failed with error: expected scalar type Long but found Float
[13:14:04] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:05] ✓ Results saved to hyperparameter_results.json
[13:14:05] ================================================================================
[13:14:05] TRIAL 990 - Starting
[13:14:05] ================================================================================
[13:14:05] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:14:05] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:14:05] ❌ Trial 990 failed with error: expected scalar type Long but found Float
[13:14:05] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:05] ✓ Results saved to hyperparameter_results.json
[13:14:05] 
================================================================================
[13:14:05] PROGRESS UPDATE - Completed 990 trials
[13:14:05] Best so far: 0.0000
[13:14:05] ================================================================================

[13:14:05] ================================================================================
[13:14:05] TRIAL 991 - Starting
[13:14:05] ================================================================================
[13:14:05] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 1e-06,
  "hidden_dim": 128,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:14:05] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 1e-06, 'hidden_dim': 128, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:14:05] ❌ Trial 991 failed with error: expected scalar type Long but found Float
[13:14:05] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:05] ✓ Results saved to hyperparameter_results.json
[13:14:05] ================================================================================
[13:14:05] TRIAL 992 - Starting
[13:14:05] ================================================================================
[13:14:05] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "none",
  "scheduler_patience": 5,
  "scheduler_factor": 0.7
}
[13:14:05] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'none', 'scheduler_patience': 5, 'scheduler_factor': 0.7}
[13:14:05] ❌ Trial 992 failed with error: expected scalar type Long but found Float
[13:14:05] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:05] ✓ Results saved to hyperparameter_results.json
[13:14:05] ================================================================================
[13:14:05] TRIAL 993 - Starting
[13:14:05] ================================================================================
[13:14:05] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0,
  "hidden_dim": 192,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:14:05] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0, 'hidden_dim': 192, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:14:05] ❌ Trial 993 failed with error: expected scalar type Long but found Float
[13:14:05] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:05] ✓ Results saved to hyperparameter_results.json
[13:14:05] ================================================================================
[13:14:05] TRIAL 994 - Starting
[13:14:05] ================================================================================
[13:14:05] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 0.0001,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 8,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.7
}
[13:14:05] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 0.0001, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 8, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.7}
[13:14:05] ❌ Trial 994 failed with error: expected scalar type Long but found Float
[13:14:05] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:05] ✓ Results saved to hyperparameter_results.json
[13:14:05] ================================================================================
[13:14:05] TRIAL 995 - Starting
[13:14:05] ================================================================================
[13:14:05] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 5,
  "batch_size": 16,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:14:06] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 5, 'batch_size': 16, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:14:06] ❌ Trial 995 failed with error: expected scalar type Long but found Float
[13:14:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:06] ✓ Results saved to hyperparameter_results.json
[13:14:06] 
================================================================================
[13:14:06] PROGRESS UPDATE - Completed 995 trials
[13:14:06] Best so far: 0.0000
[13:14:06] ================================================================================

[13:14:06] ================================================================================
[13:14:06] TRIAL 996 - Starting
[13:14:06] ================================================================================
[13:14:06] Configuration: {
  "learning_rate": 1e-05,
  "gradient_clip": 1.0,
  "dropout": 0.1,
  "weight_decay": 0.0001,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:14:06] ✓ Model created with config: {'learning_rate': 1e-05, 'gradient_clip': 1.0, 'dropout': 0.1, 'weight_decay': 0.0001, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:14:06] ❌ Trial 996 failed with error: expected scalar type Long but found Float
[13:14:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:06] ✓ Results saved to hyperparameter_results.json
[13:14:06] ================================================================================
[13:14:06] TRIAL 997 - Starting
[13:14:06] ================================================================================
[13:14:06] Configuration: {
  "learning_rate": 0.0001,
  "gradient_clip": 5.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 1.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.7
}
[13:14:06] ✓ Model created with config: {'learning_rate': 0.0001, 'gradient_clip': 5.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 1.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.7}
[13:14:06] ❌ Trial 997 failed with error: expected scalar type Long but found Float
[13:14:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:06] ✓ Results saved to hyperparameter_results.json
[13:14:06] ================================================================================
[13:14:06] TRIAL 998 - Starting
[13:14:06] ================================================================================
[13:14:06] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 1.0,
  "dropout": 0.4,
  "weight_decay": 1e-05,
  "hidden_dim": 192,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.0,
  "scheduler": "plateau",
  "scheduler_patience": 3,
  "scheduler_factor": 0.5
}
[13:14:06] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 1.0, 'dropout': 0.4, 'weight_decay': 1e-05, 'hidden_dim': 192, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.0, 'scheduler': 'plateau', 'scheduler_patience': 3, 'scheduler_factor': 0.5}
[13:14:06] ❌ Trial 998 failed with error: expected scalar type Long but found Float
[13:14:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:06] ✓ Results saved to hyperparameter_results.json
[13:14:06] ================================================================================
[13:14:06] TRIAL 999 - Starting
[13:14:06] ================================================================================
[13:14:06] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 5.0,
  "dropout": 0.3,
  "weight_decay": 0.0001,
  "hidden_dim": 64,
  "num_layers": 2,
  "batch_size": 4,
  "optimizer": "adamw",
  "class_weight_scale": 1.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:14:06] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 5.0, 'dropout': 0.3, 'weight_decay': 0.0001, 'hidden_dim': 64, 'num_layers': 2, 'batch_size': 4, 'optimizer': 'adamw', 'class_weight_scale': 1.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:14:06] ❌ Trial 999 failed with error: expected scalar type Long but found Float
[13:14:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:06] ✓ Results saved to hyperparameter_results.json
[13:14:06] ================================================================================
[13:14:06] TRIAL 1000 - Starting
[13:14:06] ================================================================================
[13:14:06] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.2,
  "weight_decay": 0.0001,
  "hidden_dim": 192,
  "num_layers": 5,
  "batch_size": 32,
  "optimizer": "adam",
  "class_weight_scale": 1.5,
  "scheduler": "plateau",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:14:06] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.2, 'weight_decay': 0.0001, 'hidden_dim': 192, 'num_layers': 5, 'batch_size': 32, 'optimizer': 'adam', 'class_weight_scale': 1.5, 'scheduler': 'plateau', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:14:06] ❌ Trial 1000 failed with error: expected scalar type Long but found Float
[13:14:06] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:07] ✓ Results saved to hyperparameter_results.json
[13:14:07] 
================================================================================
[13:14:07] PROGRESS UPDATE - Completed 1000 trials
[13:14:07] Best so far: 0.0000
[13:14:07] ================================================================================

[13:14:07] ================================================================================
[13:14:07] TRIAL 1001 - Starting
[13:14:07] ================================================================================
[13:14:07] Configuration: {
  "learning_rate": 5e-05,
  "gradient_clip": 0.5,
  "dropout": 0.5,
  "weight_decay": 5e-05,
  "hidden_dim": 64,
  "num_layers": 4,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "none",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:14:07] ✓ Model created with config: {'learning_rate': 5e-05, 'gradient_clip': 0.5, 'dropout': 0.5, 'weight_decay': 5e-05, 'hidden_dim': 64, 'num_layers': 4, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'none', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:14:07] ❌ Trial 1001 failed with error: expected scalar type Long but found Float
[13:14:07] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:07] ✓ Results saved to hyperparameter_results.json
[13:14:07] ================================================================================
[13:14:07] TRIAL 1002 - Starting
[13:14:07] ================================================================================
[13:14:07] Configuration: {
  "learning_rate": 0.0002,
  "gradient_clip": 2.0,
  "dropout": 0.3,
  "weight_decay": 1e-05,
  "hidden_dim": 256,
  "num_layers": 3,
  "batch_size": 32,
  "optimizer": "adamw",
  "class_weight_scale": 2.0,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.5
}
[13:14:07] ✓ Model created with config: {'learning_rate': 0.0002, 'gradient_clip': 2.0, 'dropout': 0.3, 'weight_decay': 1e-05, 'hidden_dim': 256, 'num_layers': 3, 'batch_size': 32, 'optimizer': 'adamw', 'class_weight_scale': 2.0, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.5}
[13:14:07] ❌ Trial 1002 failed with error: expected scalar type Long but found Float
[13:14:07] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:07] ✓ Results saved to hyperparameter_results.json
[13:14:07] ================================================================================
[13:14:07] TRIAL 1003 - Starting
[13:14:07] ================================================================================
[13:14:07] Configuration: {
  "learning_rate": 0.0005,
  "gradient_clip": 1.0,
  "dropout": 0.6,
  "weight_decay": 0,
  "hidden_dim": 128,
  "num_layers": 3,
  "batch_size": 16,
  "optimizer": "adamw",
  "class_weight_scale": 0.5,
  "scheduler": "step",
  "scheduler_patience": 7,
  "scheduler_factor": 0.3
}
[13:14:07] ✓ Model created with config: {'learning_rate': 0.0005, 'gradient_clip': 1.0, 'dropout': 0.6, 'weight_decay': 0, 'hidden_dim': 128, 'num_layers': 3, 'batch_size': 16, 'optimizer': 'adamw', 'class_weight_scale': 0.5, 'scheduler': 'step', 'scheduler_patience': 7, 'scheduler_factor': 0.3}
[13:14:07] ❌ Trial 1003 failed with error: expected scalar type Long but found Float
[13:14:07] Traceback (most recent call last):
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 500, in train_with_config
    train_metrics = self.train_epoch(model, optimizer, criterion, config)
  File "C:\Devign\devign\auto_hyperparameter_comprehensive.py", line 335, in train_epoch
    loss = criterion(outputs, targets)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\modules\loss.py", line 1310, in forward
    return F.cross_entropy(
  File "C:\Devign\devign\venv\lib\site-packages\torch\nn\functional.py", line 3462, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
RuntimeError: expected scalar type Long but found Float

[13:14:07] 

================================================================================
