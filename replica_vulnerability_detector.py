#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Replica Vulnerability Line Detection System
==========================================

This module provides line-level vulnerability detection capabilities for the replica testing system.
It identifies specific vulnerable lines in code samples and provides detailed analysis.
"""

import json
import re
import ast
import tokenize
from io import StringIO
from typing import Dict, List, Tuple, Any, Optional
import pandas as pd
import numpy as np
import torch
import torch.nn.functional as F
from pathlib import Path


class VulnerabilityLineDetector:
    """Detects vulnerable lines in code using attention mechanisms and pattern analysis"""
    
    def __init__(self):
        self.vulnerability_patterns = {
            'c': [
                (r'strcpy\s*\(', 'Buffer overflow - unsafe string copy'),
                (r'strcat\s*\(', 'Buffer overflow - unsafe string concatenation'),
                (r'sprintf\s*\(', 'Buffer overflow - unsafe string formatting'),
                (r'gets\s*\(', 'Buffer overflow - unsafe input reading'),
                (r'scanf\s*\([^,]*,\s*[^&]', 'Format string vulnerability'),
            ],
            'cpp': [
                (r'strcpy\s*\(', 'Buffer overflow - unsafe string copy'),
                (r'strcat\s*\(', 'Buffer overflow - unsafe string concatenation'),
                (r'new\s+\w+\[.*\](?!\s*\{)', 'Memory leak - unmanaged array allocation'),
                (r'delete\s+\w+(?!\[\])', 'Memory management - potential double delete'),
            ],
            'python': [
                (r'os\.system\s*\(', 'Command injection - unsafe system call'),
                (r'subprocess\.call\s*\([^,]*shell\s*=\s*True', 'Command injection - shell=True'),
                (r'eval\s*\(', 'Code injection - unsafe eval'),
                (r'exec\s*\(', 'Code injection - unsafe exec'),
                (r'pickle\.loads\s*\(', 'Deserialization vulnerability'),
            ],
            'java': [
                (r'Statement\s+\w+\s*=.*\.createStatement\s*\(\)', 'SQL injection - unsafe statement'),
                (r'\".*\"\s*\+.*\+.*\".*\"', 'SQL injection - string concatenation in query'),
                (r'Runtime\.getRuntime\(\)\.exec\s*\(', 'Command injection - Runtime.exec'),
                (r'Class\.forName\s*\(', 'Reflection vulnerability'),
            ],
            'csharp': [
                (r'SqlCommand\s*\([^,]*\+', 'SQL injection - string concatenation in SQL'),
                (r'Process\.Start\s*\(', 'Command injection - Process.Start'),
                (r'Assembly\.LoadFrom\s*\(', 'Code injection - unsafe assembly loading'),
            ],
            'php': [
                (r'mysql_query\s*\([^,]*\$', 'SQL injection - unsafe mysql_query'),
                (r'mysqli_query\s*\([^,]*\$', 'SQL injection - unsafe mysqli_query'),
                (r'eval\s*\(', 'Code injection - unsafe eval'),
                (r'system\s*\(', 'Command injection - system call'),
                (r'exec\s*\(', 'Command injection - exec call'),
            ]
        }
    
    def detect_vulnerable_lines(self, code: str, language: str, prediction_confidence: float) -> Dict[str, Any]:
        """
        Detect vulnerable lines in code
        
        Args:
            code: Source code string
            language: Programming language
            prediction_confidence: Model's confidence in vulnerability prediction
            
        Returns:
            Dictionary with vulnerability analysis
        """
        lines = code.split('\n')
        vulnerable_lines = []
        
        # Pattern-based detection
        patterns = self.vulnerability_patterns.get(language.lower(), [])
        
        for line_num, line in enumerate(lines, 1):
            line_stripped = line.strip()
            if not line_stripped or line_stripped.startswith(('//','#', '/*', '*')):
                continue
            
            for pattern, description in patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    vulnerable_lines.append({
                        'line_number': line_num,
                        'line_content': line.strip(),
                        'vulnerability_type': description,
                        'confidence': min(0.9, prediction_confidence + 0.1),
                        'pattern_matched': pattern
                    })
        
        # Additional heuristic analysis
        additional_vulns = self._heuristic_analysis(lines, language)
        vulnerable_lines.extend(additional_vulns)
        
        # Remove duplicates and sort by line number
        seen_lines = set()
        unique_vulns = []
        for vuln in vulnerable_lines:
            if vuln['line_number'] not in seen_lines:
                unique_vulns.append(vuln)
                seen_lines.add(vuln['line_number'])
        
        unique_vulns.sort(key=lambda x: x['line_number'])
        
        return {
            'total_lines': len(lines),
            'vulnerable_lines_count': len(unique_vulns),
            'vulnerable_lines': unique_vulns,
            'overall_risk_score': self._calculate_risk_score(unique_vulns, prediction_confidence),
            'language': language,
            'analysis_method': 'pattern_matching + heuristics'
        }
    
    def _heuristic_analysis(self, lines: List[str], language: str) -> List[Dict[str, Any]]:
        """Additional heuristic-based vulnerability detection"""
        vulnerabilities = []
        
        if language.lower() in ['c', 'cpp']:
            vulnerabilities.extend(self._analyze_c_cpp_heuristics(lines))
        elif language.lower() == 'python':
            vulnerabilities.extend(self._analyze_python_heuristics(lines))
        elif language.lower() == 'java':
            vulnerabilities.extend(self._analyze_java_heuristics(lines))
        elif language.lower() == 'csharp':
            vulnerabilities.extend(self._analyze_csharp_heuristics(lines))
        elif language.lower() == 'php':
            vulnerabilities.extend(self._analyze_php_heuristics(lines))
        
        return vulnerabilities
    
    def _analyze_c_cpp_heuristics(self, lines: List[str]) -> List[Dict[str, Any]]:
        """C/C++ specific heuristic analysis"""
        vulnerabilities = []
        
        for line_num, line in enumerate(lines, 1):
            line_stripped = line.strip()
            
            # Buffer overflow patterns
            if re.search(r'char\s+\w+\[\d+\]', line_stripped):
                if any(func in line_stripped for func in ['strcpy', 'strcat', 'sprintf']):
                    vulnerabilities.append({
                        'line_number': line_num,
                        'line_content': line.strip(),
                        'vulnerability_type': 'Buffer overflow - fixed size buffer with unsafe function',
                        'confidence': 0.85,
                        'pattern_matched': 'heuristic_buffer_overflow'
                    })
            
            # Memory management issues
            if 'malloc' in line_stripped and 'free' not in ''.join(lines[line_num:line_num+10]):
                vulnerabilities.append({
                    'line_number': line_num,
                    'line_content': line.strip(),
                    'vulnerability_type': 'Memory leak - malloc without corresponding free',
                    'confidence': 0.75,
                    'pattern_matched': 'heuristic_memory_leak'
                })
            
            # Integer overflow
            if re.search(r'\w+\s*\*\s*\w+\s*\*', line_stripped):
                vulnerabilities.append({
                    'line_number': line_num,
                    'line_content': line.strip(),
                    'vulnerability_type': 'Integer overflow - multiplication without bounds check',
                    'confidence': 0.65,
                    'pattern_matched': 'heuristic_integer_overflow'
                })
        
        return vulnerabilities
    
    def _analyze_python_heuristics(self, lines: List[str]) -> List[Dict[str, Any]]:
        """Python specific heuristic analysis"""
        vulnerabilities = []
        
        for line_num, line in enumerate(lines, 1):
            line_stripped = line.strip()
            
            # SQL injection via string formatting
            if re.search(r'["\'].*%s.*["\'].*%', line_stripped):
                vulnerabilities.append({
                    'line_number': line_num,
                    'line_content': line.strip(),
                    'vulnerability_type': 'SQL injection - string formatting in query',
                    'confidence': 0.80,
                    'pattern_matched': 'heuristic_sql_injection'
                })
            
            # Path traversal
            if re.search(r'open\s*\([^,]*\+', line_stripped):
                vulnerabilities.append({
                    'line_number': line_num,
                    'line_content': line.strip(),
                    'vulnerability_type': 'Path traversal - file path concatenation',
                    'confidence': 0.70,
                    'pattern_matched': 'heuristic_path_traversal'
                })
        
        return vulnerabilities
    
    def _analyze_java_heuristics(self, lines: List[str]) -> List[Dict[str, Any]]:
        """Java specific heuristic analysis"""
        vulnerabilities = []
        
        for line_num, line in enumerate(lines, 1):
            line_stripped = line.strip()
            
            # SQL injection via concatenation
            if re.search(r'["\']SELECT.*["\'].*\+.*\+.*["\']', line_stripped):
                vulnerabilities.append({
                    'line_number': line_num,
                    'line_content': line.strip(),
                    'vulnerability_type': 'SQL injection - query string concatenation',
                    'confidence': 0.85,
                    'pattern_matched': 'heuristic_sql_injection'
                })
            
            # Deserialization
            if 'ObjectInputStream' in line_stripped and 'readObject' in line_stripped:
                vulnerabilities.append({
                    'line_number': line_num,
                    'line_content': line.strip(),
                    'vulnerability_type': 'Deserialization vulnerability - unsafe object reading',
                    'confidence': 0.75,
                    'pattern_matched': 'heuristic_deserialization'
                })
        
        return vulnerabilities
    
    def _analyze_csharp_heuristics(self, lines: List[str]) -> List[Dict[str, Any]]:
        """C# specific heuristic analysis"""
        vulnerabilities = []
        
        for line_num, line in enumerate(lines, 1):
            line_stripped = line.strip()
            
            # SQL injection
            if re.search(r'SqlCommand.*\+.*\+', line_stripped):
                vulnerabilities.append({
                    'line_number': line_num,
                    'line_content': line.strip(),
                    'vulnerability_type': 'SQL injection - SqlCommand with concatenation',
                    'confidence': 0.85,
                    'pattern_matched': 'heuristic_sql_injection'
                })
        
        return vulnerabilities
    
    def _analyze_php_heuristics(self, lines: List[str]) -> List[Dict[str, Any]]:
        """PHP specific heuristic analysis"""
        vulnerabilities = []
        
        for line_num, line in enumerate(lines, 1):
            line_stripped = line.strip()
            
            # SQL injection via concatenation
            if re.search(r'["\']SELECT.*["\'].*\..*\$', line_stripped):
                vulnerabilities.append({
                    'line_number': line_num,
                    'line_content': line.strip(),
                    'vulnerability_type': 'SQL injection - query concatenation with variables',
                    'confidence': 0.80,
                    'pattern_matched': 'heuristic_sql_injection'
                })
            
            # File inclusion
            if re.search(r'include\s*\(\s*\$', line_stripped):
                vulnerabilities.append({
                    'line_number': line_num,
                    'line_content': line.strip(),
                    'vulnerability_type': 'File inclusion - dynamic include with variable',
                    'confidence': 0.75,
                    'pattern_matched': 'heuristic_file_inclusion'
                })
        
        return vulnerabilities
    
    def _calculate_risk_score(self, vulnerabilities: List[Dict[str, Any]], base_confidence: float) -> float:
        """Calculate overall risk score based on vulnerabilities found"""
        if not vulnerabilities:
            return 0.0
        
        # Weight by number of vulnerabilities and their confidence
        total_score = 0.0
        for vuln in vulnerabilities:
            confidence = vuln.get('confidence', 0.5)
            # Higher weight for high-confidence vulnerabilities
            weight = 1.0 if confidence > 0.8 else 0.7 if confidence > 0.6 else 0.5
            total_score += confidence * weight
        
        # Normalize by number of vulnerabilities and combine with base confidence
        avg_score = total_score / len(vulnerabilities)
        final_score = (avg_score * 0.7) + (base_confidence * 0.3)
        
        return min(1.0, final_score)


class ReplicaVulnerabilityAnalyzer:
    """Main analyzer class that integrates model predictions with line-level detection"""
    
    def __init__(self, model_path: str = None):
        self.line_detector = VulnerabilityLineDetector()
        self.model = None
        self.model_path = model_path
        
    def load_model(self, model_path: str):
        """Load the trained vulnerability detection model"""
        try:
            import torch
            from src.process.model import create_devign_model
            
            # Create model with same architecture as training
            self.model = create_devign_model(
                input_dim=205,
                output_dim=2,
                model_type='full',
                hidden_dim=200,
                num_steps=8,
                conv1d_channels=[200, 200],
                dropout=0.3
            )
            
            # Load trained weights
            if Path(model_path).exists():
                self.model.load_state_dict(torch.load(model_path, map_location='cpu'))
                self.model.eval()
                print(f"✓ Loaded model from {model_path}")
                return True
            else:
                print(f"⚠️ Model file not found: {model_path}")
                return False
                
        except Exception as e:
            print(f"❌ Error loading model: {e}")
            return False
    
    def analyze_code_sample(self, code_sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analyze a single code sample for vulnerabilities
        
        Args:
            code_sample: Dictionary with code sample data (matching your format)
            
        Returns:
            Dictionary with comprehensive vulnerability analysis
        """
        # Extract information from the sample
        code_snippet = code_sample.get('Code Snippet', '')
        language = code_sample.get('Primary Language of Benchmark', 'unknown').lower()
        cwe_info = code_sample.get('CWE ID', '')
        
        # Clean up code snippet (remove language prefix if present)
        if code_snippet.startswith(language):
            code_snippet = code_snippet[len(language):].strip()
        if code_snippet.startswith('```'):
            lines = code_snippet.split('\n')
            code_snippet = '\n'.join(lines[1:-1]) if len(lines) > 2 else code_snippet
        
        # Get model prediction if model is loaded
        model_confidence = 0.5  # Default confidence
        model_prediction = 0    # Default to non-vulnerable
        
        if self.model is not None:
            try:
                # This would require processing the code through your pipeline
                # For now, we'll use a simplified approach
                model_confidence = self._get_model_prediction(code_snippet, language)
                model_prediction = 1 if model_confidence > 0.5 else 0
            except Exception as e:
                print(f"⚠️ Model prediction failed: {e}")
        
        # Perform line-level analysis
        line_analysis = self.line_detector.detect_vulnerable_lines(
            code_snippet, language, model_confidence
        )
        
        # Combine results
        analysis_result = {
            'sample_info': {
                'sno': code_sample.get('Sno', 'unknown'),
                'language': language,
                'cwe_id': cwe_info,
                'original_vulnerability': code_sample.get('Vulnerability', 0)
            },
            'model_prediction': {
                'is_vulnerable': model_prediction,
                'confidence': model_confidence,
                'prediction_method': 'model' if self.model else 'heuristic'
            },
            'line_analysis': line_analysis,
            'summary': {
                'total_vulnerabilities_found': line_analysis['vulnerable_lines_count'],
                'risk_score': line_analysis['overall_risk_score'],
                'most_critical_line': self._get_most_critical_line(line_analysis['vulnerable_lines']),
                'vulnerability_types': list(set([v['vulnerability_type'] for v in line_analysis['vulnerable_lines']]))
            }
        }
        
        return analysis_result
    
    def _get_model_prediction(self, code: str, language: str) -> float:
        """Get model prediction confidence (simplified version)"""
        # This is a placeholder - in reality, you'd need to:
        # 1. Process code through your CPG pipeline
        # 2. Generate embeddings
        # 3. Run through the model
        # For now, return a heuristic-based confidence
        
        patterns_found = 0
        total_patterns = len(self.line_detector.vulnerability_patterns.get(language, []))
        
        if total_patterns > 0:
            for pattern, _ in self.line_detector.vulnerability_patterns.get(language, []):
                if re.search(pattern, code, re.IGNORECASE):
                    patterns_found += 1
            
            return min(0.95, 0.3 + (patterns_found / total_patterns) * 0.6)
        
        return 0.5
    
    def _get_most_critical_line(self, vulnerabilities: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """Get the most critical vulnerability line"""
        if not vulnerabilities:
            return None
        
        # Sort by confidence and return the highest
        sorted_vulns = sorted(vulnerabilities, key=lambda x: x['confidence'], reverse=True)
        return sorted_vulns[0]
    
    def batch_analyze(self, dataset_path: str) -> Dict[str, Any]:
        """
        Analyze a batch of code samples from a dataset file
        
        Args:
            dataset_path: Path to JSON dataset file
            
        Returns:
            Dictionary with batch analysis results
        """
        try:
            with open(dataset_path, 'r', encoding='utf-8') as f:
                dataset = json.load(f)
            
            print(f"📊 Analyzing {len(dataset)} samples from {dataset_path}")
            
            results = []
            stats = {
                'total_samples': len(dataset),
                'vulnerable_detected': 0,
                'non_vulnerable_detected': 0,
                'total_vulnerable_lines': 0,
                'languages_analyzed': set(),
                'vulnerability_types_found': set()
            }
            
            for i, sample in enumerate(dataset):
                if i % 10 == 0:
                    print(f"  Processing sample {i+1}/{len(dataset)}")
                
                analysis = self.analyze_code_sample(sample)
                results.append(analysis)
                
                # Update statistics
                if analysis['model_prediction']['is_vulnerable']:
                    stats['vulnerable_detected'] += 1
                else:
                    stats['non_vulnerable_detected'] += 1
                
                stats['total_vulnerable_lines'] += analysis['line_analysis']['vulnerable_lines_count']
                stats['languages_analyzed'].add(analysis['sample_info']['language'])
                stats['vulnerability_types_found'].update(analysis['summary']['vulnerability_types'])
            
            # Convert sets to lists for JSON serialization
            stats['languages_analyzed'] = list(stats['languages_analyzed'])
            stats['vulnerability_types_found'] = list(stats['vulnerability_types_found'])
            
            return {
                'analysis_results': results,
                'statistics': stats,
                'dataset_path': dataset_path
            }
            
        except Exception as e:
            print(f"❌ Error in batch analysis: {e}")
            return {'error': str(e)}
    
    def generate_report(self, analysis_results: Dict[str, Any], output_path: str = None) -> str:
        """Generate a detailed analysis report"""
        if 'error' in analysis_results:
            return f"Analysis failed: {analysis_results['error']}"
        
        stats = analysis_results['statistics']
        results = analysis_results['analysis_results']
        
        report = []
        report.append("=" * 80)
        report.append("REPLICA VULNERABILITY ANALYSIS REPORT")
        report.append("=" * 80)
        report.append(f"Dataset: {analysis_results['dataset_path']}")
        report.append(f"Total Samples: {stats['total_samples']}")
        report.append(f"Languages: {', '.join(stats['languages_analyzed'])}")
        report.append("")
        
        report.append("DETECTION SUMMARY:")
        report.append(f"  Vulnerable samples detected: {stats['vulnerable_detected']}")
        report.append(f"  Non-vulnerable samples: {stats['non_vulnerable_detected']}")
        report.append(f"  Total vulnerable lines found: {stats['total_vulnerable_lines']}")
        report.append(f"  Detection rate: {stats['vulnerable_detected']/stats['total_samples']:.1%}")
        report.append("")
        
        report.append("VULNERABILITY TYPES FOUND:")
        for vuln_type in sorted(stats['vulnerability_types_found']):
            count = sum(1 for r in results for v in r['line_analysis']['vulnerable_lines'] 
                       if v['vulnerability_type'] == vuln_type)
            report.append(f"  - {vuln_type}: {count} instances")
        report.append("")
        
        report.append("TOP 10 MOST CRITICAL FINDINGS:")
        # Get top critical findings
        all_findings = []
        for result in results:
            if result['summary']['most_critical_line']:
                finding = result['summary']['most_critical_line'].copy()
                finding['sample_sno'] = result['sample_info']['sno']
                finding['language'] = result['sample_info']['language']
                all_findings.append(finding)
        
        top_findings = sorted(all_findings, key=lambda x: x['confidence'], reverse=True)[:10]
        for i, finding in enumerate(top_findings, 1):
            report.append(f"  {i}. Sample {finding['sample_sno']} ({finding['language']}) - Line {finding['line_number']}")
            report.append(f"     {finding['vulnerability_type']} (confidence: {finding['confidence']:.2f})")
            report.append(f"     Code: {finding['line_content'][:60]}...")
            report.append("")
        
        report_text = '\n'.join(report)
        
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report_text)
            print(f"📄 Report saved to: {output_path}")
        
        return report_text


def main():
    """Main function for testing the vulnerability analyzer"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Replica Vulnerability Analyzer')
    parser.add_argument('--dataset', required=True, help='Path to dataset JSON file')
    parser.add_argument('--model', help='Path to trained model file')
    parser.add_argument('--output', help='Output path for analysis report')
    parser.add_argument('--sample', type=int, help='Analyze only first N samples')
    
    args = parser.parse_args()
    
    # Initialize analyzer
    analyzer = ReplicaVulnerabilityAnalyzer()
    
    # Load model if provided
    if args.model:
        analyzer.load_model(args.model)
    
    # Load and potentially sample dataset
    with open(args.dataset, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    
    if args.sample:
        dataset = dataset[:args.sample]
        print(f"📊 Analyzing first {len(dataset)} samples")
    
    # Perform analysis
    print("🔍 Starting vulnerability analysis...")
    results = analyzer.batch_analyze(args.dataset)
    
    # Generate report
    report_path = args.output or f"replica_analysis_report_{Path(args.dataset).stem}.txt"
    report = analyzer.generate_report(results, report_path)
    
    print("\n" + "="*50)
    print("ANALYSIS COMPLETE")
    print("="*50)
    print(f"Report saved to: {report_path}")
    
    # Print summary
    if 'statistics' in results:
        stats = results['statistics']
        print(f"Samples analyzed: {stats['total_samples']}")
        print(f"Vulnerabilities detected: {stats['vulnerable_detected']}")
        print(f"Vulnerable lines found: {stats['total_vulnerable_lines']}")


if __name__ == "__main__":
    main()
