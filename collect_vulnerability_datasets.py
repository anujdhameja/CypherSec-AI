#!/usr/bin/env python3
"""
Vulnerability Dataset Collection and Mapping Tool
Collects datasets from various sources and creates standardized mappings
"""

import os
import json
import requests
import zipfile
import hashlib
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging
from datetime import datetime

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class VulnerabilityDatasetCollector:
    def __init__(self, base_path: str = "C:/Devign/devign/Custom datasets"):
        self.base_path = Path(base_path)
        self.data_path = self.base_path / "data"
        self.data_path.mkdir(parents=True, exist_ok=True)
        
        # Dataset URLs and configurations
        self.dataset_configs = {
            "juliet": {
                "name": "Juliet Test Suite",
                "url": "https://samate.nist.gov/SARD/downloads/test-suites/juliet-test-suite-v1.3-for-c-cpp.zip",
                "description": "NIST SAMATE Juliet Test Suite for C/C++",
                "file_patterns": ["*.c", "*.cpp"],
                "mapping_file": "dataset_juliet_mapped.json"
            },
            "github_vuln": {
                "name": "GitHub Vulnerability Database",
                "description": "GitHub-sourced vulnerability examples",
                "mapping_file": "dataset_github_mapped.json"
            },
            "owasp": {
                "name": "OWASP WebGoat",
                "url": "https://github.com/WebGoat/WebGoat",
                "description": "OWASP WebGoat vulnerable examples",
                "mapping_file": "dataset_owasp_mapped.json"
            }
        }
        
        # CWE mappings for common vulnerabilities
        self.cwe_mappings = {
            "buffer_overflow": "CWE-119",
            "sql_injection": "CWE-89",
            "xss": "CWE-79",
            "path_traversal": "CWE-22",
            "command_injection": "CWE-78",
            "integer_overflow": "CWE-190",
            "use_after_free": "CWE-416",
            "null_pointer": "CWE-476",
            "memory_leak": "CWE-401",
            "race_condition": "CWE-362"
        }

    def download_file(self, url: str, destination: Path) -> bool:
        """Download file from URL with progress tracking"""
        try:
            logger.info(f"Downloading {url} to {destination}")
            response = requests.get(url, stream=True)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            downloaded = 0
            
            with open(destination, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        if total_size > 0:
                            progress = (downloaded / total_size) * 100
                            print(f"\rProgress: {progress:.1f}%", end='', flush=True)
            
            print()  # New line after progress
            logger.info(f"Successfully downloaded {destination}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to download {url}: {e}")
            return False

    def extract_zip(self, zip_path: Path, extract_to: Path) -> bool:
        """Extract ZIP file"""
        try:
            logger.info(f"Extracting {zip_path} to {extract_to}")
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(extract_to)
            logger.info(f"Successfully extracted to {extract_to}")
            return True
        except Exception as e:
            logger.error(f"Failed to extract {zip_path}: {e}")
            return False

    def collect_juliet_dataset(self) -> List[Dict[str, Any]]:
        """Collect and process Juliet Test Suite"""
        logger.info("Collecting Juliet Test Suite...")
        
        juliet_dir = self.data_path / "juliet"
        juliet_dir.mkdir(exist_ok=True)
        
        zip_path = juliet_dir / "juliet-test-suite.zip"
        
        # Download if not exists
        if not zip_path.exists():
            config = self.dataset_configs["juliet"]
            if not self.download_file(config["url"], zip_path):
                logger.error("Failed to download Juliet Test Suite")
                return []
        
        # Extract if not already extracted
        extract_dir = juliet_dir / "extracted"
        if not extract_dir.exists():
            if not self.extract_zip(zip_path, extract_dir):
                logger.error("Failed to extract Juliet Test Suite")
                return []
        
        # Process extracted files
        mapped_data = []
        c_files = list(extract_dir.rglob("*.c")) + list(extract_dir.rglob("*.cpp"))
        
        logger.info(f"Processing {len(c_files)} C/C++ files from Juliet...")
        
        for i, file_path in enumerate(c_files[:100]):  # Limit for initial processing
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    code_content = f.read()
                
                # Extract vulnerability info from filename/path
                file_name = file_path.name
                is_vulnerable = "bad" in file_name.lower() or "vuln" in file_name.lower()
                
                # Determine CWE from path
                cwe_id = self.extract_cwe_from_path(str(file_path))
                
                mapped_entry = {
                    "id": f"juliet_{hashlib.md5(str(file_path).encode()).hexdigest()[:8]}",
                    "source": "juliet",
                    "file": str(file_path.relative_to(extract_dir)),
                    "function": self.extract_function_name(code_content),
                    "code": code_content[:2000],  # Limit code length
                    "is_vulnerable": is_vulnerable,
                    "cwe_id": cwe_id,
                    "cve_ids": [],
                    "severity": "MEDIUM" if is_vulnerable else "LOW"
                }
                
                mapped_data.append(mapped_entry)
                
                if (i + 1) % 10 == 0:
                    logger.info(f"Processed {i + 1}/{len(c_files)} files")
                    
            except Exception as e:
                logger.warning(f"Failed to process {file_path}: {e}")
                continue
        
        logger.info(f"Collected {len(mapped_data)} entries from Juliet Test Suite")
        return mapped_data

    def collect_github_vulnerability_db(self) -> List[Dict[str, Any]]:
        """Collect GitHub vulnerability examples"""
        logger.info("Collecting GitHub vulnerability examples...")
        
        # GitHub search queries for vulnerable code
        search_queries = [
            "cwe-119 vulnerable language:c",
            "cwe-89 sql injection language:c",
            "buffer overflow vulnerable language:c",
            "use after free vulnerable language:c"
        ]
        
        mapped_data = []
        
        # Note: This is a simplified implementation
        # In practice, you'd use GitHub API to search for repositories
        # For now, we'll create some example entries
        
        example_entries = [
            {
                "id": "github_001",
                "source": "github",
                "file": "examples/buffer_overflow.c",
                "function": "vulnerable_strcpy",
                "code": """
void vulnerable_strcpy(char *input) {
    char buffer[100];
    strcpy(buffer, input);  // Vulnerable: no bounds checking
    printf("Buffer: %s\\n", buffer);
}
                """.strip(),
                "is_vulnerable": True,
                "cwe_id": "CWE-119",
                "cve_ids": [],
                "severity": "HIGH"
            },
            {
                "id": "github_002",
                "source": "github",
                "file": "examples/sql_injection.c",
                "function": "vulnerable_query",
                "code": """
void vulnerable_query(char *user_input) {
    char query[256];
    sprintf(query, "SELECT * FROM users WHERE name='%s'", user_input);
    // Vulnerable: SQL injection possible
    execute_query(query);
}
                """.strip(),
                "is_vulnerable": True,
                "cwe_id": "CWE-89",
                "cve_ids": [],
                "severity": "HIGH"
            }
        ]
        
        mapped_data.extend(example_entries)
        logger.info(f"Collected {len(mapped_data)} entries from GitHub examples")
        return mapped_data

    def collect_owasp_examples(self) -> List[Dict[str, Any]]:
        """Collect OWASP WebGoat examples"""
        logger.info("Collecting OWASP examples...")
        
        # Example OWASP vulnerable code patterns
        owasp_examples = [
            {
                "id": "owasp_001",
                "source": "owasp",
                "file": "webgoat/xss_example.c",
                "function": "display_user_input",
                "code": """
void display_user_input(char *input) {
    printf("<div>User input: %s</div>", input);
    // Vulnerable: XSS if input contains HTML/JS
}
                """.strip(),
                "is_vulnerable": True,
                "cwe_id": "CWE-79",
                "cve_ids": [],
                "severity": "MEDIUM"
            },
            {
                "id": "owasp_002",
                "source": "owasp",
                "file": "webgoat/path_traversal.c",
                "function": "read_file",
                "code": """
void read_file(char *filename) {
    FILE *fp;
    char path[256];
    sprintf(path, "/var/www/files/%s", filename);
    // Vulnerable: path traversal with ../
    fp = fopen(path, "r");
    if (fp) {
        // read file content
        fclose(fp);
    }
}
                """.strip(),
                "is_vulnerable": True,
                "cwe_id": "CWE-22",
                "cve_ids": [],
                "severity": "HIGH"
            }
        ]
        
        logger.info(f"Collected {len(owasp_examples)} entries from OWASP examples")
        return owasp_examples

    def extract_cwe_from_path(self, file_path: str) -> str:
        """Extract CWE ID from file path"""
        path_lower = file_path.lower()
        
        if "cwe" in path_lower:
            # Try to extract CWE number
            import re
            cwe_match = re.search(r'cwe[_-]?(\d+)', path_lower)
            if cwe_match:
                return f"CWE-{cwe_match.group(1)}"
        
        # Map based on keywords
        for keyword, cwe_id in self.cwe_mappings.items():
            if keyword in path_lower:
                return cwe_id
        
        return "CWE-Unknown"

    def extract_function_name(self, code: str) -> str:
        """Extract main function name from code"""
        import re
        
        # Look for function definitions
        func_pattern = r'(?:void|int|char\*?|float|double)\s+(\w+)\s*\('
        matches = re.findall(func_pattern, code)
        
        if matches:
            # Return first non-main function, or main if it's the only one
            for func in matches:
                if func != "main":
                    return func
            return matches[0]
        
        return "unknown_function"

    def save_mapped_dataset(self, data: List[Dict[str, Any]], filename: str):
        """Save mapped dataset to JSON file"""
        output_path = self.data_path / filename
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Saved {len(data)} entries to {output_path}")

    def create_combined_dataset(self):
        """Combine all datasets and remove duplicates"""
        logger.info("Creating combined dataset...")
        
        all_data = []
        stats = {}
        
        # Load all individual datasets
        for dataset_name, config in self.dataset_configs.items():
            mapping_file = self.data_path / config["mapping_file"]
            if mapping_file.exists():
                with open(mapping_file, 'r', encoding='utf-8') as f:
                    dataset_data = json.load(f)
                    all_data.extend(dataset_data)
                    stats[dataset_name] = len(dataset_data)
                    logger.info(f"Loaded {len(dataset_data)} entries from {dataset_name}")
        
        # Remove duplicates based on code content hash
        seen_hashes = set()
        unique_data = []
        
        for entry in all_data:
            code_hash = hashlib.md5(entry["code"].encode()).hexdigest()
            if code_hash not in seen_hashes:
                seen_hashes.add(code_hash)
                unique_data.append(entry)
        
        # Save combined dataset
        combined_path = self.data_path / "dataset_combined.json"
        with open(combined_path, 'w', encoding='utf-8') as f:
            json.dump(unique_data, f, indent=2, ensure_ascii=False)
        
        # Generate statistics report
        vulnerable_count = sum(1 for entry in unique_data if entry["is_vulnerable"])
        safe_count = len(unique_data) - vulnerable_count
        
        stats_report = {
            "total_entries": len(unique_data),
            "vulnerable_entries": vulnerable_count,
            "safe_entries": safe_count,
            "duplicates_removed": len(all_data) - len(unique_data),
            "sources": stats,
            "cwe_distribution": self.get_cwe_distribution(unique_data),
            "generated_at": datetime.now().isoformat()
        }
        
        stats_path = self.data_path / "dataset_statistics.json"
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats_report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Combined dataset created with {len(unique_data)} unique entries")
        logger.info(f"Statistics: {vulnerable_count} vulnerable, {safe_count} safe")
        logger.info(f"Removed {len(all_data) - len(unique_data)} duplicates")
        
        return stats_report

    def get_cwe_distribution(self, data: List[Dict[str, Any]]) -> Dict[str, int]:
        """Get distribution of CWE types"""
        cwe_counts = {}
        for entry in data:
            cwe_id = entry.get("cwe_id", "Unknown")
            cwe_counts[cwe_id] = cwe_counts.get(cwe_id, 0) + 1
        return dict(sorted(cwe_counts.items(), key=lambda x: x[1], reverse=True))

    def run_collection(self):
        """Run the complete dataset collection process"""
        logger.info("Starting vulnerability dataset collection...")
        
        # Collect from each source
        datasets = {
            "juliet": self.collect_juliet_dataset,
            "github_vuln": self.collect_github_vulnerability_db,
            "owasp": self.collect_owasp_examples
        }
        
        for dataset_name, collector_func in datasets.items():
            try:
                logger.info(f"Collecting {dataset_name} dataset...")
                data = collector_func()
                
                if data:
                    config = self.dataset_configs[dataset_name]
                    self.save_mapped_dataset(data, config["mapping_file"])
                else:
                    logger.warning(f"No data collected for {dataset_name}")
                    
            except Exception as e:
                logger.error(f"Failed to collect {dataset_name}: {e}")
                continue
        
        # Create combined dataset
        stats = self.create_combined_dataset()
        
        logger.info("Dataset collection completed!")
        return stats


def main():
    """Main execution function"""
    collector = VulnerabilityDatasetCollector()
    
    try:
        stats = collector.run_collection()
        print("\n" + "="*50)
        print("DATASET COLLECTION SUMMARY")
        print("="*50)
        print(f"Total entries: {stats['total_entries']}")
        print(f"Vulnerable: {stats['vulnerable_entries']}")
        print(f"Safe: {stats['safe_entries']}")
        print(f"Duplicates removed: {stats['duplicates_removed']}")
        print("\nSource breakdown:")
        for source, count in stats['sources'].items():
            print(f"  {source}: {count} entries")
        print("\nTop CWE types:")
        for cwe, count in list(stats['cwe_distribution'].items())[:5]:
            print(f"  {cwe}: {count} entries")
        
    except Exception as e:
        logger.error(f"Collection failed: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())